<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine learning - The way human teached machine</title>
    <link>/index.xml</link>
    <description>Recent content on Machine learning - The way human teached machine</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>vi</language>
    <lastBuildDate>Sat, 23 Sep 2017 22:33:18 +0700</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Bias và Variance trong học máy, cụ thể là gì ? (P1)</title>
      <link>/posts/bias-variance/</link>
      <pubDate>Sat, 23 Sep 2017 22:33:18 +0700</pubDate>
      
      <guid>/posts/bias-variance/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;Chữ &lt;em&gt;bias&lt;/em&gt; này xuất hiện khá nhiều khi nói về machine learning, một ví dụ gần đây là status trên FB của lão &lt;a href=&#34;https://www.facebook.com/yann.lecun/posts/10154777754622143&#34;&gt;Yann LeCun&lt;/a&gt;. Ngày xưa gặp chữ &lt;em&gt;bias&lt;/em&gt; trước, nghe dịch là &amp;ldquo;lệch&amp;rdquo;, hoặc &amp;ldquo;chệch&amp;rdquo;, đáng sợ hơn nữa là &amp;ldquo;thiên vị&amp;rdquo;. Lúc học ML, thầy cứ nói các giải thuật học máy phải có &lt;em&gt;bias&lt;/em&gt;, không có là nó không &amp;ldquo;học&amp;rdquo; được. Nghe cứ như triết học, ko hiểu chút gì.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Đoạn giới thiệu ở trên không hẳn sai, mà nó dễ gây nhầm lẫn, tại bản thân mình lúc viết đoạn intro trên cũng bị nhầm lẫn. Nhờ 1 chị xinh đẹp góp ý, mới nhận ra chữ &amp;ldquo;&lt;em&gt;bias&lt;/em&gt;&amp;rdquo; có những ý nghĩa khác nhau trong những ngữ cảnh khác nhau, sơ bộ có 3 trường hợp chính:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;bias&lt;/em&gt; được dùng trong Neuron Network, Regression, &amp;hellip; thường mang ý chỉ hệ số &lt;em&gt;bias&lt;/em&gt; ($b$) trong công thức $y=ax + b$. Có thể ngày trước thầy nói không có bias thì giải thuật không thể học được là ý chỉ chữ &lt;em&gt;bias&lt;/em&gt; này.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;bias&lt;/em&gt; trong cái post &lt;a href=&#34;https://www.facebook.com/yann.lecun/posts/10154777754622143&#34;&gt;Yann LeCun&lt;/a&gt; &amp;ldquo;hình như&amp;rdquo; ý chỉ tới &lt;em&gt;inductive bias&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;bias&lt;/em&gt; nói trong bài là &lt;em&gt;bias of an estimator&lt;/em&gt;, đây là khái niệm đến từ xác suất thống kê. Từ chỗ này đến hết bài, chữ &lt;em&gt;bias&lt;/em&gt; được hiểu thuộc loại thứ 3 này nghe :D.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Trước khi đi vào định nghĩa cho từng cái, mình dùng chung các thông tin sau như ngữ cảnh mặc định:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cho trước tập dữ liệu $D={x^{(1)}, x^{(2)}, &amp;hellip;, x^{(n)}}$. Các $x^{(i)}$ đều có &lt;em&gt;indepedent identically distribution (i.i.d)&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Tập dữ liệu $D$ chỉ là một sampling may mắn có được từ không gian data $G$. Bản chất của không gian data $G$ có thể được mô tả thông ra một tập các tham số $\theta$, ví dụ như mean, max, min, &amp;hellip;&lt;/li&gt;
&lt;li&gt;Một &lt;em&gt;Estimator&lt;/em&gt; (ước lượng) $\hat{\theta}$ được hiểu là một hàm bất kỳ, với đầu vào là tập dữ liệu $D$, đầu ra cố gắng trả về các tham số $\theta$ mô tả $G$. Với định nghĩa trên, ta có thể xem mọi model &lt;em&gt;supervised learning&lt;/em&gt; trong học máy đều có thể coi là &lt;em&gt;estimator&lt;/em&gt;, vì đường nào lúc training ta cũng nhận vào tất cả data từ $D$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;1-bias&#34;&gt;1. Bias&lt;/h2&gt;

&lt;p&gt;Cho ví dụ, mean thực tế của $G$ là $5.5$, nghĩa là $\theta=5.5$ nhưng chúng ta không đời nào biết được điều này bởi vì chúng ta không bao giờ có được $G$. Nhiệm vụ của chúng ta là cố gắng đoán ra giá trị này thông qua &lt;em&gt;estimator&lt;/em&gt; $\hat{\theta}$, mà thằng này $\hat{\theta}$ lại dựa vào data $D$. Mong muốn là nó sẽ càng gần $5.5$ càng tốt. Người ta dựa trên cái gọi là &amp;ldquo;gần&amp;rdquo; đó để đánh giá xem &lt;em&gt;estimator&lt;/em&gt; của thằng nào tốt hơn. Và chữ &amp;ldquo;&lt;em&gt;bias&lt;/em&gt;&amp;rdquo; ra đời từ đó.&lt;/p&gt;

&lt;h3 id=&#34;1-1-định-nghĩa-toán-học&#34;&gt;1.1 Định nghĩa toán học:&lt;/h3&gt;

&lt;p&gt;$$\text{bias}(\hat{\theta})= \mathbb{E}(\hat{\theta}) - \theta \tag{1} \label{bias}$$&lt;/p&gt;

&lt;p&gt;Nhìn công thức trên mới hiểu cái nghĩa &amp;ldquo;lệch/chệch&amp;rdquo; của từ &lt;em&gt;bias&lt;/em&gt;. Nếu $\text{bias}(\hat{\theta}) \neq 0$ thì ta gọi &lt;em&gt;estimator&lt;/em&gt; bị &lt;em&gt;biased&lt;/em&gt; (&lt;em&gt;biased estimator&lt;/em&gt;), còn $\text{bias}(\hat{\theta}) = 0$ thì ta gọi là &lt;em&gt;unbiased estimator&lt;/em&gt;. Rất tự nhiên là cái &lt;em&gt;unbiased estimator&lt;/em&gt; tốt hơn &lt;em&gt;biased estimator&lt;/em&gt;, vì cái mình ước đoán nó đúng y chang cái thiệt luôn. Giờ thử tính vài cái xem khi nào &lt;em&gt;bias&lt;/em&gt; khác $0$, khi nào bằng $0$.&lt;/p&gt;

&lt;h4 id=&#34;ví-dụ-1&#34;&gt;Ví dụ 1:&lt;/h4&gt;

&lt;p&gt;Cho $G$ được mô tả bởi hàm phân phối xác suất Bernoulli, hàm này rất đơn giản, chỉ cần 1 tham số mean, gọi nó là $\theta$ luôn cho giống ở trên. Trong trường hợp này, $\theta$ là một số thực R, nằm trong khoảng $(0;1)$. Tập data sample $D={x^{(1)}, x^{(2)}, &amp;hellip;, x^{(n)}}$ được chọn ra từ $G$.&lt;/p&gt;

&lt;p&gt;Rồi, giờ chúng ta đi mò mẫm tìm $\theta$ dựa trên những gì đã biết (là $D$). Con tim ta mách bảo: lấy mean của $D$ đi, có vẻ nó sẽ khá gần với mean thiệt của $G$ luôn đó. Nghe theo tiếng gọi con tim, ta không ngần ngại chọn estimator $\hat\theta$ như sau:
$$\hat\theta = \frac{1}{n} \sum_{i=1}^{n} x^{(i)}$$
Nhưng lý trí không tin mù quáng như vậy được. Bây giờ &lt;a href=&#34;https://youtu.be/BkBqYlLjIeA?t=86&#34;&gt;chọn con tim hay là nghe lý trí&lt;/a&gt;, lý trí đòi đánh giá &lt;em&gt;estimator&lt;/em&gt; $\hat{\theta}$ bằng công thức $(\ref{bias})$.&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
\text{bias}(\hat\theta) &amp;amp;= \mathbb{E}(\hat{\theta}) - \theta \\&lt;br /&gt;
&amp;amp;= \mathbb{E}(\frac{1}{n} \sum_{i=1}^{n} x^{(i)}) - \theta \\&lt;br /&gt;
&amp;amp;= \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}(x^{(i)}) - \theta \text{ (Tính chất của } \mathbb{E})\\&lt;br /&gt;
&amp;amp;= \frac{1}{n} \sum_{i=1}^{n} \theta - \theta \text{ (Tính chất của phân phối Bernoulli } \mathbb{E}(x^{(i)})=\theta )\\&lt;br /&gt;
&amp;amp;= 0
\end{aligned}
$$
Quá đẹp, vậy estimator $\hat{\theta}$ không bị &lt;em&gt;bias&lt;/em&gt;, con tim không cần suy nghĩ cũng đúng.&lt;/p&gt;

&lt;h4 id=&#34;ví-dụ-2&#34;&gt;Ví dụ 2:&lt;/h4&gt;

&lt;p&gt;Cho $G$ được mô tả bởi hàm phân phối xác suất Gaussian $\mathcal{N}(\mu, \sigma^2)$. Tập data sample $D={x^{(1)}, x^{(2)}, &amp;hellip;, x^{(n)}}$ được chọn ra từ $G$.
Yêu câu giờ là đi tìm $\sigma^2$ từ $D$.&lt;/p&gt;

&lt;p&gt;Cũng là con tim mách bảo: vì $\sigma^2$ là hệ số Variance của Gaussian, nên như ví dụ 1, mình cũng đi tìm Variance của $D$ là cho kết quả đúng thôi.
Gọi $\hat{\sigma}^2$ là &lt;em&gt;estimator&lt;/em&gt; của chúng ta, tức là:
$$\hat{\sigma}^2 = \text{Var} (D)$$
Check lại &lt;em&gt;bias&lt;/em&gt;:
\begin{align}
\text{bias}(\hat{\sigma}^2) &amp;amp;= \mathbb{E}(\hat{\sigma}^2) - \sigma^2 \\&lt;br /&gt;
&amp;amp;= \mathbb{E}[\text{Var}(D)] - \sigma^2 \\&lt;br /&gt;
\mathbb{E}[\text{Var}(D)] &amp;amp;= \mathbb{E}\{\mathbb{E}[(x^{(i)} - \hat{\mu})^2]\}  \text{ (Định nghĩa của Var, với } \hat{\mu} = \frac{1}{n}\sum_{i=1}^{n}x^{(i)}) \\&lt;br /&gt;
&amp;amp;= \mathbb{E}[\frac{1}{n}\sum_{i=1}^{n}(x^{(i)} - \hat{\mu})^2]  \text{ (Định nghĩa của } \mathbb{E}) \\&lt;br /&gt;
&amp;amp;= \frac{1}{n}\mathbb{E}[\sum_{i=1}^{n}(x^{(i)} - \frac{1}{n}\sum_{j=1}^{n}x^{(j)})^2]  \\&lt;br /&gt;
&amp;amp;= \frac{1}{n}\mathbb{E}[\sum_{i=1}^{n}\frac{((n-1)x^{(i)} - \sum_{j=1, j\neq i}^{n}x^{(j)})^2}{n^2}]  \\&lt;br /&gt;
&amp;amp;= \frac{1}{n}\mathbb{E}[\sum_{i=1}^{n}\frac{(n-1)^2x^{(i)2} - 2 \times (n-1)x^{(i)} \times \sum_{j=1, j\neq i}^{n}x^{(j)} + (\sum_{j=1, j\neq i}^{n}x^{(j)})^2 }{n^2}]  \\&lt;br /&gt;
&amp;amp;= \frac{1}{n}\sum_{i=1}^{n}\frac{(n-1)^2\mathbb{E}[x^{2}] - 2 \times (n-1)\mathbb{E}[x^{(i)} \times \sum_{j=1, j\neq i}^{n}x^{(j)}] + \mathbb{E}[(\sum_{j=1, j\neq i}^{n}x^{(j)})^2] }{n^2}]  \\&lt;br /&gt;
&amp;amp;= \frac{1}{n^3}\sum_{i=1}^{n}[(n-1)^2\mathbb{E}[x^{2}] - 2 \times (n-1)\mathbb{E}[x^{(i)} \times \sum_{j=1, j\neq i}^{n}x^{(j)}] + \mathbb{E}[(\sum_{j=1, j\neq i}^{n}x^{(j)})^2]] \\&lt;br /&gt;
&amp;amp;= \frac{1}{n^3}\sum_{i=1}^{n}[(n-1)^2A - 2 \times (n-1)B + C]  \tag{2.1} \label{2.1} \\&lt;br /&gt;
\end{align}
Căng dữ, tính từng cái $A, B, C$ cho đỡ nhức đầu:
\begin{align}
A &amp;amp;= \mathbb{E}[x^{2}] \tag{2.2} \label{2.2} \\&lt;br /&gt;
B &amp;amp;= \mathbb{E}[x^{(i)} \times \sum_{j=1, j\neq i}^{n}x^{(j)}]\\&lt;br /&gt;
&amp;amp;= \mathbb{E}[x^{(i)}] \times \mathbb{E}[\sum_{j=1, j\neq i}^{n}x^{(j)}] \text{ (} x^{(i)} \text{độc lập với} \sum_{j=1, j\neq i}^{n}x^{(j)}) \\&lt;br /&gt;
&amp;amp;= \mathbb{E}[x] \times \sum_{j=1, j\neq i}^{n}\mathbb{E}[x] \\&lt;br /&gt;
&amp;amp;= \mathbb{E}[x] \times (n-1)\mathbb{E}[x] \\&lt;br /&gt;
&amp;amp;= (n-1)(\mathbb{E}[x])^2\\\ \tag{2.3} \label{2.3}
\end{align}
Để tính $C$, không mất tính tổng quá, chọn $i=1$
\begin{align}
C &amp;amp;= \mathbb{E}[(\sum_{j=1, j\neq i}^{n}x^{(j)})^2] \\&lt;br /&gt;
C &amp;amp;= \mathbb{E}[(x^{(2)} + x^{(3)} + &amp;hellip; + x^{(n)})^2] \\&lt;br /&gt;
&amp;amp;= \mathbb{E}[(x^{(2)} + x^{(3)} + &amp;hellip; + x^{(n)}) \times (x^{(2)} + x^{(3)} + &amp;hellip; + x^{(n)})] \\&lt;br /&gt;
&amp;amp;= \begin{aligned}[t]
\mathbb{E}[
&amp;amp;x^{(2)2} + x^{(2)}x^{(3)} + &amp;hellip; + x^{(2)}x^{(n)} + \\&lt;br /&gt;
&amp;amp;x^{(3)}x^{(2)} + x^{(3)2} + &amp;hellip; + x^{(3)}x^{(n)} + \\&lt;br /&gt;
&amp;amp;x^{(4)}x^{(2)} + x^{(4)}x^{(3)} + &amp;hellip; + x^{(4)}x^{(n)} + \\&lt;br /&gt;
&amp;amp;x^{(n)}x^{(2)} + x^{(n)}x^{(3)} + &amp;hellip; + x^{(n)2}]
\end{aligned} \\&lt;br /&gt;
&amp;amp;= \begin{aligned}[t]
[
&amp;amp;\mathbb{E}(x^{(2)2}) + \mathbb{E}(x^{(2)}x^{(3)}) + &amp;hellip; + \mathbb{E}(x^{(2)}x^{(n)}) + \\&lt;br /&gt;
&amp;amp;\mathbb{E}(x^{(3)}x^{(2)}) + \mathbb{E}(x^{(3)2}) + &amp;hellip; + \mathbb{E}(x^{(3)}x^{(n)}) + \\&lt;br /&gt;
&amp;amp;\mathbb{E}(x^{(4)}x^{(2)}) + \mathbb{E}(x^{(4)}x^{(3)}) + &amp;hellip; + \mathbb{E}(x^{(4)}x^{(n)}) + \\&lt;br /&gt;
&amp;amp;\mathbb{E}(x^{(n)}x^{(2)}) + \mathbb{E}(x^{(n)}x^{(3)}) + &amp;hellip; + \mathbb{E}(x^{(n)2})]
\end{aligned} \\&lt;br /&gt;
&amp;amp;=(n-1)\mathbb{E}[x^2] + (n-1)(n-2)(\mathbb{E}[x])^2 \tag{2.4} \label{2.4} \\&lt;br /&gt;
\end{align}&lt;/p&gt;

&lt;p&gt;Thay hết \ref{2.2}, \ref{2.3}, \ref{2.4} vào \ref{2.1}, ta có:
\begin{align}
\mathbb{E}[\text{Var}(D)] &amp;amp;= \frac{1}{n^3}\sum_{i=1}^{n}[(n-1)^2A - 2 \times (n-1)B + C] \\&lt;br /&gt;
&amp;amp;= \frac{1}{n^3}\sum_{i=1}^{n}[(n-1)^2\mathbb{E}[x^{2}] - 2 \times (n-1)^2(\mathbb{E}[x])^2 + (n-1)\mathbb{E}[x^2] + (n-1)(n-2)(\mathbb{E}[x])^2] \\&lt;br /&gt;
&amp;amp;= \frac{1}{n^3}\sum_{i=1}^{n}[n(n-1)\mathbb{E}[x^{2}] - n(n-1)(\mathbb{E}[x])^2 ] \\&lt;br /&gt;
&amp;amp;= \frac{n-1}{n^2}\sum_{i=1}^{n}[\mathbb{E}[x^{2}] - (\mathbb{E}[x])^2 ] \\&lt;br /&gt;
&amp;amp;= \frac{n-1}{n^2}\sum_{i=1}^{n}\text{Var}(x) \text{ (Định nghĩa của Var, tham khảo [3])} \\&lt;br /&gt;
&amp;amp;= \frac{n-1}{n}\text{Var}(x) \\&lt;br /&gt;
&amp;amp;= \frac{n-1}{n}\sigma^2 \\&lt;br /&gt;
\end{align}&lt;/p&gt;

&lt;p&gt;Từ đó:
\begin{align}
\text{bias}(\hat{\sigma}^2) &amp;amp;= \mathbb{E}(\hat{\sigma}^2) - \sigma^2 \\&lt;br /&gt;
&amp;amp;= \frac{n-1}{n}\sigma^2 - \sigma^2 \\&lt;br /&gt;
&amp;amp;= \frac{-1}{n}\sigma^2 \neq 0 \\&lt;br /&gt;
\end{align}
Sau chặng đường dài, lần này lý trí thắng, &lt;em&gt;estimator&lt;/em&gt; này bị &lt;em&gt;bias&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&#34;1-2-kết-lại&#34;&gt;1.2 Kết lại&lt;/h3&gt;

&lt;p&gt;Hai cái ví dụ, một mớ tính toán chỉ là phần kỹ thuật, cốt lõi là để thấy rằng những cảm nhận ban đầu tưởng như đúng (như $\mathbb{E}$ của $D$ thì may mắn đúng bằng $\mathbb{E}$ của $G$ luôn), nhưng nhiều trường hợp lại sai khác ($\text{Var}$ của $G$ với phân phối Gaussian không bằng $\text{Var}$ của $D$).&lt;/p&gt;

&lt;p&gt;Về phần tính toán, ví dụ 1 thì đơn giản, tham khảo theo sách [1], ví dụ 2 căng hơn xíu, cái cách trên là mình tự làm (có phần bỏ sức trâu bò), có cách chứng minh tinh tế hơn, tham khảo số tại &lt;a href=&#34;https://stats.stackexchange.com/questions/136673/how-to-understand-that-mle-of-variance-is-*bias*ed-in-a-gaussian-distribution/136703#136703?s=908df983477d41b1b3ed0e34a316166e&#34;&gt;đây&lt;/a&gt;. Ví dụ 2 này cũng được nhắc đến trong sách số [2], tác giả dùng để đưa đến khái niệm &lt;em&gt;bias&lt;/em&gt;, nhưng chỉ &lt;del&gt;lước&lt;/del&gt; lướt qua.&lt;/p&gt;

&lt;p&gt;Toàn bộ nội dung chính của bài viết này lấy từ 5.4.2 sách [1] (trừ phần chứng minh của ví dụ 2, sách không thèm làm).&lt;/p&gt;

&lt;p&gt;Câu hỏi đặt ra là:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Việc một estimator bị &lt;em&gt;bias&lt;/em&gt; thì ảnh hưởng như thế nào đến quá trình training ?&lt;/li&gt;
&lt;li&gt;Như phần đầu có nói, trực giác thì thấy rằng &lt;em&gt;unbiased estimator&lt;/em&gt; thì tốt hơn &lt;em&gt;biased estimator&lt;/em&gt;, tuy nhiên có phải luôn luôn như vậy ? Có phải trong thực tế chọn &lt;em&gt;estimator&lt;/em&gt; không &lt;em&gt;bias&lt;/em&gt; luôn luôn là tốt nhất ?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2-variance&#34;&gt;2. Variance&lt;/h2&gt;

&lt;p&gt;Đuối &amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;tham-khảo&#34;&gt;Tham khảo&lt;/h2&gt;

&lt;p&gt;[1] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning, vol. 13, no. 1. 2015.&lt;/p&gt;

&lt;p&gt;[2] Bishop, Christopher M. &amp;ldquo;Pattern recognition.&amp;rdquo; Machine Learning 128 (2006): 1-58.&lt;/p&gt;

&lt;p&gt;[3] Probability and Statistics
Cookbook: &lt;a href=&#34;http://pages.cs.wisc.edu/~tdw/files/cookbook-en.pdf&#34;&gt;http://pages.cs.wisc.edu/~tdw/files/cookbook-en.pdf&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gradient - Cái giống khó hiểu :3</title>
      <link>/posts/gradient-what-the-fuck-is-this/</link>
      <pubDate>Sun, 28 May 2017 22:19:54 +0700</pubDate>
      
      <guid>/posts/gradient-what-the-fuck-is-this/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Gradient Descent&lt;/em&gt; là cụm từ được nghe rất nhiều khi học về MLP, Neuron Network, hay CNN. Quen em nó nhau lâu rồi, nhìn mặt nhau miết, tưởng mình đã hiểu nhau, mà tới nay mới vẽ lẽ vừa không hiểu và vừa hiểu sai em nó quá trời&amp;hellip; Nay nhờ hoàn cảnh đưa đẩy mà mình hiểu thêm được em nó chút xíu.&lt;/p&gt;

&lt;h3 id=&#34;gradient-em-nó-là-ai&#34;&gt;Gradient - em nó là ai ?&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Gradient&lt;/em&gt; của một hàm $f(x_1, x_2, &amp;hellip;, x_n)$, được ký hiệu $\nabla f$ là một vector $n$ chiều, mà mỗi thành phần trong vector đó là một đạo hàm riêng phần (&lt;em&gt;partial derivative&lt;/em&gt;) theo từng biến của hàm đó:
$$
\bbox[20px,border:1px solid black] {
\nabla f=(\frac{df}{dx_1}, \frac{df}{dx_1}, &amp;hellip;, \frac{df}{dx_n})
}
$$&lt;/p&gt;

&lt;p&gt;Sau đây là 2 điều mình mới hiểu ra:&lt;/p&gt;

&lt;h3 id=&#34;1-vector-gradient-tại-một-điểm-cho-mình-biết-từ-điểm-đó-hướng-nào-làm-tăng-giá-trị-f-nhiều-nhất-có-thể-tại-sao-lại-là-tăng&#34;&gt;&lt;strong&gt;1. Vector gradient tại một điểm cho mình biết từ điểm đó, hướng nào làm tăng giá trị $f$ nhiều nhất có thể. Tại sao lại là tăng ?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Chấp nhận điều này lâu rồi, tự nhiên hôm qua mới ngớ ra: Tại sao lại là hướng tăng (Hình 1) mà không phải là hướng giảm ?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/gradient-what-the-fuck-is-this/myth1.png&#34; alt=&#34;Example image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hình 1.&lt;/strong&gt; Hướng của vector &lt;em&gt;gradient&lt;/em&gt; luôn chỉ về phía cao hơn&lt;/p&gt;

&lt;p&gt;Wow wow, là vì do ông nào định nghĩa nó như vậy :v. Mỗi thành phần của vector &lt;em&gt;gradient&lt;/em&gt; là một đạo hàm riêng phần, giờ thử xét $\frac{df}{dx}$.
Định nghĩa đạo hàm theo [1]:
$$\frac{df}{dx} = \frac{f(x+\epsilon)-f(x)}{\epsilon}$$
với $\epsilon&amp;gt;0$ và đủ nhỏ. Mấu chốt đều nằm ở ông $\epsilon$, vì $ \epsilon&amp;gt;0$, nên chiều của $\frac{df}{dx}$ chỉ còn phụ thuộc vào tử số.&lt;/p&gt;

&lt;p&gt;Từ đó ta xét 2 trường hợp $\vec{AB}$ và $\vec{CD}$ sẽ hiểu:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Vector AB: $$\frac{df}{dx}=\frac{f(A) - f(B)}{x_A - x_B}$$
Vì $f(A)&amp;lt;f(B)$ (trong trường hợp này), cho nên tử âm, $\vec{AB}$ chỉ về hướng âm, cũng là hướng của $f$ tăng.&lt;/li&gt;
&lt;li&gt;Vector CD: $$\frac{df}{dx}=\frac{f(D) - f( C)}{x_D - x_C}$$
Vì $f( C)&amp;lt;f(D)$ (trong trường hợp này), cho nên tử dương, $\vec{CD}$ chỉ về hướng dương, cũng là hướng của $f$ tăng.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Vì vậy mà $\frac{df}{dx}$ luôn chỉ về hướng $f$ tăng.&lt;/p&gt;

&lt;h3 id=&#34;2-vector-gradient-trực-giao-perpendicular-với-contour-line-của-hàm&#34;&gt;&lt;strong&gt;2. Vector &lt;em&gt;gradient&lt;/em&gt; trực giao (&lt;em&gt;perpendicular&lt;/em&gt;) với &lt;em&gt;contour line&lt;/em&gt; của hàm&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Điều này nghe bực mình + rối rắm kinh khủng khi cứ nhớ lớp 12, được học đạo hàm của hàm $y = f(x)$ tại $x_0$ chính là pháp tuyến của $f(x)$ tại $x_0$. Rà lại, đọc về đạo hàm (&lt;em&gt;derivative&lt;/em&gt;) thấy đâu đâu cũng vẽ hình tiếp tuyến [1], cái khỉ gì giờ lại là trực giao ? Với vừa nãy ở trên mới nói là hướng làm tăng $f$, sao giờ lại có chuyện trực giao ở đây ?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/gradient-what-the-fuck-is-this/myth2a.png&#34; alt=&#34;Example image&#34; /&gt;
&lt;img src=&#34;/img/gradient-what-the-fuck-is-this/myth2b.png&#34; alt=&#34;Example image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;strong&gt;Hình 2.&lt;/strong&gt; Vector trong hình 2.a hay 2.b mới đúng là vector &lt;em&gt;gradient&lt;/em&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Mấu chốt nằm ở khái niệm &lt;em&gt;contour line&lt;/em&gt;. Nó có vài tên khác nhau: contour line, level set, level curve. Định nghĩa ở đây cả [2]. Đại khái một contour line là tập hợp những điểm làm cho hàm có cùng một giá trị $y_0$ nào đó. Hàm có miền vô hạn thì cũng có nghĩa là có vô số contour line.&lt;/p&gt;

&lt;p&gt;Vậy là Hình 2.a và 2.b đang biểu diễn hai đường khác nhau: đường màu đen trong hình 2.a là đồ thị biểu diễn sự phụ thuộc $y$ theo $x$ qua hàm $y=f(x)$, đường màu xanh trong hình 2.b là một đường &lt;em&gt;contour line&lt;/em&gt; biểu diễn của hàm $g(x)=0$. Mình bị nhầm lẫn bởi vì lâu nay học các hàm $y=f(x)$, đa số đều là hàm đơn biến, biểu diễn đồ thị của nó bằng tọa độ 2 chiều. Nhưng với các hàm đa biến (từ 2 biến trở lên), người ta khó biểu diễn đồ thị của hàm trên tọa độ 2 chiều nữa, nên người ta nghĩ ra cái &lt;em&gt;contour line&lt;/em&gt; dễ biểu diễn hơn.&lt;/p&gt;

&lt;p&gt;Khi học về Linear Regression, $y=WX + b$, người ta thường lấy ví dụ $W$ và $X$ có 2 chiều, cụ thể $y=w_1x_1 + w_2x_2 + w_0$, điều này khiến mình liên tưởng đến hàm $y=ax + b$ hồi xưa có học, chỉ là chuyển vế qua thì $x$, $y$ tương ứng $w_1$, $w_2$. Điều này sai hoàn toàn, SAI ÁC LIỆT LUÔN. Chính từ đây dẫn tới những nhầm lẫn khi đọc đến vector &lt;em&gt;gradient&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Nói chính xác thì $y=ax+b$ chỉ là một phần tử trong tập &lt;em&gt;contour line&lt;/em&gt; của $y=w_1x_1 + w_2x_2 + w_0$. Và nhiệm vụ của Linear Regression là đi tìm một &lt;em&gt;contour line&lt;/em&gt; trong tập các &lt;em&gt;contour line&lt;/em&gt; ở trên.&lt;/p&gt;

&lt;p&gt;Về chuyện ngày lớp 12 được dạy rằng đạo hàm của hàm $y=f(x)$ là một vector có phương tiếp tuyến với đồ thị $f(x)$. Điều này được giải thích như sau: Hàm $y=f(x)$ là hàm một biến. Nếu vẽ theo kiểu &lt;em&gt;contour line&lt;/em&gt;, mỗi &lt;em&gt;contour line&lt;/em&gt; sẽ là 1 điểm (hoặc một vài điểm). Vì vậy mà đương nhiên nó thoải điều kiện vector &lt;em&gt;gradient&lt;/em&gt; trực giao với đường &lt;em&gt;contour line&lt;/em&gt;. Không có mâu thuẫn gì ở đây cả.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;P.s: Viết ra mới thấy, tuy đã hiểu, đã nắm được cái bản chất, mà muốn thể hiện nó ra vẫn khó thiệt. Bài này quá lủng cũng.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;tham-khảo&#34;&gt;&lt;strong&gt;Tham khảo&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;[1] &lt;a href=&#34;https://en.wikipedia.org/wiki/Derivative&#34;&gt;https://en.wikipedia.org/wiki/Derivative&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&#34;https://en.wikipedia.org/wiki/Level_set&#34;&gt;https://en.wikipedia.org/wiki/Level_set&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] Anzai, Yuichiro. Pattern recognition and machine learning. Elsevier, 2012.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Xác suất trong học máy (Phần 2)</title>
      <link>/posts/xac-suat-trong-hoc-may-phan2/</link>
      <pubDate>Sun, 19 Mar 2017 10:19:05 +0700</pubDate>
      
      <guid>/posts/xac-suat-trong-hoc-may-phan2/</guid>
      <description>

&lt;p&gt;Xác suất là một phân ngành trong toán học, mà theo giáo sư toán Arthur Benjamin (được biết đến như một nhà ảo thuật toán &amp;ldquo;Mathemagician&amp;rdquo;) đề nghị trong bài TED talk của ông: mọi người nên học xác suất và thống kê trước khi học giải tích.&lt;/p&gt;

&lt;p&gt;Trong lĩnh vực IA, nhiều giải thuật có liên quan đến lý thuyết xác suất như: Hidden Markov Model, Bayesian Network, chứng minh ưu điểm của Random Forrest,&amp;hellip; Việc hiểu căn bản lý thuyết xác suất, đặc biệt là hiểu các định nghĩa hay dùng là rất cần thiết cho việc hiểu bản chất vấn đề và cách giải quyết trong các giải thuật học máy.&lt;/p&gt;

&lt;p&gt;Tiếp theo &lt;a href=&#34;https://ductri.github.io/post/xac-suat-trong-hoc-may/&#34;&gt;&lt;em&gt;phần 1&lt;/em&gt;&lt;/a&gt;, trong bài viết này mình sẽ nói chi tiết về các thuật ngữ xác suất hay gặp trong các giải thuật học máy. Xác suất trong học máy thường xuất hiện các khái niệm: xác suất tiên nghiệm (&lt;em&gt;prior probability&lt;/em&gt;), xác suất hậu nghiệm (&lt;em&gt;posterial probability&lt;/em&gt;), hàm &lt;em&gt;likelihood&lt;/em&gt;, xác suất có điều kiện (&lt;em&gt;conditional probability&lt;/em&gt;). Vấn đề cốt lõi các của bài toán học máy là tìm tham số w, sao cho mô hình (model) f khi nhận vào một giá trị x, cùng với tham số w, cho ra kết quả giống với thực tế nhất. Để hỗ trợ cho việc tìm w, ta thường có tập dữ liệu D cho trước. Tất nhiên w, x có thể là giá trị scala, cũng có thể có nhiều chiều. Nhưng để đơn giản, cứ tạm thời xem chúng là giá trị scala.&lt;/p&gt;

&lt;h1 id=&#34;1-xác-suất-tiên-nghiệm&#34;&gt;1. Xác suất tiên nghiệm&lt;/h1&gt;

&lt;p&gt;Cách đơn giản, ngây thơ (vô số tội) là cho ngẫu nhiên w. Khi đó ta đụng đến khái niệm đầu tiên: p(w) được gọi là xác suất tiên nghiệm (&lt;em&gt;prior probability&lt;/em&gt;). p(w) là phân phối xác suất trên w, lưu ý là phân phối xác suất, chứ ko phải là xác suất. Phân phối này hoặc là Mass, hoặc là Density. Điều này có nghĩa tổng xác suất lấy trên toàn miền giá trị w bằng 1. p(w) phản ánh sự chủ quan đối với biến w, bởi vì đơn thuần dựa trên trực giác, ta gán cho w một phân phối xác suất.&lt;/p&gt;

&lt;h1 id=&#34;2-xác-suất-hậu-nghiệm&#34;&gt;2. Xác suất hậu nghiệm&lt;/h1&gt;

&lt;p&gt;Tiếp theo, ta sử dụng đến giả thiết đã có D. Đến lược p(w|D) xuất hiện. Đây là một xác suất có điều kiện (&lt;em&gt;conditional probability&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Xác suất có điều kiện là xác suất với kết quả của một biến cố đã được biết trước. Lưu ý điều này không có nghĩa là biến cố đó phải xảy ra trước. Một bài toán đơn giản để mô tả về xác suất có điều kiện như sau: Có 2 hộp trắng và 1 hộp đen. Trong mỗi hộp trắng có 3 bi đỏ, 7 bi xanh. Trong mỗi hộp đen có 4 bi đỏ, 6 bi xanh. Chọn một hộp bất kỳ, sau đó lấy ngẫu nhiên 1 viên bi. Tìm xác suất để hộp được chọn là hộp trắng khi đã biết viên bi chọn ra có màu xanh. Đây là một xác suất có điều kiện, với điều kiện là một biến cố (chọn viên bi) xảy ra sau biến cố (chọn hộp) cần tìm xác suất.&lt;/p&gt;

&lt;p&gt;Quay trở lại p(w|D). Nhớ rằng p(w|D) là một hàm trên biến w. p(w|D) phản ánh sự ước lượng cho w khi đã biết D. Xác suất này được gọi là xác suất hậu nghiệm (&lt;em&gt;posterial probability&lt;/em&gt;), bởi vì chúng ta đang ước lượng cho w sau khi đã biết thông tin về D. Xác suất này tương ứng với việc tìm xác suất để chọn được bi xanh sau khi đã chọn hộp màu trắng. Tương tự như p(w), đây cũng là một Mass hoặc Density (tùy loại biến), cho nên tổng xác suất có điều kiện (D) trên toàn miền giá trị w cũng bằng 1.&lt;/p&gt;

&lt;h1 id=&#34;3-hàm-likelihood&#34;&gt;3. Hàm likelihood&lt;/h1&gt;

&lt;p&gt;Xác suất cuối cùng, và mình thấy hay nhất là hàm &lt;em&gt;likelihood&lt;/em&gt;: p(D|w). Nếu đã cảm thấy p(w|D) khá tự nhiên thì đến xác suất này tạo cảm giác khó hiểu lạ thường. Nhớ rằng D là cái mình đã có, w là cái mình cần tìm. Nghĩa là p(D|w) vẫn là hàm trên w như p(w|D), nhưng khác biệt là tổng xác suất trên toàn miền giá trị w không bằng 1, vì về mặt xác suất thì p(D|w) là phân bố xác suất có điều kiện trên D. Về bản chất thì rõ ràng nó là xác suất, nhưng người ta chỉ nói làm hàm likelihood chứ không gọi là xác suất likelihood. Theo mình thì người ta gọi như vậy để phân biệt với p(w) và p(w|D). Hai xác suất này là 2 phân bố xác suất thuộc &lt;em&gt;mass/density probability distribution&lt;/em&gt; trên biến w, tức là xét trên toàn miền giá trị w thì tổng giá trị xác suất luôn bằng 1. Còn p(D|w) tuy cũng là &lt;em&gt;mass/density probability distribution&lt;/em&gt; nhưng lại đối với biến D, nên tổng xác suất trên miền giá trị w không nhất thiết bằng 1.&lt;/p&gt;

&lt;p&gt;Ý nghĩa xác suất p(D|w) là thể hiện độ phù hợp của D đối với những giá trị w khác nhau. Mục đính chính của học máy thể hiện ở đây: Ta mong muốn tìm w để đạt được giá trị tối đa của p(D|w), nghĩa là tìm w sao cho phù hợp nhất với tập D đã có.&lt;/p&gt;

&lt;p&gt;Cuộc sống nhiều chuyện vượt quá tầm tay ta, D là một trong những cái đó. Trong khi đó, cách ta bước tiếp, cách ta đáp ứng với cái D đó (w), ta có thể kiểm soát được. Vậy nên thôi đừng hỏi vì sao D xảy ra, đừng mong chờ D sẽ thay đổi (dữ liệu training có sẳn rồi, cố định bà nó rồi), hãy điều chỉnh w sao cho phù hợp với D nhất, tìm w để tối ưu hóa p(D|w), đó mới là điều cần boăn khoăn, cần suy nghĩ. Nhưng cụ thể làm sao thì là chuyện riêng của mỗi bác giải thuật học máy, ở đây mình không bàn.&lt;/p&gt;

&lt;h1 id=&#34;4-quan-hệ-giữa-ba-xác-suất-trên&#34;&gt;4. Quan hệ giữa ba xác suất trên&lt;/h1&gt;

&lt;p&gt;Chốt chỗ này cái. Ta có ba xác suất: xác suất tiền nghiệm, xác suất hậu nghiệm và hàm likelihood. Quan hệ của ba xác suất suất thể hiện qua công thức xác suất Bayesian:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;p(w|D) = p(D|w) x p(w) / p(D)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Vì D là cái đã có và cố định, ta xem p(D) như hằng số. Khi đó:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;p(w|D) ~ p(D|w) x p(w)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;posterial probability ~ likelihood probability x prior probability&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Công thức trên là nền tảng cơ bản cho các giải thuật học máy, cho ta một cái nhìn về bản chất về vấn đề cần giải quyết trong học máy.&lt;/p&gt;

&lt;h1 id=&#34;tham-khảo&#34;&gt;Tham khảo&lt;/h1&gt;

&lt;p&gt;Bishop, Christopher M. &amp;ldquo;Pattern recognition.&amp;rdquo; Machine Learning 128 (2006).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Xác suất trong học máy (Phần 1)</title>
      <link>/posts/xac-suat-trong-hoc-may/</link>
      <pubDate>Sat, 18 Mar 2017 00:37:24 +0700</pubDate>
      
      <guid>/posts/xac-suat-trong-hoc-may/</guid>
      <description>

&lt;p&gt;Trong bài này, mình sẽ tổng hợp lại các khái niệm cơ bản trong xác suất, các hướng tiếp cận xác suất. Đặc biệt, mình sẽ giải thích tên gọi một số loại xác suất hay được nhắc tới trong học máy. Mình viết hoàn toàn tiếng việt, chỗ nào chen dô tiếng anh chỉ là thuật ngữ, để ai đọc sách tiếng anh dễ nhận biết.&lt;/p&gt;

&lt;p&gt;Cá nhân mình thấy mình có &amp;ldquo;hơi&amp;rdquo; nắm được lý thuyết xác suất. Nhưng xưa giờ toàn học trên tiếng Việt, nên khi đọc sách tiếng Anh lòi ra kha khá khái niệm mới mẻ, hay ho, giờ viết ra để tự hệ thống lại. Vì dài quá nên mình chia làm 2 bài. Nội dung trong bài đầu này giới thiệu sơ lược lại về xác suất, cung cấp cái nhìn tổng quát trên góc nhìn toán học, chưa liên quan gì học máy ở đây cả&amp;hellip; Bài sau sẽ mô tả chi tiết các thuật ngữ hay gặp: prior probability, likelihood probability, posterial probability, conditional probability.&lt;/p&gt;

&lt;h1 id=&#34;1-các-cách-hiểu-về-xác-suất&#34;&gt;1. Các cách hiểu về xác suất&lt;/h1&gt;

&lt;p&gt;Hiện nay có hơn một cách hiểu về xác suất ([1], [2], [3], [4]). Vì chúng ta chỉ quan tâm trong ngữ cảnh học máy, nên mình tham khảo theo Bishop[3], ông cho rằng có 2 hướng tiếp cận: &lt;strong&gt;Frenqtist Interpretation Probability&lt;/strong&gt; (gọi tắt trong bài là FIP) và &lt;strong&gt;Bayesian Probability&lt;/strong&gt; (gọi tắt trong bài là BP). Hướng FIP hiểu xác suất như tỉ lệ số lần thử nghiệm thành công, chia số lần thử nghiệm; hoặc số mẫu phù hợp chia cho kích thước không gian mẫu. Cách tiếp cận này mang hơi hướng của thống kê. Cách tiếp cận này dễ hiểu.&lt;/p&gt;

&lt;p&gt;Cách tiếp cận thứ hai BP tổng quát hơn, cũng có phần tự nhiên hơn. Ví dụ với các phát biểu như: khả năng trái đất sẽ bị hủy diệt trong 100 năm tới, khả năng chiến tranh thế giới thứ 3 nổ ra, &amp;hellip; Các sự kiện trên rất hiếm khi xảy ra, hoặc chưa bao giờ xảy ra, nên không thể tiến hành các thử nghiệm để tính xác suất theo cách hiểu FIP. Hướng tiếp cận BP có tính tổng quát hơn. Xác suất được hiểu như một độ đo về mức độ tin tưởng. Từ đó, chúng ta có thể gán xác suất cho cả các sự kiện chưa bao giờ xảy ra.&lt;/p&gt;

&lt;h1 id=&#34;2-xác-suất-trên-biến-ngẫu-nhiên-rời-rạc-và-liên-tục&#34;&gt;2. Xác suất trên biến ngẫu nhiên rời rạc và liên tục&lt;/h1&gt;

&lt;p&gt;Khi nói đến xác suất, người ta ngầm định là đang nói đến xác suất trên một/nhiều biến (biến cố). Ta thường bị nhầm lẫn giữa hai khái niệm. Khái niệm thường mặc định được hiểu là xác suất khi một biến cố X nhận giá trị x&lt;sub&gt;0&lt;/sub&gt;, được ký hiện là p(X = x&lt;sub&gt;0&lt;/sub&gt;), đôi khi được viết tắt là p(x&lt;sub&gt;0&lt;/sub&gt;). Ví dụ như nói trong hộp có 3 bi đỏ, 5 bi xanh, 4 bi vàng, 2 bi xám, thì xác suất để chọn được bi đỏ là &lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;14&lt;/sub&gt;. Nói như vậy nghĩa là xác suất để biến cố chọn bi (X) trúng bi màu đỏ là &lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;14&lt;/sub&gt;, p(X = bi đỏ) = &lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;14&lt;/sub&gt;.&lt;/p&gt;

&lt;p&gt;Khái niệm thứ hai tổng quát hơn: phân phối xác suất (&lt;strong&gt;probability distribution&lt;/strong&gt;) trên biến cố X, được ký hiện là p(X). Ta đi tìm xác suất cho mọi giá trị mà biến cố X có thể có, rồi tính xác suất cho từng giá trị đó, thì sẽ có được một phân phối xác suất. Trong ví dụ trên, tính thêm các xác suất p(X = bi xanh), p(X = bi vàng), p(X = bi xám),  thì ta sẽ có được phân phối xác suất cho biến X (biến cố X).&lt;/p&gt;

&lt;p&gt;Có 2 loại biến ngẫu nhiên: biến ngẫu nhiên rời rạc (&lt;strong&gt;dicrete variable&lt;/strong&gt;) và biến ngẫu nhiên liên tục (&lt;strong&gt;continuous variable&lt;/strong&gt;). Ở ví dụ trên, X là một biến ngẫu nhiên rời rạc. Để mô tả phân phối xác suất cho biến ngẫu nhiên rời rạc, ta lập một bảng tương tự như Bảng 1.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bảng 1.&lt;/strong&gt; Bảng phân phối xác suất trên biến X&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/bang-phan-phoi-xac-suat.png&#34; alt=&#34;Example image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Bảng 1 được gọi là Bảng phân phối xác suất trên biến X. Đây là cách để mô tả phân phối xác suất đối với biến ngẫu nhiên rời rạc. Tuy nhiên, để Bảng 1 được gọi như thế, nó cần thoải hai điều kiện:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Giá trị p(X = x&lt;sub&gt;i&lt;/sub&gt;) &amp;gt; 0&lt;/li&gt;
&lt;li&gt;Tổng các p(X = x&lt;sub&gt;i&lt;/sub&gt;) = 1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;/img/bieu-do-phan-bo-xac-suat.png&#34; alt=&#34;Example image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hình 1.&lt;/strong&gt; Biểu đồ phân bố xác suất&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/ham-mat-do-xac-suat.png&#34; alt=&#34;Example image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hình 2.&lt;/strong&gt; Hàm mật độ xác suất&lt;/p&gt;

&lt;p&gt;Ngoài ra, Bảng 1 còn có thể được mô hình hóa như Hình 1. Nếu số lượng các cột đủ nhiều, tiến dần đến vô cùng, ta sẽ dần có được cách biểu diễn xác suất cho biến ngẫu nhiên liên tục. Chỉ cần chỉnh sửa Hình 1 một chút, xóa hết các cột đi, làm mịn các đường nối lại, ta được Hình 2 là hàm mật độ xác suất trên biến X. Đây là cách biểu diễn phân phối xác suất cho biến ngẫu nhiên liên tục. Cũng tương tự như biến ngẫu nhiên rời rạc, để hàm trên được gọi tên nghe kêu như thế, nó cần thoải hai điều kiện:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Giá trị f(x&lt;sub&gt;i&lt;/sub&gt;) &amp;gt; 0&lt;/li&gt;
&lt;li&gt;Tổng các giá trị f(x) bằng 1, trong trường hợp này X liên tục nên tích phân trên toàn miền X bằng 1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tới đây ta có được hai cách mô tả về xác suất: Bảng phân phối xác suất (cho biến ngẫu nhiên rời rạc) và Hàm mật độ xác suất (cho biến ngẫu nhiên liên tục). Trong tiếng Anh, hàm mật độ xác suất được dịch là &lt;strong&gt;Probability Density Function&lt;/strong&gt;, tên này được viết tắt nghe rất hay: &lt;strong&gt;PDF&lt;/strong&gt;. Còn Bảng phân phối xác suất thì mình ít thấy người ta dịch thuần qua tiếng Anh là Table of Probability Distribution (mặc dù search google vẫn ra). Người ta hay gọi hoặch toẹt luôn là &lt;strong&gt;Probability Density Function of X&lt;/strong&gt;, rồi nói X là biến ngẫu nhiên rời rạc. Một cách gọi khác ngắn hơn cũng thường dùng là &lt;strong&gt;Probability Mass Function&lt;/strong&gt;. &amp;ldquo;Mass&amp;rdquo; ý chỉ &amp;ldquo;đống&amp;rdquo; giá trị rời rạc.&lt;/p&gt;

&lt;p&gt;Ở đây, dễ có sự lẫn lộn giữa &lt;strong&gt;Probability Distribution&lt;/strong&gt; với &lt;strong&gt;Probability Density/Mass Function&lt;/strong&gt;. Nhớ rằng &lt;strong&gt;Probability Density/Mass Function&lt;/strong&gt; là một cái riêng, cái cụ thể của &lt;strong&gt;Probability Distribution&lt;/strong&gt;. Chỉ khi nào &lt;strong&gt;Probability Distribution&lt;/strong&gt; thỏa 2 điều kiện  mới được gọi là &lt;strong&gt;Probability Density/Mass Function&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Giá trị P(X=x&lt;sub&gt;0&lt;/sub&gt;) lớn hơn 0&lt;/li&gt;
&lt;li&gt;Tổng P(X) trên toàn miền giá trị X bằng 1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Xong, hết phần 1. Phần 2 sẽ nói xác suất trong ngữ cảnh học máy.&lt;/p&gt;

&lt;h2 id=&#34;tham-khảo&#34;&gt;Tham khảo&lt;/h2&gt;

&lt;p&gt;[1] Phan Huy Khải, Các bài toán tổ hợp. Nhà xuất bản giáo dục&lt;/p&gt;

&lt;p&gt;[2] Wikipedia, &lt;a href=&#34;https://en.wikipedia.org/wiki/Frequentist_probability&#34;&gt;https://en.wikipedia.org/wiki/Frequentist_probability&lt;/a&gt;, phần Alternative views&lt;/p&gt;

&lt;p&gt;[3] Bishop, Christopher M. &amp;ldquo;Pattern recognition.&amp;rdquo; Machine Learning 128 (2006): 1-58.&lt;/p&gt;

&lt;p&gt;[4] Slide Xác suất thống kê - Trường Đại học Bách Khoa TP.HCM&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>