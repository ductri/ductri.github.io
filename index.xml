<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine learning</title>
    <link>/</link>
    <description>Recent content on Machine learning</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>vi</language>
    <lastBuildDate>Wed, 01 Jan 2020 20:37:00 +0700</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Note: Why Entropy involves logarithm</title>
      <link>/posts/why_entropy_log/</link>
      <pubDate>Wed, 01 Jan 2020 20:37:00 +0700</pubDate>
      
      <guid>/posts/why_entropy_log/</guid>
      <description>Entropy of a random variable $X$ with pdf $P(x)$ is given by: $H(x) = -\mathbb{E}_{x \sim P}[\log P(x)]$
Why the $log$ function but any other decreasing functions, $\frac{1}{x}$ for instance?
If we have two independent random variable $x$ and $y$, then the information gained from observing both of them should be the sum of that gained from each of them separately. Therefore:
$$H(x,y) = H(x) + H(y)$$ where $$p(x, y) = p(x)p(y)$$</description>
    </item>
    
    <item>
      <title>Bug sharing</title>
      <link>/posts/bugs-sharing/</link>
      <pubDate>Sat, 14 Dec 2019 16:36:00 +0700</pubDate>
      
      <guid>/posts/bugs-sharing/</guid>
      <description>1. Use pandas.series as a list x = pd.Series(range(10)) y = list(range(10)) del x[5] del y[5] y[5] → 6 x[5] → raise Key Error exception  2. Use pandas.read_csv and pandas.to_csv Sometimes, after to_csv, you reload the file with read_csv, it raises exception:
ParserError: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.  This occurs because there is a text column which contains endline characters.</description>
    </item>
    
    <item>
      <title>Cuộc sống đầy cạm bẫy</title>
      <link>/posts/trap_everywhere/</link>
      <pubDate>Sat, 08 Jun 2019 15:26:00 +0700</pubDate>
      
      <guid>/posts/trap_everywhere/</guid>
      <description>Tập hợp lại những lỗi mình đã từng gặp, hoặc do ngu hoặc do ngu và lười.
Có lỗi khá hiển nhiên, có lỗi rất khó thấy, có lỗi của do mình thiệt, có lỗi do người ta, có lỗi chẳng làm chết 1 con ruồi, có lỗi làm mình mất mấy ngày trời, có lỗi được coi là bug, có lỗi &amp;hellip; chỉ đơn thuần là lỗi.
1. Tokenize lại string đã được tokenize Mô tả Lỗi này xảy ra khi mình code service B dùng lại output của thèn service A.</description>
    </item>
    
    <item>
      <title>Cách vượt qua một cuộc tình đổ vỡ</title>
      <link>/posts/tf-vs-pytorch/</link>
      <pubDate>Wed, 27 Mar 2019 20:50:00 +0700</pubDate>
      
      <guid>/posts/tf-vs-pytorch/</guid>
      <description>Mình với em gắn bó được đâu đó hơn 1 năm. Yêu đương thì cũng có lúc lục đục, mình cũng đã cố gắng nhẫn nhịn nhiều, nhưng em cứ ko chịu thay đổi. Đến lúc chịu thay đổi thì thay đổi quá tệ. Thôi thì đành chia tay, dù cũng xót lắm, yêu được hơn 1 năm lận mà, bao nhiêu kỷ niệm với nhau :(. Cũng suy sụp lắm chớ, nhưng mà giờ thì ổn rồi, nên nay mình viết bài chia sẽ trải nghiệm của mình trên chặng đường hồi phục sau mối tình đổ vở.</description>
    </item>
    
    <item>
      <title>Expectation Maximization</title>
      <link>/posts/expectation-maximization/</link>
      <pubDate>Fri, 12 Oct 2018 21:04:00 +0700</pubDate>
      
      <guid>/posts/expectation-maximization/</guid>
      <description>Khi ta có crush :))).
Một ngày nọ, khi bạn nhận ra là bạn have a crush on someone. Bạn bắt đầu tương tư, mơ tưởng về ngày crush nhận ra và đáp lại tấm chưng tình của bạn. Bạn mất ăn mất ngủ vì crush. Cảm xúc đó, một cách tự nhiên, sẽ thôi thúc bạn, bằng mọi cách tiếp cận crush của mình. Bạn bắt chuyện, tán dóc, nói nhảm, chọc cười, làm đủ mọi trò, cố gắng có mặt ở những nơi có crush xuất hiện, &amp;hellip; Tất cả chỉ bởi vì một mục đích: bạn đang âm thầm thu thập thông tin từ crush (observed data), để trả lời cho câu hỏi lớn nhất mà bao người có crush như bạn cũng thắc mắc: Liệu crush có thích bạn không ?</description>
    </item>
    
    <item>
      <title>Làm sao máy hiểu được hình số 1</title>
      <link>/posts/my-view-about-ml/</link>
      <pubDate>Sat, 26 May 2018 12:24:58 +0700</pubDate>
      
      <guid>/posts/my-view-about-ml/</guid>
      <description>Một cái hình 10*10 có thể được ánh xạ thành 1 vector 100 chiều. Coi như máy nhìn cái hình như là nó đang được input vào 1 vector 100 chiều. Hình số 1 có thể có rất nhiều biến thể: đổi 1 pixel từ trắng sang đen, dịch sang trái 1 pixel, xoay cái hình 1 độ, &amp;hellip; những thay đổi này với con người là cực nhỏ, con người vẫn hiểu đây là hình chứa số 1.</description>
    </item>
    
    <item>
      <title>[Suy nghĩ tào lao] Static computational graph của Tensorflow</title>
      <link>/posts/static-vs-dynamic-graph/</link>
      <pubDate>Sun, 21 Jan 2018 13:03:43 +0700</pubDate>
      
      <guid>/posts/static-vs-dynamic-graph/</guid>
      <description>Từ nay, mấy bài có tag [Suy nghĩ tào lao] là mấy bài viết xàm xàm, không có dựa trên lý thuyết, sách vở gì cả.
Đặt vấn đề Gần đây, một em tên là Pytorch, bước chân vào giới showbiz với mục tiêu cạnh tranh ngang ngữa đàn anh đã có tiếng Tensorflow. Tất nhiên để cạnh tranh được thì em Pytorch, ngoài việc có phải có chất giọng riêng, kèm với &amp;ldquo;đội ngũ quản lý&amp;rdquo; chuyên nghiệp (Twitter, NVIDIA, SalesForce, ParisTech, CMU, Digital Reasoning, INRIA, ENS) đứng sau chống đỡ, làm marketing, &amp;hellip;, thì bản thân ẻm cũng phải có biệt tài đặc biệt gì đó.</description>
    </item>
    
    <item>
      <title>Tản mạn về chuyện học học máy</title>
      <link>/posts/tan-man-ve-chuyen-hoc-hoc-may/</link>
      <pubDate>Sat, 28 Oct 2017 10:14:29 +0700</pubDate>
      
      <guid>/posts/tan-man-ve-chuyen-hoc-hoc-may/</guid>
      <description>Ngày xưa học toán, điều quan trọng nhất khi đọc 1 lời giải là làm sao hiểu được cái ẩn ý khốn nạn được giấu trong cách giải. Hiểu được cái đó mới hy vọng áp dụng cho các bài khác.
Có bữa nghe một thầy chia sẻ, hiểu làm sao giải thuật chạy được là tốt, nhưng cốt cán hơn là hiểu được &amp;ldquo;the idea behind the algorithm&amp;rdquo;.
Và mấy cái ẩn ý nó thường trừu tượng và có gì đó hơi tiết lý.</description>
    </item>
    
    <item>
      <title>Bias và Variance trong học máy, cụ thể là gì ? (P1)</title>
      <link>/posts/bias-variance/</link>
      <pubDate>Sat, 23 Sep 2017 22:33:18 +0700</pubDate>
      
      <guid>/posts/bias-variance/</guid>
      <description>Chữ bias này xuất hiện khá nhiều khi nói về machine learning, một ví dụ gần đây là status trên FB của lão Yann LeCun. Ngày xưa gặp chữ bias trước, nghe dịch là &amp;ldquo;lệch&amp;rdquo;, hoặc &amp;ldquo;chệch&amp;rdquo;, đáng sợ hơn nữa là &amp;ldquo;thiên vị&amp;rdquo;. Lúc học ML, thầy cứ nói các giải thuật học máy phải có bias, không có là nó không &amp;ldquo;học&amp;rdquo; được. Nghe cứ như triết học, ko hiểu chút gì.</description>
    </item>
    
    <item>
      <title>Gradient - Cái giống khó hiểu :3</title>
      <link>/posts/gradient-what-the-fuck-is-this/</link>
      <pubDate>Sun, 28 May 2017 22:19:54 +0700</pubDate>
      
      <guid>/posts/gradient-what-the-fuck-is-this/</guid>
      <description>Gradient Descent là cụm từ được nghe rất nhiều khi học về MLP, Neuron Network, hay CNN. Quen em nó nhau lâu rồi, nhìn mặt nhau miết, tưởng mình đã hiểu nhau, mà tới nay mới vẽ lẽ vừa không hiểu và vừa hiểu sai em nó quá trời&amp;hellip; Nay nhờ hoàn cảnh đưa đẩy mà mình hiểu thêm được em nó chút xíu.
Gradient - em nó là ai ?</description>
    </item>
    
    <item>
      <title>Xác suất trong học máy (Phần 2)</title>
      <link>/posts/xac-suat-trong-hoc-may-phan2/</link>
      <pubDate>Sun, 19 Mar 2017 10:19:05 +0700</pubDate>
      
      <guid>/posts/xac-suat-trong-hoc-may-phan2/</guid>
      <description>Xác suất là một phân ngành trong toán học, mà theo giáo sư toán Arthur Benjamin (được biết đến như một nhà ảo thuật toán &amp;ldquo;Mathemagician&amp;rdquo;) đề nghị trong bài TED talk của ông: mọi người nên học xác suất và thống kê trước khi học giải tích.
Trong lĩnh vực IA, nhiều giải thuật có liên quan đến lý thuyết xác suất như: Hidden Markov Model, Bayesian Network, chứng minh ưu điểm của Random Forrest,&amp;hellip; Việc hiểu căn bản lý thuyết xác suất, đặc biệt là hiểu các định nghĩa hay dùng là rất cần thiết cho việc hiểu bản chất vấn đề và cách giải quyết trong các giải thuật học máy.</description>
    </item>
    
    <item>
      <title>Xác suất trong học máy (Phần 1)</title>
      <link>/posts/xac-suat-trong-hoc-may/</link>
      <pubDate>Sat, 18 Mar 2017 00:37:24 +0700</pubDate>
      
      <guid>/posts/xac-suat-trong-hoc-may/</guid>
      <description>Trong bài này, mình sẽ tổng hợp lại các khái niệm cơ bản trong xác suất, các hướng tiếp cận xác suất. Đặc biệt, mình sẽ giải thích tên gọi một số loại xác suất hay được nhắc tới trong học máy. Mình viết hoàn toàn tiếng việt, chỗ nào chen dô tiếng anh chỉ là thuật ngữ, để ai đọc sách tiếng anh dễ nhận biết.
Cá nhân mình thấy mình có &amp;ldquo;hơi&amp;rdquo; nắm được lý thuyết xác suất.</description>
    </item>
    
  </channel>
</rss>