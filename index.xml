<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine learning</title>
    <link>/</link>
    <description>Recent content on Machine learning</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>vi</language>
    <lastBuildDate>Tue, 30 Jun 2020 20:15:24 -0700</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tensor Decomposition in Graph Clustering</title>
      <link>/posts/tensor-decomposition-in-graph-clustering/</link>
      <pubDate>Tue, 30 Jun 2020 20:15:24 -0700</pubDate>
      
      <guid>/posts/tensor-decomposition-in-graph-clustering/</guid>
      <description>This is my very first technical report at Oregon State University. It was the final project report of course Matrix Analysis. This version has been undergone minor amendments after grading 90/100.
Abstract  Tensor decomposition has been propular for decades because of its distinctive characteristics compared to matrix. There are many works dedicated to exploiting tensor potential through various areas, particularly in graph clustering. However, there is a lack of a clear connection between traditional clustering and tensor-based methods.</description>
    </item>
    
    <item>
      <title>Bug sharing</title>
      <link>/posts/bugs-sharing/</link>
      <pubDate>Sat, 14 Dec 2019 16:36:00 +0700</pubDate>
      
      <guid>/posts/bugs-sharing/</guid>
      <description>1. Use pandas.series as a list x = pd.Series(range(10)) y = list(range(10)) del x[5] del y[5] y[5] → 6 x[5] → raise Key Error exception 2. Use pandas.read_csv and pandas.to_csv Sometimes, after to_csv, you reload the file with read_csv, it raises exception:
ParserError: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.This occurs because there is a text column which contains endline characters. To use this pair of functions properly, explicitly indicate line_terminator by endline character, as follow:</description>
    </item>
    
    <item>
      <title>Cuộc sống đầy cạm bẫy</title>
      <link>/posts/trap_everywhere/</link>
      <pubDate>Sat, 08 Jun 2019 15:26:00 +0700</pubDate>
      
      <guid>/posts/trap_everywhere/</guid>
      <description>Tập hợp lại những lỗi mình đã từng gặp, hoặc do ngu hoặc do ngu và lười.
Có lỗi khá hiển nhiên, có lỗi rất khó thấy, có lỗi của do mình thiệt, có lỗi do người ta, có lỗi chẳng làm chết 1 con ruồi, có lỗi làm mình mất mấy ngày trời, có lỗi được coi là bug, có lỗi &amp;hellip; chỉ đơn thuần là lỗi.
1. Tokenize lại string đã được tokenize Mô tả Lỗi này xảy ra khi mình code service B dùng lại output của thèn service A.</description>
    </item>
    
    <item>
      <title>Cách vượt qua một cuộc tình đổ vỡ</title>
      <link>/posts/tf-vs-pytorch/</link>
      <pubDate>Wed, 27 Mar 2019 20:50:00 +0700</pubDate>
      
      <guid>/posts/tf-vs-pytorch/</guid>
      <description>Mình với em gắn bó được đâu đó hơn 1 năm. Yêu đương thì cũng có lúc lục đục, mình cũng đã cố gắng nhẫn nhịn nhiều, nhưng em cứ ko chịu thay đổi. Đến lúc chịu thay đổi thì thay đổi quá tệ. Thôi thì đành chia tay, dù cũng xót lắm, yêu được hơn 1 năm lận mà, bao nhiêu kỷ niệm với nhau :(. Cũng suy sụp lắm chớ, nhưng mà giờ thì ổn rồi, nên nay mình viết bài chia sẽ trải nghiệm của mình trên chặng đường hồi phục sau mối tình đổ vở.</description>
    </item>
    
    <item>
      <title>Expectation Maximization</title>
      <link>/posts/expectation-maximization/</link>
      <pubDate>Fri, 12 Oct 2018 21:04:00 +0700</pubDate>
      
      <guid>/posts/expectation-maximization/</guid>
      <description>Khi ta có crush :))).
Một ngày nọ, khi bạn nhận ra là bạn have a crush on someone. Bạn bắt đầu tương tư, mơ tưởng về ngày crush nhận ra và đáp lại tấm chưng tình của bạn. Bạn mất ăn mất ngủ vì crush. Cảm xúc đó, một cách tự nhiên, sẽ thôi thúc bạn, bằng mọi cách tiếp cận crush của mình. Bạn bắt chuyện, tán dóc, nói nhảm, chọc cười, làm đủ mọi trò, cố gắng có mặt ở những nơi có crush xuất hiện, &amp;hellip; Tất cả chỉ bởi vì một mục đích: bạn đang âm thầm thu thập thông tin từ crush (observed data), để trả lời cho câu hỏi lớn nhất mà bao người có crush như bạn cũng thắc mắc: Liệu crush có thích bạn không ?</description>
    </item>
    
    <item>
      <title>Làm sao máy hiểu được hình số 1</title>
      <link>/posts/my-view-about-ml/</link>
      <pubDate>Sat, 26 May 2018 12:24:58 +0700</pubDate>
      
      <guid>/posts/my-view-about-ml/</guid>
      <description>Một cái hình 10*10 có thể được ánh xạ thành 1 vector 100 chiều. Coi như máy nhìn cái hình như là nó đang được input vào 1 vector 100 chiều. Hình số 1 có thể có rất nhiều biến thể: đổi 1 pixel từ trắng sang đen, dịch sang trái 1 pixel, xoay cái hình 1 độ, &amp;hellip; những thay đổi này với con người là cực nhỏ, con người vẫn hiểu đây là hình chứa số 1.</description>
    </item>
    
    <item>
      <title>[Suy nghĩ tào lao] Static computational graph của Tensorflow</title>
      <link>/posts/static-vs-dynamic-graph/</link>
      <pubDate>Sun, 21 Jan 2018 13:03:43 +0700</pubDate>
      
      <guid>/posts/static-vs-dynamic-graph/</guid>
      <description>Từ nay, mấy bài có tag [Suy nghĩ tào lao] là mấy bài viết xàm xàm, không có dựa trên lý thuyết, sách vở gì cả.
Đặt vấn đề Gần đây, một em tên là Pytorch, bước chân vào giới showbiz với mục tiêu cạnh tranh ngang ngữa đàn anh đã có tiếng Tensorflow. Tất nhiên để cạnh tranh được thì em Pytorch, ngoài việc có phải có chất giọng riêng, kèm với &amp;ldquo;đội ngũ quản lý&amp;rdquo; chuyên nghiệp (Twitter, NVIDIA, SalesForce, ParisTech, CMU, Digital Reasoning, INRIA, ENS) đứng sau chống đỡ, làm marketing, &amp;hellip;, thì bản thân ẻm cũng phải có biệt tài đặc biệt gì đó.</description>
    </item>
    
    <item>
      <title>Tản mạn về chuyện học học máy</title>
      <link>/posts/tan-man-ve-chuyen-hoc-hoc-may/</link>
      <pubDate>Sat, 28 Oct 2017 10:14:29 +0700</pubDate>
      
      <guid>/posts/tan-man-ve-chuyen-hoc-hoc-may/</guid>
      <description>Ngày xưa học toán, điều quan trọng nhất khi đọc 1 lời giải là làm sao hiểu được cái ẩn ý khốn nạn được giấu trong cách giải. Hiểu được cái đó mới hy vọng áp dụng cho các bài khác.
Có bữa nghe một thầy chia sẻ, hiểu làm sao giải thuật chạy được là tốt, nhưng cốt cán hơn là hiểu được &amp;ldquo;the idea behind the algorithm&amp;rdquo;.
Và mấy cái ẩn ý nó thường trừu tượng và có gì đó hơi tiết lý.</description>
    </item>
    
    <item>
      <title>Bias và Variance trong học máy, cụ thể là gì ? (P1)</title>
      <link>/posts/bias-variance/</link>
      <pubDate>Sat, 23 Sep 2017 22:33:18 +0700</pubDate>
      
      <guid>/posts/bias-variance/</guid>
      <description>Chữ bias này xuất hiện khá nhiều khi nói về machine learning, một ví dụ gần đây là status trên FB của lão Yann LeCun. Ngày xưa gặp chữ bias trước, nghe dịch là &amp;ldquo;lệch&amp;rdquo;, hoặc &amp;ldquo;chệch&amp;rdquo;, đáng sợ hơn nữa là &amp;ldquo;thiên vị&amp;rdquo;. Lúc học ML, thầy cứ nói các giải thuật học máy phải có bias, không có là nó không &amp;ldquo;học&amp;rdquo; được. Nghe cứ như triết học, ko hiểu chút gì.</description>
    </item>
    
    <item>
      <title>Gradient - Cái giống khó hiểu :3</title>
      <link>/posts/gradient-what-the-fuck-is-this/</link>
      <pubDate>Sun, 28 May 2017 22:19:54 +0700</pubDate>
      
      <guid>/posts/gradient-what-the-fuck-is-this/</guid>
      <description>Sai, sai, sai, quá sai rồi</description>
    </item>
    
    <item>
      <title>Xác suất trong học máy (Phần 2)</title>
      <link>/posts/xac-suat-trong-hoc-may-phan2/</link>
      <pubDate>Sun, 19 Mar 2017 10:19:05 +0700</pubDate>
      
      <guid>/posts/xac-suat-trong-hoc-may-phan2/</guid>
      <description>Cuộc sống nhiều chuyện vượt quá tầm tay ta, D là một trong những cái đó. Trong khi đó, cách ta bước tiếp, cách ta đáp ứng với cái D đó (w), ta có thể kiểm soát được. Vậy nên thôi đừng hỏi vì sao D xảy ra, đừng mong chờ D sẽ thay đổi (dữ liệu training có sẳn rồi, cố định bà nó rồi), hãy điều chỉnh w sao cho phù hợp với D nhất, tìm w để tối ưu hóa p(D|w), đó mới là điều cần boăn khoăn, cần suy nghĩ.</description>
    </item>
    
    <item>
      <title>Xác suất trong học máy (Phần 1)</title>
      <link>/posts/xac-suat-trong-hoc-may/</link>
      <pubDate>Sat, 18 Mar 2017 00:37:24 +0700</pubDate>
      
      <guid>/posts/xac-suat-trong-hoc-may/</guid>
      <description>Trong bài này, mình sẽ tổng hợp lại các khái niệm cơ bản trong xác suất, các hướng tiếp cận xác suất. Đặc biệt, mình sẽ giải thích tên gọi một số loại xác suất hay được nhắc tới trong học máy.</description>
    </item>
    
  </channel>
</rss>