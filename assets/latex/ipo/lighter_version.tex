\documentclass[11pt,a4paper]{article}
\usepackage{tri_preamble}

% --------------------------------------------------------- 
% TITLE, AUTHORS, ...
% --------------------------------------------------------- 
\title{LLM Alignment Fine-tuning via Human Preference: \\
DPO vs IPO}
\author{	Tri Nguyen \\\\
        \texttt{nguyetr9@oregonstate.edu} \\\\
        }
% --------------------------------------------------------- 
% BEGIN DOCUMENT
% --------------------------------------------------------- 

\begin{document}
\maketitle
LLM alignment is a crucial step in increasing LLM's usability and safety.
This topic has been received a lot of attentions, resulting in very interesting development over the last 5 years. 
I'm particularly interested in a line of development including 3 works: 
\begin{itemize}
    \item \textbf{Reinforcement Learning from Human Feedback (RLHF)} \cite{ouyang2022training,christiano2017deep,bai2022constitutional,stiennon2020learning} from OpenAI: learning score model, using RL to perform alignment
    \item \textbf{Direct preference optimization (DPO)} \cite{rafailov2023direct} from Stanford: learning score model, alignment is ``automatically'' obtained.
    \item \textbf{IPO (or $\Psi$PO )} \cite{azar2023general} from Google DeepMind: no need for learning score model, at all :))
\end{itemize}
is a very nice buildup as one addresses the previous work's issues reducing learning process' complexity. An overview comparing the three method is shown in Figure~\ref{fig:overview}.

\begin{figure}[ht]
    \centering
    \resizebox{\linewidth}{!}{
        \includesvg[width=1\linewidth]{figures/overview.svg}
    }
    \caption{An overview comparison of RLHF, DPO, and IPO.}
    \label{fig:overview}
\end{figure}

In the first 2 works, both RLHF and DPO's idea are elegant and simple to grasp. This is however not the case for IPO. When I first read the IPO paper the whole thing screams ``ad-hoc'' and \textit{unnecessarily-complicated} ;). Their derivation is a bit confusing, their empirical loss looks counter-intuitive, they  don't even have any LLM alignment experiments.
% This is especially true when you get the DPO's perspective and try to compare it with IPO.
But the fact that IPO method works well empirically (from my own experience as well as from other papers) bothers me quite a lot.
% , which drives me to spend some effort investigating this method more carefully. 
After spending some effort to examining this method more carefully, IPO's idea turns out to be quite nice and very clever. If you don't get it from skimming through the paper (like I did), I hope this blog post can convince you to have another look at this method.



\section{LLM alignment}%
\label{sec:introduction}
Large language models after a pre-training phase on vast un-labeled datasets possesses an amazingly ability of  natural-looking text completions given prompts. However, due to the nature of unsupervised training, the resulting models also exhibit many undesired behaviors (via the generation), including being unhelpful, biased, sexist, racism, hallucination.

LLM alignment aims to address this issue by steering model's behavior toward the desired characterizations using following formulation
\begin{equation}
\label{eq:original_obj}
\maximize_{\pi} \quad  \mathop{\mathbb{E}}_{\bm{x} \sim \mathcal{D}} \left[  \mathop{\mathbb{E}}_{\bm{y} \sim \pi} \left[ s(\bm{x}, \bm{y})  - \beta \Dkl{\pi}{\pi_{\rm ref}}\right]
\right] 
\end{equation}
Here, the so-called \textit{score function} $s(\bm{x}, \bm{y})$ is assumed to produce a scalar value indicating how strongly the response $\bm{y}$ is aligned with the desired characteristic given the prompt $\bm{x}$.
We wish find a policy (a language model) $\pi$ such that it retains most of the good text-generation capability of $\pi_{\rm ref}$, and at the same time producing responses $\bm{y}$ to maximize the score function.
The balance between 2 objectives is controlled by $\beta$. A too small $\beta$ might lead to overly optimized $\pi$ that loses the generally good text generation of $\pi_{\rm ref}$, while too large $\beta$ prevents $\pi$ from adjusting toward better alignment.
The reference policy $\pi_{\rm ref}$ is can be seen as a good initialization.

An obvious barrier in solving \eqref{eq:original_obj} is the unknown $s(\bm{x}, \bm{y})$. It is very non-trivial to hand-crafting the score function $s(\bm{x}, \bm{y})$. Instead, the popular approach is to learn $s(\bm{x}, \bm{y})$ using pairwise preference data. 
In particular, the preference data includes a list of tuples $(\bm{x}, \bm{y}_1, \bm{y}_2, c)$ where given 2 possible responses $\bm{y}_1, \bm{y}_2$ given the same prompt $\bm{x}$, a human annotator provides a label $c\in \set{1,2}$ indicating which response is more aligned with the alignment objective. For instance, $c=1$ implies that $\bm{y}_1$ is preferred over $\bm{y}_2$, denoting as $\bm{y}_1 \succ \bm{y}_2$.

Different methods propose to solve \eqref{eq:original_obj} in different ways but they all share the same principle: using the preference data to estimate the unknown function $s(\bm{x}, \bm{y})$.
% Amongst several methods within this approach, the RLHF, DPO, IPO are 3 methods developing by addressing one's issues. 
% Through out the rest of the article, we consider pairwise preference data, i.e., a tuple of $(\bm{x}, \bm{y}_1, \bm{y}_2, c)$ where 2 possible responses are collected given a prompt $\bm{x}$, and the binary preference label $c\in \set{1, 2}$ indicating which response is preferred.
% The use of preference data is considered much easier to obtain in compared to demonstration dataset, where an ideal response is expected given a prompt.
% A obvious (and also key) challenge in \eqref{eq:original_obj} is the unknown score function $s(\bm{x}, \bm{y})$.
% Eventually, we wish to learn more about this score function.
As the only available supervised signal is the preference dataset, it is necessary to assume certain specification to relate the unknown score function $s(\bm{x}, \bm{y})$ and the collected pairwise preference data.
% \[
% \boxed{
% \text{Unknown score function } s(\bm{x}, \bm{y}) \quad \xleftrightarrow{\quad \text{\red certain specification}\quad } \quad \text{Preference data } (\bm{x}, \bm{y}_1, \bm{y}_2, c)
% }
% \] 
\begin{tcolorbox}[center]
    \begin{equation}
        \label{eq:general_spec}
\text{Unknown score function } s(\bm{x}, \bm{y}) \quad \xleftrightarrow{\quad \text{\red certain specification}\quad } \quad \text{Preference data } (\bm{x}, \bm{y}_1, \bm{y}_2, c)
    \end{equation} 
\end{tcolorbox}
In the following, we will go through different realizations of this relations, leading to 3 different popular techniques: RLHF, DPO, and IPO.

\section{RLHF and DPO}%
\label{sec:rlhf_and_dpo}
\subsection{RLHF}%
\label{sub:rlhf}
The structure of the optimization problem \eqref{eq:original_obj} is very much a typical RL problem, except that the score function $s(\bm{x}, \bm{y})$ is unknown. 
Naturally, if one can learn $s(\bm{x}, \bm{y})$, then an off-the-shelf RL technique can be deployed to solve \eqref{eq:original_obj}. This is the very idea proposed by RLHF.

To learn $s(\bm{x}, \bm{y})$, RLHF assumes the Bradley-Terry model \cite{} for the preference data generation, which hypothesizes that
\begin{equation}
\label{eq:bt_model}
\pr{\bm{y}_1 \succ \bm{y}_2 \mid \bm{x}} \triangleq \pr{c=1 \mid \bm{x}, \bm{y}_{1}, \bm{y}_{2}} = \dfrac{\exp(s(\bm{x}, \bm{y}^{1}))}{\exp(s(\bm{x}, \bm{y}^{1})) + \exp(s(\bm{x}, \bm{y}^{2}))}
= \sigma(s(\bm{x}, \bm{y}_{1}) - s(\bm{x}, \bm{y}_{2}))
\end{equation} 
Since the score function gives a higher score for a better aligned response, it is more likely that that response is preferred over the other.

We can see that this assumption specifies the relation as mentioned in \eqref{eq:general_spec}.
If we further assume the true $s(\bm{x}, \bm{y})$ belong to certain class of deep neural network (such as a LLM-type network), then the true score function can be recovered using MLE:
\begin{align}
\label{eq:score_learning}
& \minimize_{\boldsymbol \theta} \quad -\mathop{\mathbb{E}}_{(\bm{x}, \bm{y}_1, \bm{y}_2), c \; \sim \mathcal{D}} \left[\mathcal{L}_{\rm logistic}(\sigma(s_{\boldsymbol \theta}(\bm{x}, \bm{y}_1) - s_{\boldsymbol \theta}(\bm{x}, \bm{y}_2)), c)\right]
\end{align}
where
$ \mathcal{L}_{\rm logistic}(p, c) = \mathbb{I}[c=1] \log p + \mathbb{I}[c=2] \log \left( 1-p\right)$.  In practice, $s_{\boldsymbol \theta}(\bm{x}, \bm{y})$ is parameterized using another large language model \cite{}. It is argued that pre-trained large language model has the ability to understand texts and hence play as a judge \cite{}.
After attaining the optimal solution $\boldsymbol \theta^{\star }$ to \eqref{eq:score_learning}, the alignment fine-tuning is performed by plugging $s_{\boldsymbol \theta^{\star }}$ into \eqref{eq:original_obj} and an off-the-shelf RL techniques, such as PPO \citep{schulman2017proximal} is invoked to solve \eqref{eq:original_obj}.

This approach while being straightforward, suffers from several technical difficulties: 
\begin{itemize}
    \item A 2-stage training pipeline is complex and prone to error accumulation.
    \item The use of RL require intensive and careful hyperparameter tuning.
\end{itemize}
These challenges are the main motivations for the development of DPO.
\subsection{DPO}%
\label{sub:dpo}
DPO improves upon RLHF by eliminating the RL step. In particular, DPO's authors realizes that under the same preference model (BT model), the relation between preference label and score function only depends on the relative difference in score, not the absolute score values. This enables them to use a clever trick to re-parameterize the score function via an optimal policy, effectively eliminate the needs of deploying RL.

Notice that the objective in \eqref{eq:original_obj} can be expressed as:
\begin{align*}
\mathop{\mathbb{E}}_{\bm{x}} \left[ \mathop{\mathbb{E}}_{\bm{y} \sim \pi(\cdot \mid \bm{x})} \left[  s(\bm{y}, \bm{x}) - \beta \Dkl{\pi}{\pi_{\rm ref}}\right] \right] 
&= \mathop{\mathbb{E}}_{\bm{x}} \left[  \Dkl{\pi}{\dfrac{1}{Z(\bm{x})} \exp(\beta^{-1} s(\bm{y}, \bm{x})) \pi_{\rm ref}(\bm{y} \mid \bm{x})} \right] + \text{const}
\end{align*}
where $Z(\bm{x})$ is an intractable normalizing factor.
As KL-divergence reaches minimum value at $0$, this expression suggests an optimal solution $\pi^{\star }$ as
% Let $\pi^{\star }$ be an optimal solution of \eqref{eq:original_obj}, then
\[
\pi^{\star }(\bm{y} \mid \bm{x}) = \dfrac{1}{Z(\bm{x})} \pi_{\rm ref}(\bm{y} \mid \bm{x}) \exp \left( \beta^{-1}  s^{\natural}(\bm{y}, \bm{x}) \right),
\] 
which equivalently implies
\[
s(\bm{y}, \bm{x}) = \beta \log \dfrac{\pi^{\star }(\bm{y} \mid \bm{x})}{\pi_{\rm ref}(\bm{y} \mid \bm{x})} + \beta \log Z(\bm{x})
\] 
This identity establishes a relation between an arbitrary score function $s(\bm{x}, \bm{y})$ and a corresponding optimal policy $\pi^{\star }(\bm{y}\mid \bm{x})$ with respect to that score function. It is not very useful by itself due to the intractable factor $Z(\bm{x})$. However, the relative score difference, which is all that matters, is \textbf{independent} of $Z(\bm{x})$:
\begin{equation}
    \label{eq:qoblq}
s(\bm{x}, \bm{y}_1) - s(\bm{x}, \bm{y}_2) = \beta \log \dfrac{\pi^{\star }(\bm{y}_1\mid \bm{x})}{\pi_{\rm ref}(\bm{y}_1 \mid \bm{x})} - \beta \log \dfrac{\pi^{\star }(\bm{y}_2\mid \bm{x})}{\pi_{\rm ref}(\bm{y}_2 \mid \bm{x})}.
\end{equation} 
To be more concise, define
\[
h_{\boldsymbol \theta}(\bm{x}, \bm{y}_1, \bm{y}_2)=\beta \log \dfrac{\pi_{\boldsymbol \theta }(\bm{y}_1\mid \bm{x})}{\pi_{\rm ref}(\bm{y}_1 \mid \bm{x})} - \beta \log \dfrac{\pi_{\boldsymbol \theta }(\bm{y}_2\mid \bm{x})}{\pi_{\rm ref}(\bm{y}_2 \mid \bm{x})}.
\]
and equation~\eqref{eq:qoblq} can be shorten as
\begin{equation}
\label{eq:h_theta_cond}
s(\bm{x}, \bm{y}_1) - s(\bm{x}, \bm{y}_2) = h_{\boldsymbol \theta}(\bm{x}, \bm{y}_1, \bm{y}_2).
\end{equation} 
This condition plays the key in ensuring policy $\pi_{\boldsymbol \theta }$ is the optimal solution to the original alignment formulation \eqref{eq:original_obj}. Particularly, it is shown in DPO that any policy $\pi_{\boldsymbol \theta}$ satisfying condition~\eqref{eq:h_theta_cond} is an optimal solution to the alignment formula in \eqref{eq:original_obj}, up to some trivial ambiguity.

With this insight, DPO proposed to use $h_{\boldsymbol \theta}(\bm{x}, \bm{y}_1, \bm{y}_2)$ to parameterize the relative score difference between 2 responses $\bm{y}_1, \bm{y}_2$. As before, they employ BT model and use MLE to derive the loss function:
% . To be specific, instead of using arbitrary neural network to parameterize $s(\bm{x}, \bm{y})$, DPO uses: $ s(\bm{x}, \bm{y}) = \beta \log \dfrac{\pi_{\boldsymbol \theta}(\bm{y}, \bm{x})}{\pi_{\rm ref}(\bm{y}, \bm{x})}$. 
\begin{align}
\label{eq:score_learning_dpo}
& \minimize_{\boldsymbol \theta} \quad -\mathop{\mathbb{E}}_{(\bm{x}, \bm{y}_1, \bm{y}_2), c \; \sim \mathcal{D}} \left[\mathcal{L}_{\rm logistic}\left(\sigma(h_{\boldsymbol \theta}(\bm{x}, \bm{y}_1, \bm{y}_2)), c\right)\right].
\end{align}
With this parameterization, an optimal solution $\boldsymbol \theta^{\star }$ to \eqref{eq:score_learning_dpo} also gives us an optimal policy $\pi_{\boldsymbol \theta^{\star }}$ to \eqref{eq:original_obj}.

\subsection{The Common Theme}%
\label{sub:the_common_theme}
In terms of modeling, both RLHF and DPO relies on the same specification:
\begin{tcolorbox}[center]
\resizebox{\linewidth}{!}{
\begin{equation*}
\text{Unknown score function } s(\bm{x}, \bm{y}) \quad \xleftrightarrow {\red \substack{\quad \pr{c=1 \mid \bm{y}_1 ,\bm{y}_2, \bm{x}} = \sigma(s(\bm{y}_1, \bm{x}) - s(\bm{y}_2, \bm{x}))\quad \\ \text{ and }s  \in \mathcal{F}}} \quad \text{Preference data } (\bm{x}, \bm{y}_1, \bm{y}_2, c)
\end{equation*}}
\end{tcolorbox}

There are 2 factors in this specification:
\begin{itemize}
    \item The BT model is used to relate the score function and the preference data
    \item The score function is assumed to belong to certain known hypothesis class $\mathcal{F}$, either an arbitrary neural network as in RLHF, or a structured neural network as in DPO.
\end{itemize}
The combination of the two enables learning $s(\bm{x}, \bm{y})$ using preference data.

\paragraph{Drawback.} 
\begin{itemize}
    \item The preference data relies on BT model. BT model is intuitive and is successfully deployed in many domains, such as economy, however, it is still restrictive and who knows if real data generation truly follows it.
    \item The particular functional form of BT model, i.e., the sigmoid function makes it computationally difficult to model deterministic cases. Specifically, to have $\pr{c=1 \mid \bm{x}, \bm{y}_1, \bm{y}_2} \to 1$, the quantity $s(\bm{x}, \bm{y}_1) - s(\bm{x}, \bm{y}_2)$ in RLHF, or $h_{\boldsymbol \theta}(\bm{x}, \bm{y}_1, \bm{y}_2)$ in DPO needs to reach $\infty$. This behavior causes a particular detrimental effect: worsening the reward hacking in the second phase in RLHF, or overfitting in DPO where the learned policy drifting arbitrarily far away from $\pi_{\rm ref}$ regardless of $\beta$. 
    \item And subtly, the alignment fine-tuning task is cast into a score learning problem. This is in turn solved indirectly.
What we wish to estimate is in the ``logits'' space, i.e., $\widehat{h}(\bm{y}_1, \bm{y}_2, \bm{x}) \approx s(\bm{y}_1, \bm{x}) - s(\bm{y}_2, \bm{x})$ but what the criterion enforcing is in the probability space, i.e., $\sigma(\widehat{h}(\bm{y}_1, \bm{y}_2, \bm{x})) \approx \sigma(s(\bm{y}_1, \bm{x}) - s(\bm{y}_2, \bm{x}))$. 
A small mismatch in the later could result in a much large mismatch in the former estimation, which affects the ultimate alignment task. This is, to me, the major instability issue of RLHF and DPO.
\end{itemize}
And these drawback lead to the development of IPO.

\section{IPO}%
\label{sec:ipo}

The method in RLHF/DPO can be seen to boil down to a binary classification problem: Given a sample $(\bm{x}, \bm{y}_1, \bm{y}_2)$, one wish to predict if the label is $1$ or $2$. Their loss functions are very intuitive following this view.
Take DPO for example, if annotator is very certain that the label $c=1 \Leftrightarrow \bm{y}_1 \succ \bm{y}_2$, the score gap $\bm{h}_{\boldsymbol \theta}(\bm{y}_1, \bm{y}_2, \bm{x})$ between the 2 responses should be large.
%  should be high.
The loss functions DPO \eqref{eq:score_learning_dpo} promotes this by increasing the estimated likelihood $\sigma(h_{\boldsymbol \theta}(\bm{y}_1, \bm{y}_2, \bm{x}))$. Similar mechanism can be observed in RLHF as well.

This very intuitive idea might have spoiled me so much that when I first encountered the IPO paper, every step of it felt wrong.
Let me articulate. Let's look at their final loss:
\begin{alignat*}{2}
    & \minimize_{\boldsymbol \theta} \quad && \mathop{\mathbb{E}}_{\bm{x}, \bm{y}_w, \bm{y}_{\ell }} \left[  (h_{\boldsymbol \theta}(\bm{y}_{w}, \bm{y}_{\ell }, \bm{x}) - 0.5)^2 \right],
\end{alignat*}
where notation $\bm{y}_{w}, \bm{y}_{\ell }$ is an equivalent form of using the label $c\in\set{1,2}$ and imply $\bm{y}_w \succ \bm{y}_{\ell }$. This is nothing special and everyone uses it.

Now come to the unintuitive parts:
\begin{itemize}
    \item Why regression? How can it turn a classification to a regression problem?
    \item And why regressing toward $0.5$?
    \item Furthermore, IPO claims that they even \textbf{don't need to learn the score function}. Then how do they do alignment at all!!!
\end{itemize}
These baffling points have been in my mind for a while. Unfortunately, their derivation in the paper didn't help much.
Recently, I have some troubles comparing against this IPO method, and thus I've spent some more time on it. Turned out, their solution is more elegant than I thought.

% \subsection{The motivation}%
% \label{sub:the_motivation}
% Let's start slowly with their motive: Fixing DPO's overfitting issue.
% When viewing the score learning problem as classification problem, we unintentionally undermine one key difference: in classification problem, a common approach is to use function $f(\bm{x})$ that outputs logits, and use certain normalizing functions such as \texttt{sigmoid} or \texttt{softmax} to produce valid probability. As such, we don't care about the prediction's logits as long as the probability prediction is correct. The logits and the probability predictions are apparently correlated, however their estimation errors could be widely different in magnitude.


% This criterion is very counter-intuitive. Specifically,
% \begin{itemize}
%     \item Turning a classification-based to a regression-based objective. This is ahhhhhh. But this is just because of my view, nothing wrong here.
%     \item Why regressing toward $0.5$? What special about $0.5$? And why regressing all possible samples toward $0.5$? Does it mean that we treat preference evenly among all samples? While in comparison to \eqref{eq:unified_loss}, the gap $\bm{q}_{\boldsymbol \theta}(\bm{y}_{w}, \bm{x}) - q_{\boldsymbol \theta}(\bm{y}_l, \bm{x})$ to be as large as possible.
% \end{itemize}
% Well, I have some answered, but not all sadly.
% So intuition does not help here. Let's dive into the technical development to see if that helps.
\subsection{IPO's Loss Derivation}%
\label{sub:ipo_s_loss_derivation}
% \textit{While I feel writing down the derivation step by step would enable identifying the key steps, it is okay to skip this section to jump to the conclusion.}
% In this section, I attempted to follow IPO's derivation, make the whole thing a bit more rigorous to identify the key step. 

Note that IPO's goal is to perform the alignment fine-tuning task via the same formulation \eqref{eq:original_obj} as RLHF and DPO do. Unlike previous methods where there is no structure on $s(\bm{x}, \bm{y})$, IPO assumes the following (unknown) score function:
\begin{equation}
\label{eq:s_v}
s^{\natural}(\bm{x}, \bm{y}) =  \mathop{\mathbb{E}}_{\bm{y}' \sim \mu(\cdot \mid \bm{x})} \left[ v(\bm{y}, \bm{y}', \bm{x}) \right].
\end{equation} 
The scalar-valued function $v(\bm{y}, \bm{y}', \bm{x})\in [0,1]$ denotes the probability of $\bm{y} \succ \bm{y}'$. This score function can be seen as a quantification of how a response $\bm{y}$ is preferred on average with respect to a predefined policy $\mu$. For now, $\mu$ is just an arbitrary policy.
Using this score function, the alignment task is to find a policy $\pi^{\star }$ within the proximity of $\pi_{\rm ref}$ and be better than policy $\mu$ as much as possible. In essence, the goal is quite the same, except that we have certain structure on the score function to work with. As we will see, using this unknown score function, IPO cleverly perform alignment without explicitly learning it.

Note that in the paper, IPO denote $v(\bm{y}, \bm{y}', \bm{x}) \triangleq p( \bm{y} \succ \bm{y}' \mid \bm{x} )$. Although using $p( \bm{y} \succ \bm{y}' \mid \bm{x} )$ can be intuitive, I feel much comfortable using $v(\bm{y}, \bm{y}', \bm{x})$ to explicitly remind myself that it is nothing but a special unknown function. As $v$ denote a probability of preference, we require it to satisfy:
\begin{subequations}
\begin{align}
&v(\bm{y}, \bm{y}', \bm{x}) \in [0, 1], \; \forall  \bm{y}, \bm{y}', \bm{x} \label{eq:ipo_boundedness}\\
&v(\bm{y}, \bm{y}, \bm{x}) = 0.5, \; \forall  \bm{y}, \bm{x} \label{eq:ipo_self_compare}\\
&v(\bm{y}, \bm{y}', \bm{x}) = 1 - v(\bm{y}', \bm{y}, \bm{x}) \label{eq:ipo_symmetric}
\end{align}
\end{subequations}
Condition \eqref{eq:ipo_boundedness} is to ensure valid probabilities, condition \eqref{eq:ipo_self_compare} and \eqref{eq:ipo_symmetric} are to enforce the physical meaning of pairwise preference.
With these notations, IPO's specification on the relation between preference labels and score function is as follows
\begin{tcolorbox}[center]
\resizebox{\linewidth}{!}{
\begin{equation*}
\text{Unknown score function } s(\bm{x}, \bm{y}) \quad \xleftrightarrow {\red \substack{\quad \pr{c=1 \mid \bm{y}_1, \bm{y}_2, \bm{x}} = v(\bm{y}_1, \bm{y}_2, \bm{x}),\\ s(\bm{x}, \bm{y}) = \mathop{\mathbb{E}}_{\bm{y}' \sim \mu(\cdot \mid \bm{x})}[v(\bm{y}, \bm{y}', \bm{x})], \quad \\ \textbf{ and } \bm{y}_1, \bm{y}_2 \sim \mu(\cdot \mid \bm{x})}} \quad \text{Preference data } (\bm{x}, \bm{y}_1, \bm{y}_2, c)
\end{equation*}}
\end{tcolorbox}
Notice that $v$ is still unknown but IPO don't aim to learn $v$. The new, and important requirement is that the pair of responses are assumed to be drawn from the same behavior policy $\mu$. Recall that $\mu$ plays a role in defining the score function. This seems to be a limitation. However, it is possible that particular choice of $\mu$ does not change the optimal policy much (or at all).

% Similar to DPO, the optimal policy to IPO's objective in \eqref{eq:original_obj} is
% \[
% \pi^{\star }(\bm{y} \mid \bm{x}) = \dfrac{1}{Z(\bm{x})} \pi_{\rm ref}(\bm{y} \mid \bm{x}) \exp \left( \beta^{-1} \mathop{\mathbb{E}}_{\bm{y}' \sim \mu} \left[ v(\bm{y}, \bm{y}', \bm{x}) \right] \right),
% \] 
% which enables the trick originally made by DPO by comparing 2 responses:
% \begin{equation}
% \label{eq:ipo_opt_cond}
% h^{\star }(\bm{x}, \bm{y}_1, \bm{y}_2)
% = \mathop{\mathbb{E}}_{\bm{y} \sim \mu}\left[v(\bm{y}_1, \bm{y}, \bm{x}) - v(\bm{y}_2, \bm{y}, \bm{x}) \right],
% \end{equation} 
% where $q^{\star }(\bm{y}, \bm{x}) = \beta \log \pi^{\star }(\bm{y}\mid \bm{x}) - \beta \log \pi_{\rm ref}(\bm{y} \mid \bm{x})$.

As in DPO, IPO enforces the condition \eqref{eq:h_theta_cond} to ensure optimal policy:
\[
h_{\boldsymbol \theta}(\bm{x}, \bm{y}_1, \bm{y}_2) = s(\bm{x}, \bm{y}_1) - s(\bm{x}, \bm{y}_2) = \mathop{\mathbb{E}}_{\bm{y} \sim \mu}[v(\bm{y}_1, \bm{y}, \bm{x}) - v(\bm{y}_2, \bm{y}, \bm{x})],
\]

% Let $ q_{\boldsymbol \theta}(\bm{y}, \bm{x}) = \beta \log \pi_{\boldsymbol \theta}(\bm{y}\mid \bm{x}) - \beta \log \pi_{\rm ref}(\bm{y} \mid \bm{x})$.
% All we want is to enforce the equality \eqref{eq:ipo_opt_cond} for all $\bm{y}_1, \bm{y}_2$ with respect to $q_{\boldsymbol \theta}$ , and one way to realize it is using squared loss
which can be realized by solve an optimization problem:
\begin{alignat}{2}
    \label{eq:ipo_first_square}
    & \minimize_{\boldsymbol \theta} \quad && \mathop{\mathbb{E}}_{\bm{x}, \bm{y}_1, \bm{y}_2\sim \mu} \left[ \left(  h_{\boldsymbol \theta}(\bm{x}, \bm{y}_1, \bm{y}_2)  - \mathop{\mathbb{E}}_{\bm{y} \sim \mu}\left[v(\bm{y}_1, \bm{y}, \bm{x}) - v(\bm{y}_2, \bm{y}, \bm{x}) \right] \right) ^2\right]
\end{alignat}
Notice that the regression target is unknown. However, by exploiting 2 key factors in their specification: (i) the pair of responses are drawn from the same policy $\mu$ which is also used in defining the score function, and (ii) the structure of $h_{\boldsymbol \theta}(\bm{x}, \bm{y}_1, \bm{y}_2)$, IPO shows that optimization problem \eqref{eq:ipo_first_square} is equivalent to
\begin{alignat}{2}
    \label{eq:ipo_square_2}
    & \minimize_{\boldsymbol \theta} \quad && \mathop{\mathbb{E}}_{\bm{x}, \bm{y}_1, \bm{y}_2\sim \mu} \left[ \left(  h_{\boldsymbol \theta}(\bm{x}, \bm{y}_1, \bm{y}_2)  - v(\bm{y}_1, \bm{y}_2, \bm{x}) \right) ^2\right].
\end{alignat}
I have a more rigorous derivation in the pdf attachment proving this part. The derivation is not super complex but it certainly is not trivial. I am very baffled as how they came up with this observation. 

The equivalent optimization \eqref{eq:ipo_square_2} is tremendously useful since the target in \eqref{eq:ipo_square_2} can be approximated as
\[
v(\bm{y}_1, \bm{y}_2, \bm{x}) \approx c-1
\]
% THIS PART MIGHT REVEAL TOO MUCH ONTO MY IDEA!
% COMMENT OUT THIS
\textit{(\text{In IPO paper, the estimation is $c$. The difference stems from the definition of label set $\set{0,1}$ vs $\set{1,2}$})}.
We can show that it is an unbiased estimator using the fact $\pr{c=1 \mid \bm{x}, \bm{y}_1, \bm{y}_2} = v(\bm{y}_1, \bm{y}_2, \bm{x})$:
\begin{align*}
v(\bm{y}_1, \bm{y}_2, \bm{x}) - \mathop{\mathbb{E}}[(c-1)] 
&= v(\bm{y}_1, \bm{y}_2, \bm{x}) -\pr{c=1 \mid \bm{x}, \bm{y}_1, \bm{y}_2} - 2\pr{c=2 \mid \bm{x}, \bm{y}_1, \bm{y}_2}+1 \\
&=v(\bm{y}_1, \bm{y}_2, \bm{x}) - \pr{c=2 \mid \bm{x}, \bm{y}_1, \bm{y}_2} + 1 \\
&=v(\bm{y}_1, \bm{y}_2, \bm{x}) - \pr{c=1 \mid \bm{x}, \bm{y}_1, \bm{y}_2}  \\
&=v(\bm{y}_1, \bm{y}_2, \bm{x}) - v(\bm{y}_1, \bm{y}_2, \bm{x})  = 0,
\end{align*} 
and the variance is also getting smaller when more labeled data is obtained.

With such approximation, IPO proposed the following optimization problem:
\begin{alignat}{2}
    & \minimize_{\boldsymbol \theta} \quad && \mathop{\mathbb{E}}_{\bm{x}, \bm{y}_1, \bm{y}_2\sim \mu, c} \left[ \left(  h_{\boldsymbol \theta}(\bm{x}, \bm{y}_1, \bm{y}_2)  - c+1 \right) ^2\right].
\end{alignat}
This can be further simplified when considering symmetrical property of the preference data. For instance, a sample $(\bm{y}, \bm{y}', \bm{x}, 1)$ induces another sample $(\bm{y}', \bm{y}, \bm{x}, 2)$.
Exploiting this structure and denote $\bm{y}_w, \bm{y}_\ell $ based on the label $c$ such that $\bm{y}_w \succ \bm{y}_\ell $, the previous problem is equivalent to
\begin{alignat}{2}
    & \minimize_{\boldsymbol \theta} \quad && \mathop{\mathbb{E}}_{\bm{x}, \bm{y}_w, \bm{y}_{\ell }\sim \mu} \left[ \left(  h_{\boldsymbol \theta}(\bm{x}, \bm{y}_w, \bm{y}_\ell )  - 0.5 \right) ^2\right].
\end{alignat}

\subsection{Take-Home Points}%
\label{sub:take_home_points}
To answer the questions in the beginning on this section:
\begin{itemize}
    \item How can it turn a classification to a regression problem? It does not. RLHF/DPO learn the underling model by casting the problem as a classification problem. IPO does not learn any underlying model, and hence there is no need of any classification problem. The regression problem stems from the enforcing condition \eqref{eq:h_theta_cond} for the optimal policy.
    \item Why regressing toward $0.5$? This fixed number $0.5$ is a result to a quite rough approximation $\pr{c=1\mid \bm{x}, \bm{y}_1, \bm{y}_2} = v(\bm{y}_1, \bm{y}_2, \bm{x}) \approx c-1$. 
    \item Lastly, as IPO's working directly on the logit space, it seems to be more robust compared to previous methods.

% COMMENT OUT THIS
{\blue We could do better!} The drawback is that this estimation does not exploit the correlation between samples in the datasets as each $v(\bm{y}_1, \bm{y}_2, \bm{x})$ is estimated independently.
A better approach is to do estimation using neural network so that we can amortize all samples in the dataset. Toward this direction, we can assume $v \in \mathcal{F}$ where $\mathcal{F}$ is a class of neural network, possibly with sigmoid as the last layer  to enforce $[0,1]$ constraint.
The formulation could be something like
\begin{alignat}{2}
    & \minimize_{\boldsymbol \theta_1, \boldsymbol \theta_2} \quad && \mathop{\mathbb{E}}_{\bm{x}, \bm{y}_w, \bm{y}_{\ell }\sim \mu} \left[ \left(  h_{\boldsymbol \theta_1}(\bm{x}, \bm{y}_w, \bm{y}_\ell )  - \sigma(v_{\boldsymbol \theta_2}(\bm{y}_w, \bm{y}_\ell , \bm{x})) \right) ^2 - \gamma \log (\sigma(v_{\boldsymbol \theta_2}(\bm{y}_w, \bm{y}_\ell , \bm{x})))\right].
\end{alignat}

By doing this,
\begin{itemize}
    \item {\green Learning the ``preference function'' $v(\bm{y}_1, \bm{y}_2, \bm{x})$ to improve the quality of the regression target, instead of using a fixed target of $0.5$}
    \item {\green Avoiding the overfitting issue of DPO. This is because the regression is acting on probability space, \textbf{not the logits space}}.
    \item {\green Do not rely on BT model when modeling preference data}
    \item {\red But possibly doubling computation, might need some careful design}
\end{itemize}
Similar but not same idea has started to be considered (not successfully yet):
\begin{itemize}
    \item Google Deepmind \citep{fisch2024robust}, rejected at NeurIPS 2024.
\end{itemize}
\item What about magnitude of the regression target? Should it always be within $[0,1]$? It could be okay, as the magnitude can be tune by $\beta$ eventually.
\item What about noise? How does it behave in case of label noise?

\end{itemize}


\subsection{Technical Derivation}%
\begin{lemma}
    Optimization problems~\eqref{eq:ipo_first_square} and \eqref{eq:ipo_square_2} are equivalent.
\end{lemma}
\begin{proof}
Define 
\[
\bm{q}_{\boldsymbol \theta}(\bm{y}, \bm{x}) = \beta \log \dfrac{\pi_{\boldsymbol \theta}(\bm{y} \mid \bm{x})}{\pi_{\rm ref }(\bm{y} \mid \bm{x})},
\]
then $\bm{h}_{\boldsymbol \theta}(\bm{y}_1, \bm{y}_2, \bm{x})= \bm{q}_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) - \bm{q}_{\boldsymbol \theta}(\bm{y}_2, \bm{x})$.
Expanding the squared term in the objective of \eqref{eq:ipo_first_square}, the optimization problem reduces to
\begin{equation}
\label{eq:qioqli}
\min_{\boldsymbol \theta}  \mathop{\mathbb{E}}_{\substack{\bm{x},\\ \bm{y}_1, \bm{y}_2\sim \mu}} \left[ (q_{\boldsymbol \theta}(\bm{y}_1, \bm{x})- q_{\boldsymbol \theta}(\bm{y}_2, \bm{x}))^2 - 2 [q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) - q_{\boldsymbol \theta}(\bm{y}_2, \bm{x})]\mathop{\mathbb{E}}_{\bm{y} \sim \mu}\left[v(\bm{y}_1, \bm{y}, \bm{x}) - v(\bm{y}_2, \bm{y}, \bm{x}) \right]  \right]
\end{equation}
The issue lays in the unknown factor in the second term.
Now comes to the IPO's peculiar derivations. The second term can be written as
\begin{align*}
\quad \quad &\mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2 \sim \mu} \left[ [q_{\boldsymbol \theta}(\bm{y}_1, \bm{x})- q_{\boldsymbol \theta}(\bm{y}_2, \bm{x})]  \mathop{\mathbb{E}}_{\bm{y} \sim \mu}\left[ v(\bm{y}_1, \bm{y}, \bm{x}) - v(\bm{y}_2, \bm{y}, \bm{x}) \right]  \right] \\
&= \mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2, \bm{y} \sim \mu}[({\green q_{\boldsymbol \theta}(\bm{y}_1, \bm{x})} - {\blue q_{\boldsymbol \theta}(\bm{y}_2, \bm{x})}) \left({\green v(\bm{y}_1, \bm{y}, \bm{x})} - {\blue v(\bm{y}_2, \bm{y}, \bm{x})} \right)] \\
&= \underbrace{\mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2, \bm{y} \sim \mu} [{\green q_{\boldsymbol \theta}({\green \bm{y}_1}, \bm{x}) v(\bm{y}_1, \bm{y}, \bm{x})} + {\blue q_{\boldsymbol \theta}(\bm{y}_2, \bm{x}) v(\bm{y}_2, \bm{y}, \bm{x})}]}_{(*)} \\
&\qquad \qquad - \underbrace{\mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2, \bm{y} \sim \mu } [{\green q_{\boldsymbol \theta}(\bm{y}_1, \bm{x})} {\blue v(\bm{y}_2, \bm{y}, \bm{x})}+ {\blue q_{\boldsymbol \theta}(\bm{y}_2, \bm{x})} {\green v(\bm{y}_1, \bm{y}, \bm{x})}] }_{(**)}
\end{align*}
For (*), notice that the two terms are essentially the same under expectation when $\bm{y}_1, \bm{y}_2, \bm{y}$ are i.i.d.
\begin{align*}
(*) &=\mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2, \bm{y}} [q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) v(\bm{y}_1, \bm{y}, \bm{x}) + q_{\boldsymbol \theta}(\bm{y}_2, \bm{x}) v(\bm{y}_2, \bm{y}, \bm{x})] \\
&=\mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}} [q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) v(\bm{y}_1, \bm{y}, \bm{x})] + \mathop{\mathbb{E}}_{\bm{y}_2, \bm{y}} [q_{\boldsymbol \theta}(\bm{y}_2, \bm{x} ) v(\bm{y}_2, \bm{y}, \bm{x})] \\
&=\mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2} [{q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) v(\bm{y}_1, \bm{y}_2, \bm{x})}] + \mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}} [{q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) v(\bm{y}_1, \bm{y}, \bm{x})}] \\
&=\mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2} [{q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) v(\bm{y}_1, \bm{y}_2, \bm{x})}] + \mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2} [{q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) v(\bm{y}_1, \bm{y}_2, \bm{x})}] \\
&= 2\mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2} [q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) v(\bm{y}_1, \bm{y}_2, \bm{x})] \numberthis \label{eq:qobpal}
\end{align*}

\begin{tcolorbox}[center,colback=white]
The particular names of variables under expectation do not matter as long as they share the same distribution. A simple example demonstrating this point:
\[
\mathop{\mathbb{E}}_{x \sim p}[f(x)] + \mathop{\mathbb{E}}_{y \sim p}[f(y)]
= \mathop{\mathbb{E}}_{x \sim p}[f(x)] + \mathop{\mathbb{E}}_{x \sim p}[f(x)]
= 2\mathop{\mathbb{E}}_{x\sim p}[f(x)].
\]
\end{tcolorbox}

Similar trick can be applied for $(**)$ as well:
\begin{align*}
(**)&=\mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2, \bm{y} \sim \mu } [q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) v(\bm{y}_2, \bm{y}, \bm{x}) + q_{\boldsymbol \theta}(\bm{y}_2, \bm{x}) v(\bm{y}_1, \bm{y}, \bm{x})]  \\
&= 2\mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2, \bm{y} \sim \mu } [q_{\boldsymbol \theta}(\bm{y}_1,\bm{x}) v(\bm{y}_2, \bm{y}, \bm{x})] \\
&= 2\mathop{\mathbb{E}}_{\bm{y}_1 \sim \mu } [q_{\boldsymbol \theta}(\bm{y}_1, \bm{x})] \mathop{\mathbb{E}}_{\bm{y}_2, \bm{y} \sim \mu}[v(\bm{y}_2, \bm{y}, \bm{x})] \\
&=\mathop{\mathbb{E}}_{\bm{y}_1 \sim \mu } [q_{\boldsymbol \theta}(\bm{y}_1, \bm{x})] \numberthis \label{eq:qoaibl}
\end{align*}
where the last equality holds because $\bm{y}_1, \bm{y}_2, \bm{y}$ are independent, and $ \mathop{\mathbb{E}}_{\bm{y}_2, \bm{y} \sim \mu}[v(\bm{y}_2, \bm{y}, \bm{x})] = 0.5$ which in turn can be derived from conditions \eqref{eq:ipo_self_compare} and \eqref{eq:ipo_symmetric}.
Combining \eqref{eq:qobpal}, \eqref{eq:qoaibl} gives
\begin{align*}
(*) - (**)
&= 2\mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2}[q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) v(\bm{y}_1, \bm{y}_2, \bm{x})] - \mathop{\mathbb{E}}_{\bm{y}_1} [q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) ] \\
&= \mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2} \left[2q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) v(\bm{y}_1, \bm{y}_2, \bm{x}) - (v(\bm{y}_1, \bm{y}_2, \bm{x})+v(\bm{y}_2, \bm{y}_1, \bm{x}))q_{\boldsymbol \theta}(\bm{y}_1, \bm{x} )  \right] \quad \text{(by \eqref{eq:ipo_symmetric})}\\
&= \mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2} \left[q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) v(\bm{y}_1, \bm{y}_2, \bm{x}) - v(\bm{y}_2, \bm{y}_1, \bm{x})q_{\boldsymbol \theta}(\bm{y}_1, \bm{x} )  \right] \\
&= \mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2} \left[q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) v(\bm{y}_1, \bm{y}_2, \bm{x}) - v(\bm{y}_1, \bm{y}_2, \bm{x})q_{\boldsymbol \theta}(\bm{y}_2, \bm{x} )  \right] \\
&= \mathop{\mathbb{E}}_{\bm{y}_1, \bm{y}_2} \left[(q_{\boldsymbol \theta}(\bm{y}_1, \bm{x})-q_{\boldsymbol \theta}(\bm{y}_2, \bm{x})) v(\bm{y}_1, \bm{y}_2, \bm{x}) \right]
\end{align*}
The expression in \eqref{eq:qioqli} then becomes
\begin{align*}
&\argmin_{\boldsymbol \theta} \; \mathop{\mathbb{E}}_{\substack{\bm{x}, \\ \bm{y}_1, \bm{y}_2 \sim \mu}} \left[ (q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) - q_{\boldsymbol \theta}(\bm{y}_2, \bm{x}))^2 \right] - 2 \mathop{\mathbb{E}}_{\substack{\bm{x}, \\ \bm{y}_1, \bm{y}_2 \sim \mu}} \left[ (q_{\boldsymbol \theta}(\bm{y}_1, \bm{x}) - q_{\boldsymbol \theta}(\bm{y}_2, \bm{x})) v(\bm{y}_1, \bm{y}_2, \bm{x}) \right] \\
&=\argmin_{\boldsymbol \theta} \mathop{\mathbb{E}}_{\substack{\bm{x}, \\ \bm{y}_1, \bm{y}_2 \sim \mu}} \left[ ( h_{\boldsymbol \theta}(\bm{y}_1, \bm{y}_2, \bm{x}) - v(\bm{y}_1, \bm{y}_2, \bm{x}))^2 \right]
\end{align*} 
\end{proof}

\paragraph{``Notation''.} 
I am aware that I have used some terminologies quite freely and that could confuse rigorous readers. Here I like to clarify some of them
\begin{itemize}
    \item By ``learning'', I mean finding a mapping that can produce the right output given unseen input.
    \item By ``policy'', I mean a language model
    \item By ``generation'', I mean certain mechanism to sample $\bm{y} \sim \pi(\bm{y}\mid \bm{x})$
\end{itemize}


\end{document}

