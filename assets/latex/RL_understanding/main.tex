\documentclass[11pt,a4paper]{article}

% --------------------------------------------------------- 
% PACKAGES AND PREDEFINED SETTINGS
% --------------------------------------------------------- 

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage[utf8x]{inputenc}
\usepackage[linesnumbered,lined,boxed,commentsnumbered,ruled,vlined]{algorithm2e}
\usepackage{enumitem}
%\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx} % Allows including images
\usepackage[hidelinks]{hyperref} % the option is there to remove the square around links which is what I don't like.
\usepackage{perpage} 
\MakePerPage{footnote} % Reset the footnote counter perpage. may require to run latex twice.
\usepackage[margin=2cm]{geometry} % This is here to fit more text into the page.
% \setlength{\parindent}{0pt} % No indentation for paragraphs. Because that is just old.
%\setlength{\parskip}{\baselineskip} % Instead use vertical paragraph spacing.
\fontencoding{T1} % the better font encoding.
\usepackage{chngcntr} % https://tex.stackexchange.com/questions/28333/continuous-v-per-chapter-section-numbering-of-figures-tables-and-other-docume
%\usepackage{thmtools} % http://ftp.math.purdue.edu/mirrors/ctan.org/macros/latex/exptl/thmtools/thmtools.pdf
\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}

\usepackage{listings}
\lstset{language=python}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
\usepackage[
backend=biber,
style=authoryear,
citestyle=authoryear
]{biblatex}
\addbibresource{refs.bib}
\usepackage{afterpage}
\usepackage{multirow}
\usepackage{bm}
\usepackage[bottom]{footmisc}

\usepackage{framed}
\colorlet{shadecolor}{orange!15}

% --------------------------------------------------------- 
% SETTINGS
% --------------------------------------------------------- 
\counterwithin{figure}{section}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
% --------------------------------------------------------- 
% CUSTOM COMMANDS
% --------------------------------------------------------- 

\def\green{\color{green}}
\def\red{\color{red}}
\def\blue{\color{blue}}
\newcommand{\rank}[1]{\text{rank}(#1)}
\newcommand{\pr}[1]{\text{Pr}\left(#1\right)}
\newcommand{\st}{\text{subject to}\quad }
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\vect}[1]{\text{vec}\left(#1\right)}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % https://tex.stackexchange.com/questions/42726/align-but-show-one-equation-number-at-the-end 
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\T}{\!\top\!}

\newcommand{\incfig}[2][1]{%
    \def\svgwidth{#1\columnwidth}
    \import{./figures/}{#2.pdf_tex}
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}
\newcommand{\indep}{\perp \!\!\! \perp}

% --------------------------------------------------------- 
% CUSTOM ENVIRONMENTS
% --------------------------------------------------------- 
% \theoremstyle{plain}
\theoremstyle{definition}
\newcounter{block_counter}
\newtheorem{blockx}[block_counter]{Block}
\newenvironment{block}
  {\begin{shaded}\begin{blockx}}
  {\end{blockx}\end{shaded}}

\newcounter{definition_counter}
\newtheorem{definitionx}[definition_counter]{Definition}
\newenvironment{definition}
{
    \definecolor{shadecolor}{named}{SkyBlue}
    \begin{shaded}
    \begin{definitionx}
}
  {\end{definitionx}
\end{shaded}}

% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{lemma}{Lemma}[section]
%
% \theoremstyle{definition}
% \newtheorem{definition}{Definition}[section]
% \newtheorem{corollary}{Corollary}[theorem]
%
\newcounter{remark_counter}
\theoremstyle{remark}
\newtheorem{remark}[remark_counter]{Remark}

% --------------------------------------------------------- 
% TITLE, AUTHORS, ...
% --------------------------------------------------------- 
\title{Reinforcement Learning is So Confusing}
\author{	Tri Nguyen \\
        \texttt{nguyetr9@oregonstate.edu} \\
        }

% --------------------------------------------------------- 
% BEGIN DOCUMENT
% --------------------------------------------------------- 
\begin{document}
\maketitle

I've read this paper \parencite{madjiheurem2021expected} a long time ago. The authors motivate their new type of updates with this line of reasoning. The well-known TD-update only update value of the immediately preceding states (given if the reward is not zero). The eligibility trace improves upon this by making the update propagating back multiple states along the observed trajectory. Then they proposed that we can even make a step further by not only updating all states along observed trajectory but also all ``plausible'' counterfactual trajectories.

Although the reasoning is nicely motivated, I am not so convinced and want to know why or how do we have all these kinds of updating. There should be a principle that they are all based on which the author seemingly assume all audience are aware of. I think the origin problem starts with the TD-update. Let's trace back from it. Actually, this note starts from the very beginning of introducing Reinforcement Learning (RL).

\paragraph{RL Goal.} 
While the ultimate goal of using RL might be abstract and unqualitative, one must explicitly provide some objective so that we can have a certain of how good/bad are we doing. One of the such common goal is discounted cumulative reward. Be aware of that there are others goal, the reason of choosing this is out of scope for now. It is defined as
\[
G_{\pi} = \mathbb{E} \left[   \sum^{\infty}_{t=0} \gamma^{t} r_t\right]
\] 
where the expectation is taken over all random variables. So we want to maximize a sum of all rewards, including the immediate reward with higher weight and future rewards with less significant weights, and taking an average over multiple runs.
\begin{alignat*}{2}
    & \minimize_{\pi} \quad &&  G_\pi
\end{alignat*}
\begin{block}
The goal does not concern with variance, which means it might output a good policy in terms of average but might be very very unstable, e.g., getting a very high $G$ on a particular run, and moderate $G$ for other $99$ runs.
\end{block}

Immediately, one could ask: What are challenges of the optimization problem above? It looks like an unconstrained problem, how hard it could be? At the moment, it is infeasible due to the expectation. But hey, all supervised learning problems involve expectation and we've cracked them like eating noodle.
In statistical learning, under supervised classification setting, it is guaranteed that if we do well on empirical loss, then the true loss (involving expectation) would be okay. That said, although ultimate goal is true loss, we have a surrogate function containing no expectation to work on. 
{\red Hmm, then what would be a surrogate function for $G_\pi$? } Hold this thought, I'd like to come back later.



\paragraph{Notation.} We use $S_i$ as a random variable of state at time step $t$, $s_i$ as a particular state in a set of states  $\mathcal{S}, \abs{\mathcal{S}}=N$. It is really important to note that $S_i, i \to \infty$ while $s_i, 1\leq i \leq N$.
Similar, $A_i$ is used as a random variable of action at time step $i$,  $a_i$ as a particular action in a set of actions  $\mathcal{A}, \abs{\mathcal{A}}=M$;
$r(S_i, A_i): \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ as a reward received at time step $i$;

% \paragraph{Dynamic programming.} 
A way to decompose this $G$ is
\begin{align*}
     G_{\pi} &= \mathbb{E} \left[  r(S_0, A_0) + \gamma r(S_1, A_1) + \gamma^2 r(S_2, A_2) + \ldots + \gamma^{t}r(S_t, A_t) + \ldots \right] \\
     &= \mathbb{E} \left[r(S_0, A_0) + \gamma \underbrace{\mathbb{E} \left[ r(S_1, A_1) + \ldots  + \gamma^{t-1} r(S_t, A_t) + \ldots  \right]}_{\text{this term is still a RV}} \right]\\
     &= \mathbb{E}[r(S_0, A_0) + \gamma  \underbrace{V(1, S_1)}_{\text{a RV}} ],
\end{align*}
where we define something called \textit{value function} as follow.

\begin{definition}[Value function]
A value function of a policy is a real-value function, $V_{\pi}: \mathcal{S} \rightarrow \mathbb{R}$, and is defined as
\begin{equation}
    \label{eq:V_def}
% V_{\pi}(S_1=s) := \mathbb{E} \left[ r(s, A_1) + \gamma r(S_2, A_2) + \ldots  + \gamma^{t-1} r(S_t, A_t) + \ldots | S_1 = s \right]
V_{\pi}(i, s) := \mathbb{E} \left[ r(s, A_i) + \gamma r(S_{i+1}, A_{i+1}) + \ldots  + \gamma^{t-1} r(S_t, A_t) + \ldots | S_i = s \right]
\end{equation} 
% where $S_1$ is the state input,  $S_2, S_3, \ldots $ are next states relatively to $S_1$ (index $1$ does not mean anything specific here, just indicate a relative temporal index).
\end{definition}
One immediate question is whether this quantity is finite, i.e., $V_{\pi}(i, s) < \infty , \; \forall i, s$? Roughly speaking, it should be bounded, since $r(s, a) < \infty , \; \forall s, a$, hence it is upper bounded by harmonic series, which is finite.  And of course, the expectation being finite is followed.
% Nevertheless, let's show it in detail.
The conditioning $S_i=s$ is necessary since all remaining RVs $A_{i}, S_{i+1}, A_{i+1}, \ldots $ depend on that information (although do not show the dependence explicitly)\footnote{To demonstrate, you will see the following quantity makes no sense
\[
\mathbb{E} \left[ r(s, A_1) + \gamma r(S_2, A_2) + \ldots  + \gamma^{t-1} r(S_t, A_t) + \ldots \right]
\] 
}. 

Then, next question is if the index $i$ is a necessary parameter of $V_\pi$. It \textit{seems} not: considering infinite horizon, index $i$ doesn't matter, i.e., let $i' = i + k, k> 0$,
\begin{equation}
    \label{eq:value_time_invariant}
V_\pi(i', s) = V_\pi(i', s)
\end{equation} 
We will show evidence of this later, but short answer is this holds (in the limit).

\begin{remark}
    Value function at a state $s$ at particular timestep $i$, i.e., $V_\pi(i, s)$ is a deterministic quantity.
\end{remark}
\begin{remark}
 Value function is time invariant, i.e., $V_\pi(i, s) = V_\pi(i',s) $ for any $i, i' \geq 0$, considering that there are infinitely many more timesteps after the timestep $\max (i, i')$.
Since it is time invariant, we will use $V_\pi(s)$ as convention.
\end{remark}
\begin{remark}
Value function of a state can be described by value function of other states. 
    \begin{align*}
    V_{\pi}(1, s) 
    &= \mathbb{E}[r(s, A_1) + \gamma r(S_2, A_2) + \ldots + \gamma^{t-1} r(S_t, A_t) + \ldots  | S_1 = s] \\
    &= \mathop{\mathbb{E}}_{A_1} \left[r(s, A_1) + \gamma \mathbb{E} [r(S_2, A_2) + \ldots + \gamma^{t-1} r(S_t, A_t) + \ldots  ] \mid S_1 = s \right] \quad \text{(splitting RVs)}\\
    &= \sum_{i=1} \text{Pr}(A_1=a_i|S_1=s) \left(r(s, a_i) + \gamma \mathbb{E} [r(S_2, A_2) + \ldots + \gamma^{t-1} r(S_t, A_t) + \ldots \mid S_1=s, A_1 = a_i ]  \right) 
    \end{align*} 
\end{remark}
The last term is
\begin{align*}
\quad &\mathbb{E}[r(S_2, A_2) + \gamma r(S_3, A_3) + \ldots | S_1=s, A_1=a_i] \\
&= \sum_{j=1} \text{Pr}(S_2=s_j|S_1=s, A_1=a_i) \mathbb{E}\left[ r(s_j, A_2) + \gamma r(S_3, A_3) + \ldots | S_1=s, A_1=a_i, S_2=s_j \right] \\
&= \sum_{j=1} \text{Pr}(S_2=s_j|S_1=s, A_1=a_i) \mathbb{E}\left[ r(s_j, A_2) + \gamma r(S_3, A_3) + \ldots | S_2=s_j \right] \quad \text{(Markov property)} \\
&= \sum_{j=1} \text{Pr}(S_2=s_j|S_1=s, A_1=a_i) V_{\pi}(2, s_j) 
\end{align*}
Combine those, 
\[
V_\pi(1, s) = \sum_{i=1} \text{Pr}(A_1=a_i|S_1=s) \left( r(s, a_i) +  \gamma \sum_{j=1} \text{Pr}(S_2=s_j \mid S_1=s, A_1=a_i) V_\pi(2, s_j) \right)
\] 
With deterministic policy, the outer sum reduces to a single quantity, ({\red well, when shall we do stochastic?})
\[
V_\pi(1, s) = r(s, \pi(s)) + \gamma \sum_{j=1} \text{Pr}(S_2=s_j \mid S_1=s, A_1=\pi(s)) V_\pi(2, s_j) 
\] 
\begin{block}
    \label{block:value_function_relation}
    For deterministic policy, 
\[
V_\pi(1, s) = r(s, \pi(s)) + \gamma \sum_{j=1} \text{Pr}(S_2=s_j \mid S_1=s, A_1=\pi(s)) V_\pi(2, s_j) 
\] 
\end{block}
We can describe this in a more compactly using matrix/vector notation. Define the following quantities
\begin{align*}
&\bm{v}_1 = [V_\pi(1, s_1), V_\pi(1, s_2), \ldots , V_\pi(1, s_{N})]^{\T} \in \mathbb{R}^{N},  \\
&\bm{v}_2 = [V_\pi(2, s_1), V_\pi(2, s_2), \ldots , V_\pi(2, s_{N})]^{\T} \in \mathbb{R}^{N},  \\
&\bm{P}_{12} = \begin{bmatrix}
    \text{Pr}(S_2=s_1| S_1=s_1, A_1=\pi(s_1)) & \ldots & \text{Pr}(S_2=s_{N} \mid S_1=s_1, A_1=\pi(s_1) ) \\
                                          \vdots & \vdots   & \vdots \\
      \text{Pr}(S_2=s_1 | S_1=s_N, A_1=\pi(s_N)) & \ldots & \text{Pr}(S_2=s_N | S_1=s_N, A_1=\pi(s_N)) 
\end{bmatrix}  \in \mathbb{R}^{NM \times N}, \\
& \bm{r}_1 = [r(s_1, \pi(s_1)), r(s_2, \pi(s_2)), \ldots , r(s_N, \pi(s_N))]^{\T} \in \mathbb{R}^{N}
\end{align*} 
Notice that all subscripts above is used for timestep:
\begin{itemize}
    \item $\bm{v}_1, \bm{v}_2$ are values at timestep 1 and 2;
    \item $\bm{P}_{12}$ are transition matrix from timestep 1 to timestep 2. $\bm{P}_{12}$ depends on environment's property and the policy. That makes sense since value function depends on the policy $\pi$. However, since both transition defined by environment and the policy is time invariant, $\bm{P}_{12}$ is the same as $\bm{P}_{i(i+1)}$ for any $i$. For that reason, let just use $\bm{P}$.
    \item $\bm{r}_1$ is reward at timestep 1. Similarly, it is timestep invariant, let's use $\bm{r}$.
\end{itemize}  
Then, Block~\ref{block:value_function_relation} can be written as
\[
\bm{v}_1 = \bm{r} + \gamma \bm{P} \bm{v}_2,
\] 
and it holds for any timestep $i$,
\[
\bm{v}_i = \bm{r} + \gamma \bm{P} \bm{v}_{i+1},
\] 

(Well, it takes forever to reach the contraction operator :( , but here we come).

Define a linear\footnote{is it linear?} operator $T_{\gamma}: \mathbb{R}^{N} \rightarrow \mathbb{R}^{N}$ as
\[
T_\gamma (\bm{v}) \triangleq \bm{r} + \gamma \bm{P} \bm{v}
\] 
Hence,
\[
\bm{v}_{i} = T_\gamma (\bm{v}_{i+1})
\] 
{\red Why does it look unnatural (opposite direction is more natural)?}
Funny enough, this suggests that we should start from the tail and then go backward to $\bm{v}_1$. And it is actually what people do.

Okay, in terms of theory, it is okay to go backward, just need to assume index can be negative and goes to negative infinite. The important thing is $T_{\gamma}$ is a $\gamma$-contractor. That means, going backward far enough, i.e.,  $T \ll 0$, 
\[
\norm{\bm{v}_{T} - \bm{v}_{T+1}}_{\infty} \approx 0
\] 
This also confirms the speculation in \eqref{eq:value_time_invariant}.

Okay, we figured out some thing about the values function. But still this shreds no light into how to optimize $G_\pi$. In fact, it is now more confusing of how all these thing relate to $G_\pi$.

Hmm, we need another start: A starting point from control theory.
Every luckily, we are pointed to a very good direction: \parencite{bertsekas2012dynamic} !!!

\paragraph{Some other very vague and unorganized thoughts.}%
\begin{itemize}
    \item 
Consider deterministic policy, all the randomness in $G_{\pi}$ are from environment (transition matrix mostly). So if we can estimate these randomness, we could solve this optimization as a linear programming, couldn't we?
\item The RL problem has 2 interacting parts: estimating the dynamics of environment, find the best policy. One can solve each sub-problem independently, or in a more involving way. I believe that nature of dealing with 2 sub-problem simultaneously is what distinguishes RL from other learning problems. Let's call these problems \textit{Estimation} and \textit{Control}, resp.
\item 
{\blue In the beginning of chapter 5.2, one of the discussing issues is that some $(s,a)$ is never visited, hence values at these are hard to estimate, hence one needs exploration so that every possible place is visited. But I'm wondering, is it a generalization issue as in supervised learning?}
\item 
The problem formulation itself is not a typical optimization problem: the objective function evolves over times. So it is a completely different problem class. Compared to classical optimization problem, we are solving many different problems, each at one timestep\footnote{of course, they should relate to each other in someway}, which is parameterized by the state and the optimization variable is the action.
{\blue Did people try to learn the reward function using neural network as in supervised learning setting?}
\end{itemize}



% \section{Dynamic Programming}%
% \label{sec:dynamic_programming}
% We are reading \parencite{bertsekas2012dynamic}!!!
%
% The so-called \textit{principle of optimality} holds because whatever actions we are about to make, it won't change accumulated cost up to the moment. 
% Structures of the problem: every next state only depends on previous state (and action), and cost function is decomposible as sum over states play no role here.
%

% \begin{definition}
%     A action-value function of a policy $\pi$ is a real-value function $Q_{\pi}(s, a): \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ defined as
%     \[
%     Q_\pi(S_1=s, A_1=a) = r(s, a) + 
%     \] 
% \end{definition}
%
% We have partitioned $G_{\pi}$ into a so-called immediate reward $r_0(S_0, A_0)$ and a future reward $\gamma V(S_1)$. We can actually do a further step that we decompose $V(S_1)$
%
% \begin{align*}
%     V_{\pi}(S_1) 
%     &= \mathbb{E} \left[ r_1(S_1, A_1) + \gamma \mathbb{E}\left[ r_2(S_2, A_2) + \ldots + \gamma^{t-2} r_{t}(S_t, A_t) + \ldots  \right] \right] \\
%     &= \mathbb{E}[r_1(S_1, A_1) + \gamma V(S_2)] \\
%     &= \sum_{i=1} 
% \end{align*}
% {\blue please define what is $S_i$? a random variable?}
%
% The outer expectation is taking over 
% \begin{itemize}
%     \item $S_0$
%     \item Policy $\pi$ at  $S_0$.
%     \item Reward at $S_0$, i.e., $r(S_0, A_0)$.
%     \item Transition from $S_0$ to  $S_1$, i.e.,  $p(S_1 | S_0, A_0)$.
% \end{itemize}
%
% Now after we are clear about objective, and getting some insight, let's talk about method. Define an operator $\mathcal{T}$ that receives an array $\bm{V} \in \mathbb{R}^{|\mathcal{S}|}$, it will output another vector with the same dimension. Vector $\bm{V}$ is defined as
% \[
% \bm{V} = [V(s_1), V(s_2), \ldots , V(s_n)]
% \] 
% that is the $i$th element is value of function $V$ evaluating at state $s_i$. Be aware that $V$ is a function defined in \eqref{eq:V_def}.
% Confusingly, definition of function $V$ involves time, while this vector $\bm{V}$ does not.
%
% Next, define transition matrix, $\bm{P} \in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}||\mathcal{S}|}$, $P(s| s', a)$.
%
% There is a relation between $\bm{V}^{t}$ and $\bm{V}^{t-1}$, which is

\pagebreak
\printbibliography
\end{document}
