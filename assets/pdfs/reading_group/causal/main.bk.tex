
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[10pt]{beamer}

\mode<presentation>
\usetheme{default}
\usecolortheme{orchid}
\usefonttheme{professionalfonts}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{xcolor} 
\usetikzlibrary{calc}
\usetikzlibrary{arrows,automata}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.text}
\usetikzlibrary{decorations.pathmorphing}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usetikzlibrary{tikzmark,calc}
\usepackage{mathtools}
\usepackage{amsthm}
%\usepackage{enumitem}

\usepackage[english]{babel}
\usepackage{listings}
\lstset{language=python}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage[citestyle=authoryear,maxnames=1]{biblatex}
\addbibresource{refs.bib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{colorlinks,citecolor=blue,linkcolor=blue}
\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{bm}
% \usepackage[scaled]{helvet}
% --------------------------------------------------------- 
% SETTINGS
% --------------------------------------------------------- 
\counterwithin{figure}{section}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\definecolor{codegreen}{rgb}{0,0.6,0}
% --------------------------------------------------------- 
% CUSTOM COMMANDS
% --------------------------------------------------------- 

\def\green{\color{green}}
\def\red{\color{red}}
\def\blue{\color{blue}}
\newcommand{\rank}[1]{\text{rank}(#1)}
\newcommand{\pr}[1]{\text{Pr}\left(#1\right)}
\newcommand{\st}{\text{subject to}\quad }
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\vect}[1]{\text{vec}\left(#1\right)}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} % https://tex.stackexchange.com/questions/42726/align-but-show-one-equation-number-at-the-end 
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

\newcommand{\incfig}[2][1]{%
    \def\svgwidth{#1\columnwidth}
    \import{./figures/}{#2.pdf_tex}
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}
% to change colors
\newcommand{\fillcol}{green!10}
\newcommand{\bordercol}{black}

\newcommand\DrawBox[3][]{%
  \begin{tikzpicture}[remember picture,overlay]
    \draw[overlay,fill=gray!30,#1] 
    ([xshift=-8em,yshift=2.1ex]{pic cs:#2}) 
    rectangle 
    ([xshift=2pt,yshift=-0.7ex]pic cs:#3);
  \end{tikzpicture}%
}
\newcommand*{\captionsource}[2]{%
  \caption[{#1}]{%
    #1%
    \\\hspace{\linewidth}%
    \textbf{Source:} #2%
  }%
}
\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\newcounter{saveenumi}
\newcommand{\seti}{\setcounter{saveenumi}{\value{enumi}}}
\newcommand{\conti}{\setcounter{enumi}{\value{saveenumi}}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\notindep}{\not\!\perp \!\!\! \perp}

\usepackage{tikz}
\usetikzlibrary{bayesnet}
% --------------------------------------------------------- 
% CUSTOM ENVIRONMENTS
% --------------------------------------------------------- 
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{lemma}{Lemma}[section]
%
% \theoremstyle{definition}
% \newtheorem{definition}{Definition}[section]
% \newtheorem{corollary}{Corollary}[theorem]
%
% \theoremstyle{remark}
% \newtheorem{remark}{Remark}[section]
\newtheorem{proposition}[theorem]{\translate{Proposition}}
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Causal Discovery]{Learning Directed Acyclic Graph via Score-based Approach} 

\author{Tri Nguyen }
\institute[OSU] 
{
Oregon State University \\ 
}
\date{\today} % Date, can be changed to a custom date


\makeatletter
\makeatother


\begin{document}
%------------------------------------------------
\begin{frame}
\titlepage 
\end{frame}
%------------------------------------------------

\section*{Outline}%
\label{sec:outline}
\begin{frame}
    \tableofcontents
\end{frame}

\section{Problem}%

\begin{frame}
\frametitle{Directed Acyclic Graph}
\begin{itemize}
    \item A graph $G$ is directed acyclic graph (DAG) if it is directed and there is no cyclic. 
    \item Given a DAG $G$ and a joint distribution  $P(X_1, \ldots , X_p)$. Graph $G$ is called \textbf{Markov relative} to  $G$ if 
                \[
        P(X_1, \ldots , X_p) = \prod_{i} P(X_i | \text{pa}_i)
                \] 
    \item $P$ is Markov relative to  $G$ iff
        \[
        \bm{A} \indep_{G} \bm{B} \mid \bm{C} \Rightarrow \bm{A} \indep \bm{B} \mid \bm{C}
        \] 
        where $\indep_{G}$ denotes d-separation property, and $\bm{A}, \bm{B}, \bm{C}$ are disjoint subset of RVs.
    \item Example: 
        \[
        P(X_1, X_2, X_3) = P(X_1, X_2 | X_3)P(X_3) := P(X_1|X_3)P(X_2|X_3) P(X_3)
        \]
        is Markov relative to both $G_1, G_2$.
\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
    \resizebox{0.8\textwidth}{!} {
    \begin{tikzpicture}
        \node[latent](X1){$X_1$};
        \node[latent, right=of X1](X2){$X_2$};
        \node[latent, right=of X2](X3){$X_3$};
        \draw[->] (X1) -- (X2);
        \draw[->] (X2) -- (X3);
    \end{tikzpicture}
}
\subcaption*{ $G_1$ }
    \label{fig:markov_equiv_1}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
    \resizebox{0.8\textwidth}{!}{
    \begin{tikzpicture}
        \node[latent](X1){$X_1$};
        \node[latent, right=of X1](X2){$X_2$};
        \node[latent, right=of X2](X3){$X_3$};
        \draw[->] (X1) -- (X2);
        \draw[->] (X2) -- (X3);
        \draw[->] (X1) to [out=30,in=150] (X3);
    \end{tikzpicture}
}
    \subcaption*{ $G_2$ }
    \label{fig:markov_equiv_2}
    \end{subfigure}
\end{figure}
\end{itemize}
\end{frame}

\begin{frame}
    \begin{itemize}
        \item The joint probability $P(X_1, \ldots , X_p)$ is \textbf{minimal} with respect to a DAG $G$ if it is Markov relative to $G$ but not to any proper subgraph of $G$.
    \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Structural Equation Model}
\begin{itemize}
    \item A graph $G$ is directed acyclic graph (DAG) if it is directed and there is no cyclic. 
    \item Given a DAG $G$ and a joint distribution  $P(X_1, \ldots , X_p)$. Graph $G$ is called \textbf{Markov relative} to  $G$ if 
                \[
        P(X_1, \ldots , X_p) = \prod_{i} P(X_i | \text{pa}_i)
                \] 
    \item $P$ is Markov relative to  $G$ iff
        \[
        A \indep_{G} B \mid C \Rightarrow A \indep B |\mid C
        \] 
        where $\indep_{G}$ denotes d-separation property.
    \item Consider a class $\mathcal{D}$ of directed acyclic graph $G=\langle V, E \rangle$ where $V=\set{X_1, \ldots , X_p}$.
        A joint pdf $P(X_1, \ldots , X_p)$ is called \textbf{Markov relative} to $G \in \mathcal{D}$ if
        \[
        P(X_1, \ldots , X_p) = \prod_{i} P(X_i | \text{pa}_i)
        \] 
        where $\text{pa}_i$ is set of parent of $X_i$ in $G$ [x]
    \item 

    \item \textbf{Structural equation models (SEM)} is a set of $p$ equations that describes relationship between  $p$ RVs, i.e., 
\[
X_i = f_i(\text{pa}_i, \epsilon_i), \text{ for } i=1, \ldots , p
\]
and $\text{pa}_i$ is a subset of $\set{X_1, \ldots , X_p}$ that directly determine the value of $X_i$, and $\epsilon_i$s are mutually independent, and also independent with $\text{pa}_i$ [x].

\item The ``structure'' part of a SEM can be shown using a directed acyclic graph.

\item Problem: Given $n$ i.i.d data points $\bm{X} \in \mathbb{R}^{n\times  p}$, can we recover $f_i$?
\item A linear SEM is a SEM with all $f_i$ being linear functions, i.e.,
     \[
    X_i = \sum_{X_j \in \text{pa}_i} w_j X_j + \epsilon_i
    \] 
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Structural Equation Model (SEM)}

\begin{definition}[Markov constraints]
Let a graph $G=\langle V, E \rangle$ be directed acyclic graph (DAG). Markov constraints are set of independence relations defined by $G$.
\end{definition}
\begin{definition}[Markov equivalent]
Two DAGs are Markov equivalent if they have the same Markov constraints.
\end{definition}

\begin{figure}
    \centering
    \begin{subfigure}{0.3\textwidth}
    \resizebox{0.8\textwidth}{!} {
    \begin{tikzpicture}
        \node[latent](A){$A$};
        \node[latent, right=of A](B){$B$};
        \node[latent, right=of B](C){$C$};

        \edge{A}{B};
        \edge{B}{C};
    \end{tikzpicture}}
    \subcaption{ }
    \label{fig:markov_equiv_1}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
    \resizebox{0.8\textwidth}{!}{
    \begin{tikzpicture}
        \node[latent](A){$A$};
        \node[latent, right=of A](B){$B$};
        \node[latent, right=of B](C){$C$};

        \edge{B}{A};
        \edge{C}{B};
    \end{tikzpicture}}
    \subcaption{ }
    \label{fig:markov_equiv_2}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
    \resizebox{0.8\textwidth}{!}{
    \begin{tikzpicture}
        \node[latent](A){$A$};
        \node[latent, right=of A](B){$B$};
        \node[latent, right=of B](C){$C$};

        \edge{A}{B};
        \edge{C}{B};
    \end{tikzpicture}}
    \subcaption{ }
    \label{fig:markov_equiv_2}
    \end{subfigure}
    \caption{(a) and (b) are Markov equivalent, while (c) are not. Both (a) and (b) have only one Markov constraint $A \indep C | B$, while in (c), it is $A \indep C$.}
\end{figure}
{\red what is $A \rightarrow B$?}
\begin{block}{Goal}
    Determine $G$ up to Markov equivalent ambiguity.
\end{block}
\end{frame}

\begin{frame}
\frametitle{Another example}
\begin{figure}
    \centering
    \begin{subfigure}{0.2\textwidth}
    \resizebox{0.8\textwidth}{!} {
    \begin{tikzpicture}
        \node[latent](A){$A$};
        \node[latent, below=of A](B){$B$};
        \node[latent, below=of B, xshift=-1.5cm](C){$C$};
        \node[latent, below=of B,xshift=+1.5cm](D){$D$};

        \edge{A}{B};
        \edge{B}{C,D};
        \edge{C}{D};
    \end{tikzpicture}}
    \subcaption{ }
    \label{fig:markov_equiv_1}
    \end{subfigure}
    \begin{subfigure}{0.2\textwidth}
    \resizebox{0.8\textwidth}{!}{
    \begin{tikzpicture}
        \node[latent](A){$A$};
        \node[latent, below=of A](B){$B$};
        \node[latent, below=of B, xshift=-1.5cm](C){$C$};
        \node[latent, below=of B,xshift=+1.5cm](D){$D$};

        \edge{B}{A};
        \edge{B}{C,D};
        \edge{C}{D};
    \end{tikzpicture}}
    \subcaption{ }
    \label{fig:markov_equiv_2}
    \end{subfigure}
    \begin{subfigure}{0.2\textwidth}
    \resizebox{0.8\textwidth}{!}{
    \begin{tikzpicture}
        \node[latent](A){$A$};
        \node[latent, below=of A](B){$B$};
        \node[latent, below=of B, xshift=-1.5cm](C){$C$};
        \node[latent, below=of B,xshift=+1.5cm](D){$D$};

        \edge{A,C}{B};
        \edge{B}{D};
        \edge{C}{D};
    \end{tikzpicture}}
    \subcaption{ }
    \label{fig:markov_equiv_2}
    \end{subfigure}
    \caption{(a) and (b) are Markov equivalent, while (c) are not.}
\end{figure}
\begin{definition}[Skeleton]
    A undirected induced from DAG $G$ is skeleton of  $G$.
\end{definition}
\begin{definition}[Immorality]
    A subgraph of 3 nodes $X, Y, Z$ induced for a DAG $G$  is called immorality if $X \rightarrow Z$,  $Y \rightarrow Z$ and $X, Y$ are non-adjacent.
\end{definition}
    
\begin{theorem}[A well known result]
    Two DAGs are equivalent if they have the same skeleton and same set of immoralities.
\end{theorem}
\end{frame}

\subsection{Constraint-based Approach}%
\begin{frame}
\frametitle{Constraint-based Approach: the PC Algorithm}
some demo of the method
\end{frame}

\begin{frame}
\frametitle{Pros and Cons}
\begin{itemize}
    \item Statistical independent testing
    \item Running time is exponentially increasing to number of variables.
        \begin{itemize}
            \item Parallelism \cite{le_fast_2019}.
            \item Heuristic? kind of comprimising
        \end{itemize}
\end{itemize}
\end{frame}

\subsection{Score-based Approach}%
\begin{frame}
\frametitle{The xxx score}    
This seems quite interesting and also diverse. Method in this class has 2 components:
\begin{itemize}
    \item Evaluate a score
    \item Search: Greedy search is the most common
\end{itemize}
\end{frame}

\section{Proposed formulation}%
\label{sec:proposed_formulation}

\subsection{Design criteria}%
\label{sub:criteria}

\begin{frame}
    \frametitle{A Score-based Perspective}
    \begin{equation*}
    \begin{aligned}
        &\min_{\mathbf{W} \in \mathbb{R}^{d \times d}}  & & F(\mathbf{W}) \\
        &\text{subject to} && G(\mathbf{W}) \in \texttt{DAG}
    \end{aligned}
    \quad 
    \Leftrightarrow
    \quad 
    \begin{aligned}
        &\min_{\mathbf{W} \in \mathbb{R}^{d \times d}}  & & F(\mathbf{W}) \\
        &\text{subject to} && h(\mathbf{W}) = 0
    \end{aligned}
    \end{equation*}
    where we wish $h$ to be
    \begin{itemize}
        \item $h(\mathbf{W}) = 0$ if and only if $G(\mathbf{W})$ is acyclic.
        \item $h(\mathbf{W}) = 0$ measures the ``DAG-ness'' of the graph.
        \item $h(\mathbf{W})$ is smooth.
        \item $h(\mathbf{W})$ and its derivatives are easy to compute.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Binary Case}
    \begin{proposition}[Infinite series]
        Suppose $\bm{B} \in \set{0,1}^{d \times d}$ and $\abs{\lambda_{\max}(\bm{B})} < 1$. Then $G(\bm{B})$ is a DAG if and only if 
        \[
        \text{tr}(\mathbf{I} - \bm{B})^{-1} = d.
        \] 
    \end{proposition}
    \begin{proof}
        \begin{itemize}
            \item Number of length-$2$ paths from $i$ to $j$ is  $\sum^{d}_{t=1} B(i, t) B(t, j) = \bm{B}^{2}(i, j)$.
            \item Number of length-$k$ paths from $i$ to  $j$ is  $\bm{B}^{k}(i, j)$.
            \item Number of closed length-$k$ paths from $i$ to $i$ is $\bm{B}^{k}(i, i)$.
            \item Number of closed length-$k$ paths is  $\text{tr} (\bm{B}^{k})$.
            \item A graph is acyclic if and only if 
                $ \sum^{\infty}_{k=1} \text{tr}(\bm{B}^{k}) = 0$ 
        \end{itemize}

    \end{proof}
\end{frame}
\begin{frame}
    For any square matrix $\bm{B}$,
    \begin{align*}
        (\bm{I} - \bm{B})^{-1} 
        &= \bm{I} + (\bm{I} - \bm{B})^{-1} \bm{B} \\
        &= \bm{I} + (\bm{I} + (\bm{I}-\bm{B})^{-1}\bm{B}) \bm{B} \\
        &= \ldots  \\
        &= \bm{I}+ \bm{B} + \bm{B}^{2} + \ldots 
    \end{align*} 
    Since
    \[
    \abs{\lambda_{\max}(\bm{B})} < 1, \bm{B}^{k} \xrightarrow{k \to \infty} \textbf{0}
\]
    Therefore
    \[
    (\bm{I} - \bm{B})^{-1} = \sum^{\infty}_{k=0} \bm{B}^{k} \text{ is well defined}
    \] 
and,
    \[
    \text{tr}(\bm{I} - \bm{B})^{-1} = \text{tr}(\bm{I}) + \sum^{\infty}_{k=1} \text{tr}(\bm{B}^{k}) = d
    \] 
\end{frame}

\begin{frame}
    \frametitle{A Better Formula}

    \begin{proposition}
        A binary matrix $\bm{B} \in \set{0, 1}^{d \times d}$ is a DAG if and only if 
        \[
        \text{tr} (e^{\bm{B}}) = d.
        \] 
        where 
\[
e^{\bm{B}} = \sum^{\infty}_{k=0} \dfrac{1}{k!} \bm{B}^{k}
\] 

    \end{proposition}
    \begin{block}{Remark}
        \begin{itemize}
            \item $e^{\bm{B}}$ is always well-defined for all square matrix $\bm{B}$.
            \item The equivalence of having none cyclic path and $\text{tr}(\bm{B}^{k}) = 0$ for all $k$ only hold if $\bm{B}>0$.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Any Weighted Matrix $\bm{B}$}
    \begin{theorem}
        A matrix $\bm{W}$ is a DAG iff 
        \[
        h(\bm{W}) = \text{tr} \left( e^{W*W} \right) - d = 0
        \] 
    \end{theorem}
\end{frame}

\begin{frame}
    \frametitle{Notes for myself}
    \begin{itemize}
        \item Funny, this works has nothing to do with do-calculus, hence not really a causal inference work. But I'd guess other causal inference works has been based on this one.
    \end{itemize}
\end{frame}

\subsection{Optimization Method}%
\label{sub:optimization_method}


\section{Experiment Result}%
\label{sec:experiment_result}

\subsection{Synthetic data}%
\label{sub:synthetic_data}

\subsection{Real data}%
\label{sub:real_data}


\begin{frame}
    \frametitle{}
\end{frame}






% \begin{frame}
% \frametitle{Related works}
% Start from a very simple idea: \cite{hoyer2008nonlinear}
% \begin{itemize}
%     \item Assume a linear causal relationship: $Y = aX + N$. Given observed $X, Y$, how to determine causal relation between  $X$ and  $Y$?
%     \item The model says that the noise $N$ is independent to $X$, but it is not independent to $Y$.
%     \item Hence, do regression $Y = f(X)$, find the residual $\widehat{N}$, and check if $\widehat{N}$ is independent of $X$.
%     \item If no, do the reversion,  $X = f(Y)$, \ldots 
%     \item It won't work in case of $N \sim \mathcal{N}(\mu, \sigma^2)$, since the noise in 
% \end{itemize}
% \end{frame}
%
% \begin{frame}
% \begin{theorem}[Linear Gaussian]
%     \label{theorem:linear-gaussian}
%     Given that $Y = \alpha X  + N_Y$, where $X \indep N_Y$. Then there exists $N_X$ and  $\beta$ such that
%     $ X = \beta Y + N_X$ such that $Y \indep N_X$ if and only if both $X$ and  $N_Y$ are normally distributed.
% \end{theorem}
% \end{frame}

\end{document}
