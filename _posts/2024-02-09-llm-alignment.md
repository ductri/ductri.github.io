---
layout: post
title: "LLM Alignment: DPO"
date: 2024-02-09 21:23:00
description: LLM alignment fine-tuning with DPO
tags: DPO, RHLF, alignment, preference learning
categories: note
published: true
usemathjax: false
pdf_dir: /assets/pdfs/reading_group/llm_alignment/
---

A brief reading about Direct Preference Optimization method to address the alignment fine tuning for LLM.
<!--more-->
A brief reading about Direct Preference Optimization method to address the alignment fine tuning for LLM.
<!-- <embed src="{{page.pdf_dir}}/dpo.pdf" width= "100%" height= "600"> -->

<object data="{{page.pdf_dir}}/dpo.pdf" width="100%" height="600" type='application/pdf'></object>

