<!DOCTYPE html>
<html lang="vi">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Expectation Maximization | Machine learning</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">/home/machine learning</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/">~/home</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/categories/">~/categories</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
  
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110926018-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110926018-1');
</script>

</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">Expectation Maximization</span></h1>

<h2 class="date">2018/10/12</h2>
<p class="terms">
  
  
  
  
  
</p>
</div>



<main>
<p>Khi ta có crush :))).</p>
<p>Một ngày nọ, khi bạn nhận ra là bạn have a crush on someone. Bạn bắt đầu tương tư, mơ tưởng về ngày crush nhận ra và đáp lại tấm chưng tình của bạn. Bạn mất ăn mất ngủ vì crush. Cảm xúc đó, một cách tự nhiên, sẽ thôi thúc bạn, bằng mọi cách tiếp cận crush của mình. Bạn bắt chuyện, tán dóc, nói nhảm, chọc cười, làm đủ mọi trò, cố gắng có mặt ở những nơi có crush xuất hiện, &hellip; Tất cả chỉ bởi vì một mục đích: bạn đang âm thầm thu thập thông tin từ crush (<em>observed data</em>), để trả lời cho câu hỏi lớn nhất mà bao người có crush như bạn cũng thắc mắc: Liệu crush có thích bạn không ? Hay chi tiết hơn, crush đang thích bạn với mức độ nào trong thang đo từ 0 (đếch quan tâm gì cả) đến 1 (say đắm bạn rồi) (<em>unknown parameter</em>).</p>
<p>Ai có học chút toán thống kê chắc sẽ tự tin nghĩ là mình trả lời được câu hỏi trên dễ dàng, vì chỉ có 1 ẩn số, và nó ảnh hưởng trực tiếp tới nụ cười ấm áp mà crush dành cho bạn, và tất nhiên là bạn có ghi lại hết những lần crush cười với mình. Một trong những cách, chẳng hạn, dùng phương pháp tối đa hóa MAP (Maximum A Posterior), đi tìm GTLN của $$p(\text{mức độ crush thích bạn}| \text{crush cười khi nhìn bạn})$$
Bài này học sinh cấp 1 cũng giải ra, chẳng qua vì tụi nhỏ chưa đủ tuổi yêu đương nên người ta chưa dạy thôi.</p>
<p>Nhưng &ldquo;đời đâu như mơ&rdquo; (thầy Hiệp dạy), bạn quan sát thấy crush cười khi <strong>nhìn về phía bạn</strong>, có nghĩa là: crush có thể đang chính xác nhìn bạn hoặc crush đang tia cái thằng ngồi kế bạn. Đời khốn nạn thay, cái thèn kia cứ ngồi sát bạn, thành ra bạn không thể biết được chính xác crush đang ngắm ai mà cười. Lúc này, bạn có tới 2 boăn khoăn chứ không phải 1 như trước:</p>
<ul>
<li>Crush nhìn ai mà cười vậy ? (<em>latent variable</em>)</li>
<li>Crush có thích mình không vậy ? (<em>unknown parameter</em>)</li>
</ul>
<p>Tất nhiên, cuối cùng thì bạn vẫn mong muốn đáp án cho câu hỏi thứ 2 thôi, nhưng không biết câu trả lời cho câu hỏi thứ nhất thì câu hỏi thứ 2 coi như mò kim đáy bể. Cả đời đèn sách chỉ dạy cho bạn MLE, MAP, giờ đều không xài được. Bạn chỉ còn biết nhìn nụ cười tỏa nắng của crush mà trong lòng đau khổ, ngậm ngùi, boăn khoăn, trằn trọc, hập hực, bực tức &hellip; đủ thức cảm xúc gây stress khó chịu.</p>
<p>Yeah, nếu bạn đang ở trong tình huống trên, cực kỳ may mắn cho bạn là đã tiếp cận được bài viết này, bởi chúng tôi biết cách (<em>algorithm</em>) để trả lời câu hỏi của bạn. Tất cả gói gọn trong 2 chữ: Expectation-Maximization (gọi tắt là EM &lt;3 :).</p>
<p>EM sinh ra để giải quyết vấn đề khi mà những gì bạn quan sát được (<em>observed data</em>) phụ thuộc vào tới 2 thành phần bí ẩn (<em>latent variable</em> và <em>unknown parameter</em>)</p>
<h1 id="động-lực-để-đi-hiểu-em">Động lực để đi hiểu EM</h1>
<p>Dùng ở rất nhiều chỗ, coi Wiki mục application là thấy. Riêng mình đi coi EM vì đọc 2 cái này đều nhắc tới EM: Variational Inference: A review for Statisticians, Gaussian Mixture Model.</p>
<h1 id="vì-sao-phải-dùng-em">Vì sao phải dùng EM</h1>
<p>Tóm tắt theo wiki: EM được dùng để tìm Maximum likelihood (MLE) hoặc maximum a posterior (MAP), khi mà những giá trị này không thể tìm bằng cách thông thường được. Nói là ko thì ko hẳn, đúng hơn là khó, khó bởi vì model của bạn ngoài cái <em>unknown parameter</em> ra, còn có thêm <em>latent variable</em>. Đều là không biết cả, thì tại sao phải phân biệt ra 2 khái niệm này ? &ndash;&gt; Coi ở mục tham khảo, ko phải trọng tâm của bài.</p>
<p>Cụ thể, đây là ví dụ cho trường hợp MLE hay MAP &ldquo;khó tìm&rdquo;.Ví dụ này ai ai cũng đưa ví dụ này ra để nâng bi EM, mình chỉ bắt chước thôi :v</p>
<p>Có 2 đồng xu bị dị tật, nghĩa là xác suất ra mặt ngửa của mỗi đồng không phải là 0.5. Để xác định tính dị tật, ta làm thí nghiệm:</p>
<ul>
<li>Chọn ngẫu nhiên 1 xu ($a$ hoặc $b$). Việc chọn là ngẫu nhiên nên xác suất chọn trúng $a$ hay $b$ đều như nhau.</li>
<li>Gieo đồng xu đã chọn 10 lần, ghi lại số lần đồng xu ra mặt ngửa.</li>
</ul>
<p>Lặp lại 2 bước trên 5 lần. Data thu được như sau:</p>
<ul>
<li>Round 1: 4/10 lần đồng xu ra mặt ngữa</li>
<li>Round 2: 5/10 &hellip;</li>
<li>Round 3: 1/10 &hellip;</li>
<li>Round 4: 2/10 &hellip;</li>
<li>Round 5: 3/10 &hellip;</li>
</ul>
<p>Hỏi tỉ lệ cho ra mặt ngửa của mỗi đồng xu là bao nhiêu ?</p>
<p>Giải:</p>
<ul>
<li>Gọi $Z$ là biến chọn đồng xu, $Z$ có hai giá trị là $a$ hoặc $b$. Theo đề thì:
$$P(Z=a) = P(Z=b) = 0.5$$</li>
<li>Gọi $\theta_a$, $\theta_b$ là xác suất mặt ngữa của đồng $a$, $b$; $H$ là sự kiện ra mặt ngữa, vậy thì:
$$
P(H|Z=a) = \theta_a \\\<br>
P(H|Z=b) = \theta_b
$$</li>
</ul>
<p>Mục tiêu là tìm $\theta_a$, $ \theta_b $.</p>
<p>Có <strong>VÀI</strong> cách.</p>
<h2 id="cách-1">Cách 1</h2>
<p>Cách trông tự nhiên nhất là tìm $\theta_a$, $ \theta_b $ sao cho chúng là maximum:
$ \theta_a, \theta_b = \underset{\theta_a, \theta_b}{\operatorname{argmax}} p(\theta_a, \theta_b | X)$</p>
<p>Cách này có tên là Maximum a posterior estimation (MAP).
$$\underset{\theta_a, \theta_b}{\operatorname{argmax}} p(\theta_a, \theta_b | X) = \underset{\theta_a, \theta_b}{\operatorname{argmax}} \frac{p(X | \theta_a, \theta_b)  p(\theta_a, \theta_b)} {p(X)}$$</p>
<p>Vấn đề của cách này là chưa có $p(\theta_a, \theta_b)$, cần phải đưa thêm giả sử vào đây mới giải tiếp được. Trong một số bài toán thì cái giải thiết $p(\theta_a, \theta_b)$ được chọn tinh tế lắm mới giải được, thường là người ta chọn $p$ là xác suất &ldquo;conjugate family&rdquo;. Rồi cho như được rồi thì biểu thức trên vẫn dính tới 1 đại lượng gọi là Likelihood ở cách 2.</p>
<h2 id="cách-2">Cách 2</h2>
<p>Cách 1 phức tạp quá, vứt. Tiếp tới, nếu ta không tìm được $\theta_a, \theta_b$ để maximum $p(\theta_a, \theta_b | X)$ thì dùng đỡ cái $p(X|\theta_a, \theta_b)$ cũng tạm, tuy hơi thiếu tự nhiên so với cái trên, nhưng nhìn nhìn cũng make sense.
Cách này có tên (nghe rất đại chúng) là Maximum likelihood (MLE)</p>
<p>Căn bản thì MLE tìm $\theta$ bằng cách:
$$\theta_a, \theta_b = \underset{\theta_a, \theta_b}{\operatorname{argmax}} p(X| \theta_a, \theta_b)$$
Lợi thế của thèn này là mình khai triển cái $p(X| \theta_a, \theta_b)$ dễ dàng với giả định là ta biết được cách dữ liệu được sinh ra. Thử nề:</p>
<p>\begin{align}
p(X| \theta_a, \theta_b) &amp;= \prod_i^5 p(X_i | \theta_a, \theta_b) \\\<br>
&amp;= \prod_i^5 \bigl [p(X_i, Z=a| \theta_a, \theta_b) + p(X_i, Z=b | \theta_a, \theta_b) \bigl ]\\\<br>
&amp;= \prod_i^5 \bigl [p(X_i | Z=a,\theta_a, \theta_b)p(Z=a|\theta_a, \theta_b) + p(X_i| Z=b , \theta_a, \theta_b)p(Z=b | \theta_a, \theta_b) \bigl ]\\\<br>
\end{align}
Tùm lum quá, thử tính với $i=1$:</p>
<p>\begin{align}
&amp; p(X_1 | Z=a,\theta_a, \theta_b)p(Z=a|\theta_a, \theta_b) + p(X_1| Z=b , \theta_a, \theta_b)p(Z=b | \theta_a, \theta_b) \bigl ] \\\<br>
&amp;= \binom {10}{4}{\theta_a}^4(1-\theta_a)^6\times 0.5 + \binom {10}{4}{\theta_b}^4(1-\theta_b)^6\times 0.5 \\\<br>
&amp;= 0.5\binom {10}{4}\bigl[{\theta_a}^4(1-\theta_a)^6  + {\theta_b}^4(1-\theta_b)^6\bigl] \\\<br>
\end{align}
Làm tương tự với các $i=2,3,4,5$, ta được:
\begin{align}
p(X| \theta_a, \theta_b) &amp;= 0.5\binom {10}{4}\bigl[{\theta_a}^4(1-\theta_a)^6  + {\theta_b}^4(1-\theta_b)^6\bigl] \times  \\\<br>
&amp; \quad \quad \quad \quad \quad \quad \quad \quad \quad &hellip; \quad \quad \quad \quad \quad \times \\\<br>
&amp; \quad \quad 0.5\binom {10}{3}\bigl[{\theta_a}^3(1-\theta_a)^7  + {\theta_b}^3(1-\theta_b)^7\bigl]
\end{align}
Oke, tới đây, vấn đề còn lại chỉ là bài toán optimization thôi, chắc là easy rồi: chỉ cần giải (hệ) phương trình sau với biến là $\theta_a, \theta_b$
$$\frac{d [ \log p(X| \theta_a, \theta_b) ]}{d(\theta_a, \theta_b)} = 0$$
Chuyển cái kia về đạo hàm riêng phần theo từng cái $\theta_a, \theta_b$, giải hệ, xong. Nói vậy chớ ta không tin là giải nổi cái hệ nớ đâu, lấy giấy ra làm đạo hàm thử đi rồi biết, nhìn tích tổng tùm lum.</p>
<p>Nói túm lại, cách này cũng failed.</p>
<p>Đó, <em>mọi con đường đều dẫn tới thành Rome</em> (EM) (bởi các con đường khác đều dẫn tới ngõ cụt :v)</p>
<h2 id="lý-thuyết-về-expectation---maximization-em">Lý thuyết về Expectation - Maximization (EM)</h2>
<h3 id="tổng-quan">Tổng quan</h3>
<ul>
<li>Hai cách trên thực ra là 2 phương pháp tìm point estimator. Một estimator là một statistic bất kỳ trên observed data. Tất nhiên là estimator chỉ có ích khi nó dùng để dự đoán/ước đoán parameter của data thực.</li>
<li>EM thì khác với 2 thèn trên, EM ko phải là một estimator, mà là 1 phương pháp để tìm estimator, cụ thể ở đây là MLE. Để tìm MLE thì như trên có thử rồi đó, ta có thể lấy đạo hàm rồi giải tay hệ phương trình, có thể dùng gradient descent, &hellip; EM chỉ là 1 trong số đó.</li>
<li>EM đặc biệt phù hợp với những <em>MLE based on missing data (latent variable)</em>. Latent variable trong ví dụ của chúng ta là thèn $Z$, cái thèn mà ở mỗi round, chọn ngẫu nhiên 1 đồng xu rồi mới tung 10 lần. Latent variable trong cái intro là chuyện chẳng biết được chính xác crush nhìn mình hay nhìn thèn bạn mình.</li>
<li>Trong bối cảnh của EM, có 2 loại MLE: <em>complete-data likelihood</em> and <em>incomplete-data likelihood</em>.</li>
</ul>
<h3 id="chi-tiết">Chi tiết</h3>
<p>Mục tiêu là đi tìm $\theta$ sao cho nó maximize likelihook, mà hàm log thì đồng biến nên chuyện này cũng tương đương với maximum log likelihood:
\begin{align}
\theta &amp;= \underset{\theta}{\operatorname{argmax}} I(\theta) \\\<br>
I(\theta) &amp;= \log p(X| \theta) = \log \sum_Z \bigl [p(X,Z|\theta)\bigl ]\\\<br>
\end{align}
Bước cuối ta phải thêm thằng $Z$ vào, vì quy trình sinh ra data của ta có phụ thuộc vào nó.
Như đã nói ở trên, tìm max thèn này thì những cách sau đã ko thành công rồi:</p>
<ul>
<li>Giải tay phương trình $\frac{dI(\theta)}{d\theta}=0$</li>
<li>Gradient descent thì sao ??? chưa thử :v, sách nói là tìm gradient cũng rất phức tạp, nên mặc định cho là làm ko đc.</li>
</ul>
<p>Thời thế tạo anh hùng, khi mà các bậc anh tài bó tay hết, EM mới nghĩ ra 1 kế: nếu mà 1 bước định thiên hạ khó quá (tìm trực tiếp max luôn), thì hãy đi từng bước nhỏ, tìm các $\theta_i$ sao cho $$I(\theta_1) \leq I(\theta_2) \leq I(\theta_3) \leq &hellip; I(\theta_n)$$
thì đến lúc nào đó, rất nhiều khả năng là ta đã đi tới được (hoặc tới rất gần) cái $\theta$ làm maximize $I(\theta)$. Theo cái định hướng đó, EM phân tích thằng MLE thử:</p>
<p>Vì $\sum_Z p(Z|\theta) = 1$ và $\log$ là hàm lõm, ta có thể áp dụng bất đẳng thức Jensen:
\begin{align}
I(\theta) &amp;= \log p(X| \theta) \\\<br>
&amp;= \log \sum_Z  p(X,Z|\theta) \\\<br>
&amp;= \log \sum_Z \bigl [ q(Z) \times \frac{p(X,Z|\theta)}{q(Z)}   \bigl ] \\\<br>
&amp; \geq \sum_Z \left [ q(Z) \times \log \frac{p(X,Z|\theta)}{q(Z)} \right ])\\\<br>
&amp;= \mathbb{E}_{Z \sim q(Z)} \log \frac{p(X,Z|\theta)}{q(Z)} \\\<br>
&amp;= \mathbb{E}_{Z \sim q(Z)} \log p(X,Z|\theta) - \mathbb{E}_{Z \sim q(Z)} \log q(Z) \\\<br>
\end{align}
Để cho gọn, ở bước cuối, ta đặt $F(\theta ,q)=\mathbb{E}_{Z \sim q(Z)} \log p(X,Z|\theta) - \mathbb{E}_{Z \sim q(Z)} \log q(Z)$, thì chuỗi biến đổi trên có thể tóm gọn lại là:
$$
F(\theta ,q) \leq I(\theta) \quad \forall \theta, q \tag{^^} \label{eq:0}
$$
Vài cái tên hay ho, biết đâu bạn gặp đâu đó trong dòng đời tấp nập:</p>
<ul>
<li>$F$ được một số nơi đặt tên là <em>Variational Free Energy</em>.</li>
<li>Cái xác suất nằm trong phần tử đầu tiên của $F(\theta, q)$ là $p(X,Z|\theta)$, được đặt tên là <em>complete-data likelihood</em>, phân biệt với thèn $\log(X|\theta)$ là <em>incomplete-data likelihood</em>.</li>
</ul>
<p>Khoan khoan, ta đang muốn tìm $\theta_2$ sao cho $I(\theta_2) \geq I(\theta_1)$, vậy chuyện đi phát hiện ra rằng $F(\theta, q) \leq I(\theta)$ có gì hay ho ? Tinh tế là chỗ đó, vì đây là bất đẳng thức, nên thèn EM thấy rằng có thể tận dụng cả phần &ldquo;<strong>bất</strong>&rdquo; và phần &ldquo;<strong>đẳng</strong>&quot;:
\begin{align}
I(\theta_1) &amp;= F(\theta_1, q^*) \quad \text{(khai thác phần &ldquo;đẳng&rdquo;)} \tag{1}\label{eq:step.1} \\\<br>
&amp;\leq F(\theta_2, q) \quad \text{(tự làm mà ăn)}  \tag{2}\label{eq:step.2}\\\<br>
&amp;\leq I(\theta_2) \quad \text{(khai thác phần &ldquo;bất&rdquo;)} \tag{3}\label{eq:step.3}\\\<br>
\end{align}</p>
<p>Ở đây, chỉ có $\eqref{eq:step.1}$ và $\eqref{eq:step.2}$ là cần ta nhúng tay vào, và đó cũng là 2 bước tạo ra cái tên của giải thuật.</p>
<h3 id="step-1-khai-thác-phần-đẳng">Step 1: Khai thác phần đẳng</h3>
<p>Tìm $q$ cho đẳng thức xảy ra. Dấu bằng trong bđt Jensen xảy ra khi và chỉ khi:
\begin{align}
\frac{p(X,Z_i|\theta)}{q(Z_i)} &amp;= \frac{p(X,Z_j|\theta)}{q(Z_i)} \quad ,\forall i,j \\\<br>
% \frac{p(X,Z_i|\theta)}{p(Z_i| \theta)} &amp;= \frac{p(X,Z_j|\theta)}{p(Z_j| \theta)} \quad ,\forall i,j \\\<br>
\end{align}
Tạm gọi gọn 2 cái trên là $p(Z)$ với $q(Z)$. Phương trình trên tương đương: $\frac{p(Z)}{q(Z)} = \text{const} = c$ ở mọi điểm của 2 cái phân bố xác suất (theo $Z$).
Vì 2 ông này đều là pmf nên ta có:
$$
\left \{
\begin{aligned} 
\sum_i q(Z_i) &amp;= 1 \\\<br>
\sum_i p(Z_i) &amp;= 1
\end{aligned} 
\right .
\Rightarrow \sum_{Z_i} c \times q(Z_i) = 1 \Leftrightarrow c \sum_i q(Z_i) = 1 \Leftrightarrow c=1 \Leftrightarrow q(Z_i)=p(Z_i) \quad \forall i
$$
Ta có $q(Z) = p(Z, X|\theta)$ là một nghiệm. Haha, nghiệm này ko đem lại nhiều ý nghĩa đâu, vì $p=q$ thì $\log=0$ rồi, giải làm màu thôi :v.</p>
<p>Nghiệm khác có ý nghĩa đây, $q(Z) = p(Z|X, \theta)$, thử thay vào coi:
\begin{align}
\frac{p(X,Z_i|\theta)}{q(Z_i)} &amp;= \frac{p(X,Z_j|\theta)}{q(Z_i)} \quad ,\forall i,j \\\<br>
\Rightarrow \frac{p(X,Z_i|\theta)}{p(Z_i|X, \theta)} &amp;= \frac{p(X,Z_j|\theta)}{p(Z_j| X, \theta)} \quad ,\forall i,j \\\<br>
\Leftrightarrow \frac{p(Z_i|X, \theta) p(X|\theta))}{p(Z_i|X, \theta)} &amp;= \frac{p(Z_j|X,\theta) p(X|\theta)}{p(Z_j| X, \theta)} \quad ,\forall i,j \\\<br>
\Leftrightarrow p(X|\theta) &amp;= p(X|\theta) \\\<br>
\end{align}
Đó, vậy $q(Z) = p(Z|X, \theta)$ là nghiệm thứ 2. Còn có nghiệm thứ 3, 4, &hellip; không thì ta không biết. Với EM thì nghiệm này là đủ rồi. Chốt cái, kết quả của bước 1 là: khi $q(Z) = p(Z|X, \theta)$, bất đẳng thức $\eqref{eq:0}$ trở thành đẳng thức, dùng $\theta=\theta_1$ luôn, thì ta có
$$
I(\theta_1) = F(\theta_1, p(Z | X, \theta_1))
$$
XONG bước 1.
Bước này được đặt tên là Expectation bởi ta đã chuyển cái $I(\theta)$ về cái $F$, là một hàm của expectation dựa trên $\theta$</p>
<h3 id="step-2-tự-làm-mà-ăn">Step 2: Tự làm mà ăn</h3>
<p>Bước 1 khỏe vì ông Jensen đã đưa ra điều kiện để dấu bằng xảy ra, ta chỉ có ráp dô mà tìm $q$ thôi. Bước này mới cực, ta phải tự đi tìm giá trị $\theta_2$ sao cho $F(\theta_2, q^*) \geq F(\theta_1, q^*)$. Nhưng EM trẻ trâu mà , đã chơi thì chơi cho lớn luôn, ko chỉ tìm $\theta_2$ thỏa cái điều kiện cỏn con $F(\theta_2, q^*) \geq F(\theta_1, q^*)$, mà tìm sao cho $F(\theta, q^*)$ là lớn nhất luôn. Bởi vậy EM mới có cái danh Maximization.</p>
<p>\begin{align}
\theta_2 &amp;= \underset{\theta}{\operatorname{argmax}} F(\theta, q^*) \\\<br>
&amp;= \underset{\theta}{\operatorname{argmax}} F(\theta, p(Z|X, \theta_1)) \\\<br>
&amp;= \underset{\theta}{\operatorname{argmax}} \mathbb{E}_{Z \sim p(Z|X, \theta_1)} \log p(X,Z|\theta) - \mathbb{E}_{Z \sim p(Z|X, \theta_1)} \log p(Z|X, \theta_1) \\\<br>
&amp;= \underset{\theta}{\operatorname{argmax}} \mathbb{E}_{Z \sim p(Z|X, \theta_1)} \log p(X,Z|\theta) \\\<br>
\end{align}
THẤY CÁI GÌ KO :))) à, ngó lơ cá $\mathbb{E}$ đi, nó ko quan trọng, xử lý dễ, nhìn vô cái $\log$ thôi. Ta đã chuyển từ bài khó: Maximize log likelihood thành bài toán, cũng là maximize likelihood :)). <strong>NHƯNG MÀ MẤU CHỐT LÀ ĐÃ CHUYỂN $\log p(X| \theta)$ VỀ $\log p(X, Z| \theta)$</strong>, tức là từ <em>incomplete-data likelihood</em> về <em>complete-data likelihood</em>. Cái sau giải dễ hơn nhiều. Tới đây thì EM chỉ cần thuế đám lâu la xử lý bằng cách giải phương trình đạo hàm bằng 0 là xong, một nốt nhạc.</p>
<p>OKE, xong step 2, quay lại làm step 1 với $\theta = \theta_2$, rồi tới step 2, &hellip; cứ lặp như vậy, ta có chuỗi:
\begin{align}
I(\theta_1) &amp;= F(\theta_1, p(Z|X, \theta_1)) \quad \text{(bước 1)}\\\<br>
&amp;\leq F(\theta_2, p(Z|X, \theta_1)) \quad \text{(bước 2)}\\\<br>
&amp;\leq I(\theta_2) \quad \quad \quad \quad \quad \quad \text{(dựa trên } \eqref{eq:step.3} \text{ )}\\\<br>
&amp;= F(\theta_2, p(Z|X, \theta_2)) \quad \text{(bước 1)}\\\<br>
&amp;\leq F(\theta_3, p(Z|X, \theta_2)) \quad \text{(bước 2)}\\\<br>
&amp;\leq I(\theta_3) \quad \quad \quad \quad \quad \quad \text{(dựa trên } \eqref{eq:step.3} \text{ )}\\\<br>
&amp;= F(\theta_3, p(Z|X, \theta_3)) \quad \text{(bước 1)}\\\<br>
&amp; &hellip;
\end{align}
Quy trình trên sẽ dẫn ta đến một cái giá trị $\theta$ càng ngày càng làm cho $I(\theta)$ chỉ có thể giữ nguyên hoặc tăng thôi, không bao giờ giảm. Nên cứ cặm cụi làm miết, làm miết làm mãi cũng sẽ tới đích thôi.</p>
<h1 id="cần-coi-thêm">Cần coi thêm</h1>
<ul>
<li>
<p>Có chắc là tới đích ko :v, có khi nào nó đứng miết 1 chỗ ko ? Nhớ là sách có chứng minh bằng $\lim$ sao đó, mà mệt quá, lơ thôi.</p>
</li>
<li>
<p>Giải cái ví dụ ban đầu bằng EM. Dài quá nên lười. Trong ref có bài giải nớ.</p>
</li>
<li>
<p>Đọc sách thì thấy sách chỉ nói EM dùng để tìm MLE, còn wiki nói EM có thể dùng để tìm MAP nữa.</p>
</li>
<li>
<p>EM có một đối thủ đáng ghờm là Variantion inference: <a href="https://www.quora.com/When-should-I-use-variational-inference-vs-expectation-maximization-for-fitting-a-Gaussian-mixture-model">https://www.quora.com/When-should-I-use-variational-inference-vs-expectation-maximization-for-fitting-a-Gaussian-mixture-model</a></p>
</li>
<li>
<p>Ở trên mình đã đi giải thích trên phương diện toán (chưa đầy đủ) tại sao EM nó hội tụ được. Nhưng quan trọng nên hiểu được intuition của nó, một vài nguồn trên quora:</p>
<ul>
<li><a href="https://www.quora.com/Why-does-the-EM-expectation-maximization-algorithm-necessarily-converge-What-are-some-of-its-common-applications">https://www.quora.com/Why-does-the-EM-expectation-maximization-algorithm-necessarily-converge-What-are-some-of-its-common-applications</a></li>
</ul>
</li>
<li>
<p>Có thèn bạn nói K-means là trường hợp riêng của EM, hoặc ít nhất là có thể dùng EM để giải thích K-means, cũng hay ho mà chưa coi.</p>
</li>
</ul>
<h1 id="chưa-hiểu">Chưa hiểu:</h1>
<ul>
<li>$I(\theta)$ sẽ hội tụ, vậy giá trị của $\theta$ với $Z$ có hội tụ ko ?</li>
<li>ME có đảm bảo tìm ra global maximum ?</li>
<li>Đi biến đổi thôi (đẳng thức) mà lại dùng tới bất đẳng thức (step 1) ? Điều này vừa ngược đời vừa hay ho !</li>
<li>Giải MLE bằng gradient descent thì sao ? Mấy framework tự động tính gradient được hết rồi mà.</li>
<li>Vì sao ko dùng cái nghiệm trivial (làm màu) kia ?</li>
</ul>
<h1 id="reference">Reference:</h1>
<ul>
<li>Slide này giải cái ví dụ trên nề: <a href="http://people.inf.ethz.ch/ganeao/em_tutorial.pdf">http://people.inf.ethz.ch/ganeao/em_tutorial.pdf</a></li>
<li>Tương tự cái mình muốn viết: <a href="http://rstudio-pubs-static.s3.amazonaws.com/1001_3177e85f5e4840be840c84452780db52.html">http://rstudio-pubs-static.s3.amazonaws.com/1001_3177e85f5e4840be840c84452780db52.html</a></li>
<li>Khi nào coi là Latent: <a href="https://stats.stackexchange.com/questions/5136/when-do-you-consider-a-variable-is-a-latent-variable">https://stats.stackexchange.com/questions/5136/when-do-you-consider-a-variable-is-a-latent-variable</a></li>
<li>Cái note này hay quá: <a href="http://www.stats.ox.ac.uk/~sejdinov/teaching/dmml/17_notes3.pdf">http://www.stats.ox.ac.uk/~sejdinov/teaching/dmml/17_notes3.pdf</a>. Mà từ cái note này học thêm đc 1 điều, muốn search gì về statistics thì cứ google nhắm: inurl:stats.ox, tài liệu ở đây hay cực.</li>
</ul>

</main>

    <footer>
      
<script async src="//yihui.name/js/center-img.js"></script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {
  	inlineMath: [['$','$'], ['\\(','\\)']],
  	displayMath: [['$$','$$'], ['\\[','\\]']],
  	processEscapes: true
  }});
</script>

      
      <hr/>
      Blog | <a href="https://github.com/ductri">Github</a> | <a href="https://www.facebook.com/ductrivn">Facebook</a>
      
    </footer>
  </body>
</html>

