<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description"
    content="Some random papers on LLM We have readings on the trendy LLM. I collected some papers myself here. The list is still updating
 4-th year Ph.D. student">
  <title> Some random papers on LLM - Tri Nguyen </title>

  <!-- stylesheets -->
  <link media="all" href="//netdna.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" rel="stylesheet">
  <link media="all" href="//netdna.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
  <link media="all" rel="stylesheet" href="//assets/css/site.css">

<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>
  <header>
  <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container-fluid">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a id="site-title" class="navbar-brand" href="//">
          <img class="pull-left img-responsive" src="https://ductri.github.io/assets/images/naruto.jpg"> 
          <span class="name-holder">Tri Nguyen</span>
        </a>
      </div>

      <div class="collapse navbar-collapse" id="navbar">

        <ul id="main-menu" class="nav navbar-nav navbar-right">

          
            <li><a href="//">
              <i class="fa fa-home"></i> Home</a>
            </li>
          
            <li><a href="//documents/">
              <i class="fa fa-file"></i> Documents</a>
            </li>
          
            <li><a href="//about/">
              <i class="fa fa-info-circle"></i> About Me</a>
            </li>
          

        </ul>
      </div>
      <!-- /.navbar-collapse -->
    </div>
    <!-- /.container-fluid -->
  </nav>

</header>

  
  <div class="container">
  <div class="row">
    <div class="col-xs-12 col-sm-12 col-md-offset-1 col-md-10 col-lg-offset-2 col-lg-8">
      <div class="page">

        <header class="page-header">
          <h3 class="page-title"><a href="//note/2023/09/25/llm-abc.html">Some random papers on LLM</a></h3>
          <div class="post-metadata">
            <div> <i class="fa fa-calendar"></i>
              <time>
                Monday, September 25, 2023
              </time>
            </div>
            <div> <i class="fa fa-tag"></i>
                
                  <span class="label label-tag">llm</span>
                
                  <span class="label label-tag">attention</span>
                
            </div>
          </div>

        </header>

        <article class="page-content">
          <p>We have readings on the trendy LLM. I collected some papers myself here. The list is still updating
<!--more--></p>

<h1 id="attentiontransformer">Attention/Transformer</h1>

<p>The goal is to encode an sequential data: $x_1 \to x_2 \to \ldots \to x_t$.</p>

<p>As usual, since $x_i$ is discrete, $x_i \in \mathcal{V}$. Each $v \in \mathcal{V}$ is represented as a trainable vector. In Transformer, this vector is partitioned into 3 disjoint parts:</p>

<ul>
  <li>$\textbf{q} \in \mathbb{R}^{d_k}$</li>
  <li>$\textbf{k} \in \mathbb{R}^{d_k}$</li>
  <li>$\textbf{v} \in \mathbb{R}^{d_v}$</li>
</ul>

<p>This way, a sentence is encoded by 3 matrices $\textbf{Q} \in \mathbb{R}^{t \times d_k}, \textbf{K} \in \mathbb{R}^{t \times d_k}, \textbf{V} \in \mathbb{R}^{t \times d_v}$.</p>

<p>Next idea: the presentation of $x_i$ is a convex combination of other $\textbf{x}_j$ where $j=1, \ldots , i-1$.
The presentation is realized by $\textbf{v}_i$, so
\(\text{atten}_i(\textbf{v}_i) = \sum_{\ell =1}^{i} a_\ell  \textbf{v}_\ell  
= \textbf{a}^{\sf T} \textbf{V}, \quad \textbf{a} \in \mathbb{R}^{t}\)</p>

<!-- - The final presentation should be fixed although the sequence length is varying. -->
<!-- - The encoding mechanism should take into acount the sequential order of the input sample. -->
<!-- - Each $x_i$ should have its own representation, and representation of $x_i$ could be affected by $x_j$, $j<i$ as the context. -->

<p><img src="/assets/images/2023-09-25-llm/attention-arch.png" width="600px" style="border: 1px solid  black;" />
<img src="/assets/images/2023-09-25-llm/attention.png" width="500px" style="border: 1px solid  black;" /></p>

<p>Now the coefficient $a_i$ must be learned somehow. Attention suggests that 
\(\begin{align*}
&amp;\widetilde{\textbf{a}}_i = [\widetilde{a}_1, \ldots , \widetilde{a}_t] = [\textbf{q}_i^{\sf T} \textbf{k}_1, \ldots , \textbf{q}_i^{\sf T} \textbf{k}_\ell, \ldots , \textbf{q}_i^{\sf T} \textbf{k}_t] = \textbf{q}_i^{\sf T} \textbf{K} \\
&amp;\widetilde{\textbf{A}} = [\widetilde{\textbf{a}}_1, \ldots , \widetilde{\textbf{a}}_t] = [\textbf{q}_1^{\sf T} \textbf{K}, \ldots , \textbf{q}_t^{\sf T} \textbf{K}] = \textbf{Q}^{\sf T} \textbf{K} \\
&amp;\textbf{A} = \text{softmax} (\widetilde{\textbf{A}}) \triangleq [\text{softmax}(\widetilde{\textbf{a}}_i), \ldots , \text{softmax}(\widetilde{\textbf{a}}_t)] \in \mathbb{R}^{t \times t}
\end{align*}\) 
Each vector $\textbf{a}_i$ represent distribution of ‚Äúattention‚Äù of word $i$ paying over the whole sentence.</p>

<p>So take everything as matricies, we have
\(\begin{align*}
\text{attention} 
&amp;= \textbf{A}^{\sf T} \textbf{V}, \quad \textbf{A} \in \mathbb{R}^{d_v \times t} \\
&amp;= \text{softmax}(\textbf{K}^{\sf T} \textbf{Q}) \textbf{V} \in \mathbb{R}^{t \times d_v}
\end{align*}\)</p>

<h4 id="multiheads">Multiheads</h4>

<p>Then, in order to allow for multiple learned patterns, each word is now presented with $H$ different triples $(\textbf{Q}_h, \textbf{K}_h, \textbf{V}_h)$.
\(\text{attention} (\textbf{Q}\textbf{W}_h^{(1)}, \textbf{K}\textbf{W}_h^{(2)}, \textbf{V}\textbf{W}_h^{(3)}), \quad h=1, \ldots , H\) 
Then as usuall, every thing is concatenated and input to a final FC layer.</p>

<p>Motivation:</p>

<p><img src="/assets/images/2023-09-25-llm/att_moti.png" width="500px" /></p>

<h4 id="order-encoding">Order Encoding</h4>

<p>Now as the representation is just a convex combination of some set, there is no notion of order. Hence it is necessary that the order info is encoded in the $\textbf{v}$ vector.</p>

<p><img src="/assets/images/2023-09-25-llm/order-emb.png" width="600px" />
<img src="/assets/images/2023-09-25-llm/tmp_img_2023-09-19-12-46-39.png" width="800px" /></p>

<p>So that‚Äôs basically it.</p>

<p><img src="/assets/images/2023-09-25-llm/transformer-arch.png" width="500px" style="border: 1px solid  black;" /></p>

<h1 id="flamingo-a-visual-language-model-for-few-shot-learning">Flamingo: a visual language model for few-shot learning</h1>
<p><img src="/assets/images/2023-09-25-llm/paper1.png" width="600px" /></p>

<p><strong>Task</strong>:</p>

<p><img src="/assets/images/2023-09-25-llm/flamingo.png" width="800px" /></p>

<p>Mixing text and image, predict next word token, pretrained LLM, vision input is undergone a pretrain feature extractor, then to a trainable network to produce a fixed length vector for each image/video input.</p>

<p>Dataset is crawl from webpage, image is replaced by special token  <imgx></imgx></p>

<p>The vision module produce a fixed number of tokens. These tokens are treated as word tokens.</p>

<h3 id="method">Method</h3>

<p><img src="/assets/images/2023-09-25-llm/fla-mle.png" width="800px" /></p>

<p>Input example:
<img src="/assets/images/2023-09-25-llm/fla-example-input.png" width="800px" style="border: 1px solid  black;" /></p>

<p><img src="/assets/images/2023-09-25-llm/fla-architecture.png" width="800px" style="border: 1px solid  black;" /></p>

<p><img src="/assets/images/2023-09-25-llm/fla-archi-zoomin.png" width="800px" style="border: 1px solid  black;" /></p>

<h3 id="in-more-details-">In more details ‚Ä¶</h3>

<p>Data collection:</p>

<ul>
  <li>43 million webpages. Sample a random subsequence of ùêø = 256 tokens and take up to the first ùëÅ = 5 images included in the sampled sequence</li>
  <li>For image text pairs,
    <ul>
      <li>ALIGN [50] dataset contains 1.8 billion images paired with alt-text</li>
      <li>LTIP dataset consists of 312 million image and text pairs</li>
      <li>VTP dataset contains 27 million short videos (approximately 22 seconds on average) paired with sentence descriptions</li>
    </ul>
  </li>
  <li>beam search for decoding
    <h3 id="evaluation">Evaluation</h3>
  </li>
  <li>
    <p>What can it do? It can learn to perform new task pretty quickly using ‚ÄúIn-context learning‚Äù \ldots like what has been used in GPT3.</p>
  </li>
  <li>Few shot learning: using only 4 examples</li>
</ul>

<h1 id="llm-knowledge-retrieval">LLM knowledge retrieval</h1>
<p><img src="/assets/images/2023-09-25-llm/paper2.png" width="800px" /></p>

<p>Setting: Given a dataset of text pairs (x, y), like x: question, y: answer.</p>

<h3 id="idea">Idea</h3>

<ul>
  <li>Model: Receive a sequence $x$, and output a prediction of sequence $\widehat{y}$</li>
</ul>

<p>LLM contains knowledge somehow, and can be seen to have a parametric memory. Let‚Äôs extend that by adding a non-parametric external memory, in this case from Wiki. 
So given, for example, a question, model uses its internal knowledge, retrieve external resource, combine them and generate an answer.</p>

<p>More concretely, authors proposed a probabilistic model with 2 ways to do inference approximately: RAG-Sequence Model and RAG-Token Model,
<img src="/assets/images/2023-09-25-llm/llm-knowl-model-app.png" width="800px" /></p>

<ul>
  <li>
    <p>Dive in to the model architecture:
<img src="/assets/images/2023-09-25-llm/llm-knowledge-arch.png" width="800px" style="border: 1px solid  black;" />
<img src="/assets/images/2023-09-25-llm/llm-knowledge-arch1.png" width="800px" style="border: 1px solid  black;" /></p>

    <ul>
      <li>The generator: BART-large, 400M parameters. Input is the concatenation of $x$ and top-k latent documents $z$. This BART-large model is accountable for ‚Äòparametric memory‚Äô.</li>
    </ul>
  </li>
  <li>
    <p>Train both Query encoder and Generator.
Training objective is marginal log-likelihood of the target like usual, like in sequence generation.</p>
  </li>
</ul>

<h3 id="thoughts">Thoughts?</h3>

<ul>
  <li>Knowledge vs overfitting?</li>
  <li>What could be extended?
    <ul>
      <li>Offer evidence like in Bing.</li>
      <li>Instead of using Wiki, get top 5 articles from Google search, input them to the BERT decoder. Or in general, hot-swap memory? Why do they have to replace the whole Wiki instead of substituting relevant articles?</li>
    </ul>
  </li>
</ul>


        </article>
      </div>
    </div>
  </div>
</div>


  

<footer class="footer">
    <div class="container">
      <div class="row">
        <div class="col-lg-offset-3 col-lg-6">
            <div class="row">
              <div class="col-xs-12 text-center">
                <small>&copy; rifyll theme, 2014</small>
              </div>
            </div>
        </div>
      </div>
    </div>
  </footer>


  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZGK7PGG6HV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZGK7PGG6HV');
</script>


  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
  <script src="//netdna.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
  <!--<script src="//assets/js/prism.js"></script> -->
  <script src="//assets/js/site.js"></script>

</body>

</html>
