<!DOCTYPE html><html lang="en" dir="ltr"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><title>Some random papers on LLM | Tri Nguyen</title><meta name="description" content="some papers I have read about LLM"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="icon" href="/favicon.ico" type="image/x-icon"><link href='/feed.xml' rel='alternate' type='application/atom+xml'><link rel="canonical" href="/note/2023/09/25/llm-abc/"><link rel="stylesheet" href="/style.css"> <!--<link rel="stylesheet" href="/tufte.css"/> --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css" integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js" integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"></script> <script> document.addEventListener("DOMContentLoaded", function() { renderMathInElement(document.body, { // customised options // • auto-render specific keys, e.g.: delimiters: [ {left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\(', right: '\\)', display: false}, {left: '\\[', right: '\\]', display: true} ], // • rendering keys, e.g.: throwOnError : false }); }); </script><body><main><h2 class="title">Tri Nguyen</h2><nav class="sans"> <input class="menu-btn" type="checkbox" id="menu-btn" /><p><label class="menu-icon" for="menu-btn">Menu</label></p><ul class="site-nav"><li><a href="/">Home</a><li><a href="/articles">Articles</a><li><a href="/feed.xml">RSS</a></ul></nav><article><h1>Some random papers on LLM</h1><p class="subtitle">Sep 25, 2023</p><p>We have readings on the trendy LLM. I collected some papers myself here. The list is still updating <!--more--></p><h1 id="attentiontransformer">Attention/Transformer</h1><p>The goal is to encode an sequential data: $x_1 \to x_2 \to \ldots \to x_t$.</p><p>As usual, since $x_i$ is discrete, $x_i \in \mathcal{V}$. Each $v \in \mathcal{V}$ is represented as a trainable vector. In Transformer, this vector is partitioned into 3 disjoint parts:</p><ul><li>$\textbf{q} \in \mathbb{R}^{d_k}$<li>$\textbf{k} \in \mathbb{R}^{d_k}$<li>$\textbf{v} \in \mathbb{R}^{d_v}$</ul><p>This way, a sentence is encoded by 3 matrices $\textbf{Q} \in \mathbb{R}^{t \times d_k}, \textbf{K} \in \mathbb{R}^{t \times d_k}, \textbf{V} \in \mathbb{R}^{t \times d_v}$.</p><p>Next idea: the presentation of $x_i$ is a convex combination of other $\textbf{x}_j$ where $j=1, \ldots , i-1$. The presentation is realized by $\textbf{v}_i$, so <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>atten</mtext><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">v</mtext><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></msubsup><msub><mi>a</mi><mi mathvariant="normal">ℓ</mi></msub><msub><mtext mathvariant="bold">v</mtext><mi mathvariant="normal">ℓ</mi></msub><mo>=</mo><msup><mtext mathvariant="bold">a</mtext><mi mathvariant="sans-serif">T</mi></msup><mtext mathvariant="bold">V</mtext><mo separator="true">,</mo><mspace width="1em"/><mtext mathvariant="bold">a</mtext><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\text{atten}_i(\textbf{v}_i) = \sum_{\ell =1}^{i} a_\ell \textbf{v}_\ell = \textbf{a}^{\sf T} \textbf{V}, \quad \textbf{a} \in \mathbb{R}^{t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord text"><span class="mord">atten</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord textbf">v</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2643em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9646em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">ℓ</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">ℓ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord text"><span class="mord textbf">v</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">ℓ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0435em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">a</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathsf mtight">T</span></span></span></span></span></span></span></span></span></span><span class="mord text"><span class="mord textbf">V</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord textbf">a</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7936em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span></span></span></span></span></p><!-- - The final presentation should be fixed although the sequence length is varying. --> <!-- - The encoding mechanism should take into acount the sequential order of the input sample. --> <!-- - Each $x_i$ should have its own representation, and representation of $x_i$ could be affected by $x_j$, $j<i$ as the context. --><p><img src="/assets/images/2023-09-25-llm/attention-arch.png" width="600px" style="border: 1px solid black;" /> <img src="/assets/images/2023-09-25-llm/attention.png" width="500px" style="border: 1px solid black;" /></p><p>Now the coefficient $a_i$ must be learned somehow. Attention suggests that <span class="katex-error" title="ParseError: KaTeX parse error: {align*} can be used only in display mode." style="color:#cc0000">\begin{align*} &amp;\widetilde{\textbf{a}}_i = [\widetilde{a}_1, \ldots , \widetilde{a}_t] = [\textbf{q}_i^{\sf T} \textbf{k}_1, \ldots , \textbf{q}_i^{\sf T} \textbf{k}_\ell, \ldots , \textbf{q}_i^{\sf T} \textbf{k}_t] = \textbf{q}_i^{\sf T} \textbf{K} \\ &amp;\widetilde{\textbf{A}} = [\widetilde{\textbf{a}}_1, \ldots , \widetilde{\textbf{a}}_t] = [\textbf{q}_1^{\sf T} \textbf{K}, \ldots , \textbf{q}_t^{\sf T} \textbf{K}] = \textbf{Q}^{\sf T} \textbf{K} \\ &amp;\textbf{A} = \text{softmax} (\widetilde{\textbf{A}}) \triangleq [\text{softmax}(\widetilde{\textbf{a}}_i), \ldots , \text{softmax}(\widetilde{\textbf{a}}_t)] \in \mathbb{R}^{t \times t} \end{align*}</span> Each vector $\textbf{a}_i$ represent distribution of “attention” of word $i$ paying over the whole sentence.</p><p>So take everything as matricies, we have <span class="katex-error" title="ParseError: KaTeX parse error: {align*} can be used only in display mode." style="color:#cc0000">\begin{align*} \text{attention} &amp;= \textbf{A}^{\sf T} \textbf{V}, \quad \textbf{A} \in \mathbb{R}^{d_v \times t} \\ &amp;= \text{softmax}(\textbf{K}^{\sf T} \textbf{Q}) \textbf{V} \in \mathbb{R}^{t \times d_v} \end{align*}</span></p><h4 id="multiheads">Multiheads</h4><p>Then, in order to allow for multiple learned patterns, each word is now presented with $H$ different triples $(\textbf{Q}_h, \textbf{K}_h, \textbf{V}_h)$. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>attention</mtext><mo stretchy="false">(</mo><mtext mathvariant="bold">Q</mtext><msubsup><mtext mathvariant="bold">W</mtext><mi>h</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo><mtext mathvariant="bold">K</mtext><msubsup><mtext mathvariant="bold">W</mtext><mi>h</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo><mtext mathvariant="bold">V</mtext><msubsup><mtext mathvariant="bold">W</mtext><mi>h</mi><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo><mo separator="true">,</mo><mspace width="1em"/><mi>h</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">\text{attention} (\textbf{Q}\textbf{W}_h^{(1)}, \textbf{K}\textbf{W}_h^{(2)}, \textbf{V}\textbf{W}_h^{(3)}), \quad h=1, \ldots , H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3461em;vertical-align:-0.3013em;"></span><span class="mord text"><span class="mord">attention</span></span><span class="mopen">(</span><span class="mord text"><span class="mord textbf">Q</span></span><span class="mord"><span class="mord text"><span class="mord textbf">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.3987em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3013em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord textbf">K</span></span><span class="mord"><span class="mord text"><span class="mord textbf">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.3987em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3013em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord textbf">V</span></span><span class="mord"><span class="mord text"><span class="mord textbf">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.3987em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">3</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3013em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span> Then as usuall, every thing is concatenated and input to a final FC layer.</p><p>Motivation:</p><p><img src="/assets/images/2023-09-25-llm/att_moti.png" width="500px" /></p><h4 id="order-encoding">Order Encoding</h4><p>Now as the representation is just a convex combination of some set, there is no notion of order. Hence it is necessary that the order info is encoded in the $\textbf{v}$ vector.</p><p><img src="/assets/images/2023-09-25-llm/order-emb.png" width="600px" /> <img src="/assets/images/2023-09-25-llm/tmp_img_2023-09-19-12-46-39.png" width="800px" /></p><p>So that’s basically it.</p><p><img src="/assets/images/2023-09-25-llm/transformer-arch.png" width="500px" style="border: 1px solid black;" /></p><h1 id="flamingo-a-visual-language-model-for-few-shot-learning">Flamingo: a visual language model for few-shot learning</h1><p><img src="/assets/images/2023-09-25-llm/paper1.png" width="600px" /></p><p><strong>Task</strong>:</p><p><img src="/assets/images/2023-09-25-llm/flamingo.png" width="800px" /></p><p>Mixing text and image, predict next word token, pretrained LLM, vision input is undergone a pretrain feature extractor, then to a trainable network to produce a fixed length vector for each image/video input.</p><p>Dataset is crawl from webpage, image is replaced by special token <imgx></imgx></p><p>The vision module produce a fixed number of tokens. These tokens are treated as word tokens.</p><h3 id="method">Method</h3><p><img src="/assets/images/2023-09-25-llm/fla-mle.png" width="800px" /></p><p>Input example: <img src="/assets/images/2023-09-25-llm/fla-example-input.png" width="800px" style="border: 1px solid black;" /></p><p><img src="/assets/images/2023-09-25-llm/fla-architecture.png" width="800px" style="border: 1px solid black;" /></p><p><img src="/assets/images/2023-09-25-llm/fla-archi-zoomin.png" width="800px" style="border: 1px solid black;" /></p><h3 id="in-more-details-">In more details …</h3><p>Data collection:</p><ul><li>43 million webpages. Sample a random subsequence of 𝐿 = 256 tokens and take up to the first 𝑁 = 5 images included in the sampled sequence<li>For image text pairs,<ul><li>ALIGN [50] dataset contains 1.8 billion images paired with alt-text<li>LTIP dataset consists of 312 million image and text pairs<li>VTP dataset contains 27 million short videos (approximately 22 seconds on average) paired with sentence descriptions</ul><li>beam search for decoding<h3 id="evaluation">Evaluation</h3><li><p>What can it do? It can learn to perform new task pretty quickly using “In-context learning” \ldots like what has been used in GPT3.</p><li>Few shot learning: using only 4 examples</ul><h1 id="llm-knowledge-retrieval">LLM knowledge retrieval</h1><p><img src="/assets/images/2023-09-25-llm/paper2.png" width="800px" /></p><p>Setting: Given a dataset of text pairs (x, y), like x: question, y: answer.</p><h3 id="idea">Idea</h3><ul><li>Model: Receive a sequence $x$, and output a prediction of sequence $\widehat{y}$</ul><p>LLM contains knowledge somehow, and can be seen to have a parametric memory. Let’s extend that by adding a non-parametric external memory, in this case from Wiki. So given, for example, a question, model uses its internal knowledge, retrieve external resource, combine them and generate an answer.</p><p>More concretely, authors proposed a probabilistic model with 2 ways to do inference approximately: RAG-Sequence Model and RAG-Token Model, <img src="/assets/images/2023-09-25-llm/llm-knowl-model-app.png" width="800px" /></p><ul><li><p>Dive in to the model architecture: <img src="/assets/images/2023-09-25-llm/llm-knowledge-arch.png" width="800px" style="border: 1px solid black;" /> <img src="/assets/images/2023-09-25-llm/llm-knowledge-arch1.png" width="800px" style="border: 1px solid black;" /></p><ul><li>The generator: BART-large, 400M parameters. Input is the concatenation of $x$ and top-k latent documents $z$. This BART-large model is accountable for ‘parametric memory’.</ul><li><p>Train both Query encoder and Generator. Training objective is marginal log-likelihood of the target like usual, like in sequence generation.</p></ul><h3 id="thoughts">Thoughts?</h3><ul><li>Knowledge vs overfitting?<li>What could be extended?<ul><li>Offer evidence like in Bing.<li>Instead of using Wiki, get top 5 articles from Google search, input them to the BERT decoder. Or in general, hot-swap memory? Why do they have to replace the whole Wiki instead of substituting relevant articles?</ul></ul><hr><p><a href="/articles">← Back to all articles</a></p></article></main><div class="bottom-footer"> <span class="mini-note">You've reached the end of the page. Good job!</span></div></script>
